<!doctype html>
<html>
<head>


<center>
<table width="40%" border="0" >
	<tr>
		<td>Dani's <b>research</b> hypothesis:</td>
		<td>``ESP exists"</td>
	</tr>
	<tr>
		<td>Dani's <b>statistical</b> hypothesis: </td>
		<td>\( \theta \neq 0.5 \)</td>
	</tr>
</table>
</center>



<table width="50%" border="1" >
	<tr>
		<td>&nbsp;</td>
		<td>retain \( H_0 \)</td>
		<td>reject  \( H_0 \) </td>
	</tr>
	<tr>
		<td> \( H_0 \) is true </td>
		<td>correct decision</td>
		<td> error (type I)</td>
	</tr>
	<tr>
		<td>\( H_0 \) is false </td>
		<td>error (type II)</td>
		<td>correct decision</td>
	</tr>
</table>


<table width="50%" border="1" >
	<tr>
		<td>&nbsp;</td>
		<td>retain \( H_0 \)</td>
		<td>reject  \( H_0 \) </td>
	</tr>
	<tr>
		<td> \( H_0 \) is true </td>
		<td> \(1 - \alpha\) (probability of correct retention) </td>
		<td> \(\alpha\)  (type I error rate)</td>
	</tr>
	<tr>
		<td>\( H_0 \) is false </td>
		<td>\(\beta\) (type II error rate)</td>
		<td> \(1 - \beta\) (power of the test)</td>
	</tr>
</table>


<table width="50%" border="1" >
	<tr>
		<td>Value of \( \alpha \)</td>
		<td>0.05</td>
		<td>0.04 </td>
		<td>0.03</td>
		<td>0.02</td>
		<td>0.01 </td>
	</tr>
	<tr>
		<td> Reject the null? </td>
		<td> Yes </td>
		<td> Yes</td>
		<td> Yes </td>
		<td> No </td>
		<td> No</td>
	</tr>
</table>




<table width="50%" border="1" >
	<tr>
		<td>Usual notation</td>
		<td>Signif. stars</td>
		<td>English translation </td>
		<td>The null is...</td>
	</tr>
	<tr>
		<td> \( p \) &gt; .05 </td>
		<td> &nbsp; </td>
		<td> The test wasn't significant</td>
		<td> Retained </td>
	</tr>
	<tr>
		<td> \( p \) &lt; .05 </td>
		<td> * </td>
		<td> The test was significant at \( \alpha \) = .05 but not at \( \alpha \) = .01 or \( \alpha \) = .001. </td>
		<td> Rejected </td>
	</tr>
	<tr>
		<td> \( p \) &lt; .01 </td>
		<td> ** </td>
		<td> The test was significant at \( \alpha \) = .05  and \( \alpha \) = .01 but not at \( \alpha \) = .001. </td>
		<td> Rejected </td>
	</tr>
	<tr>
		<td> \( p \) &lt; .001 </td>
		<td> *** </td>
		<td> The test was significant at all levels </td>
		<td> Rejected </td>
	</tr>
</table>

<table width="50%" border="1" ><caption>Table 9.2: A crude guide to understanding the relationship between statistical significance and
effect sizes. Basically, if you don’t have a significant result then the effect size is pretty meaningless because you
don’t have any evidence that it’s even real. On the other hand, if you do
have a significant effect but your effect size is small then there’s a pretty good chance that your
result (although real) isn’t all that interesting. However, this guide is very crude. It depends
a lot on what exactly you’re studying. Small effects can be of massive practical importance in
some situations. So don’t take this table too seriously. It’s a rough guide at best.</caption>
	<tr>
		<td>&nbsp;</td>
		<td>big effect size</td>
		<td>small effect size </td>
	</tr>
	<tr>
		<td> significant result </br> </td>
		<td> difference is real, and </br> of practical importance </td>
		<td>  difference is real, but </br>  might not be interesting </td>
	</tr>
	<tr>
		<td> non-significant result </td>
		<td> no effect observed </td>
		<td> no effect observed </td>
	</tr>
</table>
