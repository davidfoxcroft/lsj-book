<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>LEARNING STATISTICS WITH JAMOVI - 12&nbsp; Correlation and linear regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13-Comparing-several-means-one-way-ANOVA.html" rel="next">
<link href="./11-Comparing-two-means.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['S$','S$'], ["\\[","\\]"] ]
      processEscapes: true
    }
  });
</script>



  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="LEARNING STATISTICS WITH JAMOVI - 12&nbsp; Correlation and linear regression">
<meta property="og:description" content="">
<meta property="og:image" content="https://davidfoxcroft.github.io/lsj-book/images/fig12-1a.png">
<meta property="og:site-name" content="LEARNING STATISTICS WITH JAMOVI">
<meta property="og:image:height" content="1458">
<meta property="og:image:width" content="1483">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-Categorical-data-analysis.html">Statistical tools</a></li><li class="breadcrumb-item"><a href="./12-Correlation-and-linear-regression.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Correlation and linear regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">LEARNING STATISTICS WITH JAMOVI</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://www.openbookpublishers.com/books/10.11647/obp.0333" rel="" title="Order hard copy" class="quarto-navigation-tool px-1" aria-label="Order hard copy"><i class="bi bi-book"></i></a>
    <a href="https://github.com/davidfoxcroft/lsj-book/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./LEARNING-STATISTICS-WITH-JAMOVI.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Beginnings</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Why-do-we-learn-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Why do we learn statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-A-brief-introduction-to-research-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">A brief introduction to research design</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">An introduction to jamovi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Getting-started-with-jamovi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Getting started with jamovi</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Working with data</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-Descriptive-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Drawing-graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Drawing graphs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Pragmatic-matters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pragmatic matters</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Statistical theory</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Prelude-Part-IV.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prelude</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Introduction-to-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Introduction to probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-Estimating-unknown-quantities-from-a-sample.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Estimating unknown quantities from a sample</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-Hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis testing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Statistical tools</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Categorical-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Categorical data analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Comparing-two-means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Comparing two means</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Correlation-and-linear-regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Correlation and linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Comparing-several-means-one-way-ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Comparing several means (one-way ANOVA)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-Factorial-ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Factorial ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-Factor-Analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Factor Analysis</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">Endings, alternatives and prospects</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-Bayesian-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Bayesian statistics</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Epilogue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Epilogue</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./obp-team.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the team</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#correlations" id="toc-correlations" class="nav-link active" data-scroll-target="#correlations"><span class="header-section-number">12.1</span> Correlations</a>
  <ul class="collapse">
  <li><a href="#the-data" id="toc-the-data" class="nav-link" data-scroll-target="#the-data"><span class="header-section-number">12.1.1</span> The data</a></li>
  <li><a href="#the-strength-and-direction-of-a-relationship" id="toc-the-strength-and-direction-of-a-relationship" class="nav-link" data-scroll-target="#the-strength-and-direction-of-a-relationship"><span class="header-section-number">12.1.2</span> The strength and direction of a relationship</a></li>
  <li><a href="#the-correlation-coefficient" id="toc-the-correlation-coefficient" class="nav-link" data-scroll-target="#the-correlation-coefficient"><span class="header-section-number">12.1.3</span> The correlation coefficient</a></li>
  <li><a href="#calculating-correlations-in-jamovi" id="toc-calculating-correlations-in-jamovi" class="nav-link" data-scroll-target="#calculating-correlations-in-jamovi"><span class="header-section-number">12.1.4</span> Calculating correlations in jamovi</a></li>
  <li><a href="#interpreting-a-correlation" id="toc-interpreting-a-correlation" class="nav-link" data-scroll-target="#interpreting-a-correlation"><span class="header-section-number">12.1.5</span> Interpreting a correlation</a></li>
  <li><a href="#spearmans-rank-correlations" id="toc-spearmans-rank-correlations" class="nav-link" data-scroll-target="#spearmans-rank-correlations"><span class="header-section-number">12.1.6</span> Spearman’s rank correlations</a></li>
  </ul></li>
  <li><a href="#scatterplots" id="toc-scatterplots" class="nav-link" data-scroll-target="#scatterplots"><span class="header-section-number">12.2</span> Scatterplots</a>
  <ul class="collapse">
  <li><a href="#more-elaborate-options" id="toc-more-elaborate-options" class="nav-link" data-scroll-target="#more-elaborate-options"><span class="header-section-number">12.2.1</span> More elaborate options</a></li>
  </ul></li>
  <li><a href="#what-is-a-linear-regression-model" id="toc-what-is-a-linear-regression-model" class="nav-link" data-scroll-target="#what-is-a-linear-regression-model"><span class="header-section-number">12.3</span> What is a linear regression model?</a></li>
  <li><a href="#estimating-a-linear-regression-model" id="toc-estimating-a-linear-regression-model" class="nav-link" data-scroll-target="#estimating-a-linear-regression-model"><span class="header-section-number">12.4</span> Estimating a linear regression model</a>
  <ul class="collapse">
  <li><a href="#linear-regression-in-jamovi" id="toc-linear-regression-in-jamovi" class="nav-link" data-scroll-target="#linear-regression-in-jamovi"><span class="header-section-number">12.4.1</span> Linear regression in jamovi</a></li>
  <li><a href="#interpreting-the-estimated-model" id="toc-interpreting-the-estimated-model" class="nav-link" data-scroll-target="#interpreting-the-estimated-model"><span class="header-section-number">12.4.2</span> Interpreting the estimated model</a></li>
  </ul></li>
  <li><a href="#multiple-linear-regression" id="toc-multiple-linear-regression" class="nav-link" data-scroll-target="#multiple-linear-regression"><span class="header-section-number">12.5</span> Multiple linear regression</a>
  <ul class="collapse">
  <li><a href="#doing-it-in-jamovi" id="toc-doing-it-in-jamovi" class="nav-link" data-scroll-target="#doing-it-in-jamovi"><span class="header-section-number">12.5.1</span> Doing it in jamovi</a></li>
  </ul></li>
  <li><a href="#quantifying-the-fit-of-the-regression-model" id="toc-quantifying-the-fit-of-the-regression-model" class="nav-link" data-scroll-target="#quantifying-the-fit-of-the-regression-model"><span class="header-section-number">12.6</span> Quantifying the fit of the regression model</a>
  <ul class="collapse">
  <li><a href="#sec-The-R2-value" id="toc-sec-The-R2-value" class="nav-link" data-scroll-target="#sec-The-R2-value"><span class="header-section-number">12.6.1</span> The <span class="math inline">\(R^2\)</span> value</a></li>
  <li><a href="#the-relationship-between-regression-and-correlation" id="toc-the-relationship-between-regression-and-correlation" class="nav-link" data-scroll-target="#the-relationship-between-regression-and-correlation"><span class="header-section-number">12.6.2</span> The relationship between regression and correlation</a></li>
  <li><a href="#the-adjusted-r2-value" id="toc-the-adjusted-r2-value" class="nav-link" data-scroll-target="#the-adjusted-r2-value"><span class="header-section-number">12.6.3</span> The adjusted <span class="math inline">\(R^2\)</span> value</a></li>
  </ul></li>
  <li><a href="#hypothesis-tests-for-regression-models" id="toc-hypothesis-tests-for-regression-models" class="nav-link" data-scroll-target="#hypothesis-tests-for-regression-models"><span class="header-section-number">12.7</span> Hypothesis tests for regression models</a>
  <ul class="collapse">
  <li><a href="#testing-the-model-as-a-whole" id="toc-testing-the-model-as-a-whole" class="nav-link" data-scroll-target="#testing-the-model-as-a-whole"><span class="header-section-number">12.7.1</span> Testing the model as a whole</a></li>
  <li><a href="#tests-for-individual-coefficients" id="toc-tests-for-individual-coefficients" class="nav-link" data-scroll-target="#tests-for-individual-coefficients"><span class="header-section-number">12.7.2</span> Tests for individual coefficients</a></li>
  <li><a href="#running-the-hypothesis-tests-in-jamovi" id="toc-running-the-hypothesis-tests-in-jamovi" class="nav-link" data-scroll-target="#running-the-hypothesis-tests-in-jamovi"><span class="header-section-number">12.7.3</span> Running the hypothesis tests in jamovi</a></li>
  </ul></li>
  <li><a href="#regarding-regression-coefficients" id="toc-regarding-regression-coefficients" class="nav-link" data-scroll-target="#regarding-regression-coefficients"><span class="header-section-number">12.8</span> Regarding regression coefficients</a>
  <ul class="collapse">
  <li><a href="#confidence-intervals-for-the-coefficients" id="toc-confidence-intervals-for-the-coefficients" class="nav-link" data-scroll-target="#confidence-intervals-for-the-coefficients"><span class="header-section-number">12.8.1</span> Confidence intervals for the coefficients</a></li>
  <li><a href="#calculating-standardised-regression-coefficients" id="toc-calculating-standardised-regression-coefficients" class="nav-link" data-scroll-target="#calculating-standardised-regression-coefficients"><span class="header-section-number">12.8.2</span> Calculating standardised regression coefficients</a></li>
  </ul></li>
  <li><a href="#assumptions-of-regression" id="toc-assumptions-of-regression" class="nav-link" data-scroll-target="#assumptions-of-regression"><span class="header-section-number">12.9</span> Assumptions of regression</a></li>
  <li><a href="#sec-Model-checking" id="toc-sec-Model-checking" class="nav-link" data-scroll-target="#sec-Model-checking"><span class="header-section-number">12.10</span> Model checking</a>
  <ul class="collapse">
  <li><a href="#three-kinds-of-residuals" id="toc-three-kinds-of-residuals" class="nav-link" data-scroll-target="#three-kinds-of-residuals"><span class="header-section-number">12.10.1</span> Three kinds of residuals</a></li>
  <li><a href="#checking-the-linearity-of-the-relationship" id="toc-checking-the-linearity-of-the-relationship" class="nav-link" data-scroll-target="#checking-the-linearity-of-the-relationship"><span class="header-section-number">12.10.2</span> Checking the linearity of the relationship</a></li>
  <li><a href="#sec-Checking-the-normality-of-the-residuals" id="toc-sec-Checking-the-normality-of-the-residuals" class="nav-link" data-scroll-target="#sec-Checking-the-normality-of-the-residuals"><span class="header-section-number">12.10.3</span> Checking the normality of the residuals</a></li>
  <li><a href="#checking-equality-of-variance" id="toc-checking-equality-of-variance" class="nav-link" data-scroll-target="#checking-equality-of-variance"><span class="header-section-number">12.10.4</span> Checking equality of variance</a></li>
  <li><a href="#checking-for-collinearity" id="toc-checking-for-collinearity" class="nav-link" data-scroll-target="#checking-for-collinearity"><span class="header-section-number">12.10.5</span> Checking for collinearity</a></li>
  <li><a href="#outliers-and-anomalous-data" id="toc-outliers-and-anomalous-data" class="nav-link" data-scroll-target="#outliers-and-anomalous-data"><span class="header-section-number">12.10.6</span> Outliers and anomalous data</a></li>
  </ul></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">12.11</span> Model selection</a>
  <ul class="collapse">
  <li><a href="#backward-elimination" id="toc-backward-elimination" class="nav-link" data-scroll-target="#backward-elimination"><span class="header-section-number">12.11.1</span> Backward elimination</a></li>
  <li><a href="#forward-selection" id="toc-forward-selection" class="nav-link" data-scroll-target="#forward-selection"><span class="header-section-number">12.11.2</span> Forward selection</a></li>
  <li><a href="#a-caveat" id="toc-a-caveat" class="nav-link" data-scroll-target="#a-caveat"><span class="header-section-number">12.11.3</span> A caveat</a></li>
  <li><a href="#comparing-two-regression-models" id="toc-comparing-two-regression-models" class="nav-link" data-scroll-target="#comparing-two-regression-models"><span class="header-section-number">12.11.4</span> Comparing two regression models</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">12.12</span> Summary</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/davidfoxcroft/lsj-book/edit/main/12-Correlation-and-linear-regression.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/davidfoxcroft/lsj-book/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7"><span id="sec-Correlation-and-linear-regression" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Correlation and linear regression</span></span></h1>

</header>

<p>The goal in this chapter is to introduce <strong>correlation</strong> and <strong>linear regression</strong>. These are the standard tools that statisticians rely on when analysing the relationship between continuous predictors and continuous outcomes.</p>
<section id="correlations" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="correlations"><span class="header-section-number">12.1</span> Correlations</h2>
<p>In this section we’ll talk about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the <strong>correlation</strong> between variables. But first, we need some data (<a href="#tbl-tab12-1">Table&nbsp;<span>12.1</span></a>).</p>
<section id="the-data" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="the-data"><span class="header-section-number">12.1.1</span> The data</h3>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab12-1" class="anchored">

<table class="huxtable" data-quarto-disable-processing="true" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; width: 90%; margin-left: auto; margin-right: auto;  " id="tab:tbl-tab12-1"><caption>Table&nbsp;12.1.  <p>Data for correlation analysis – descriptive statistics for the
<em>parenthood</em> data</p> </caption>
<colgroup><col style="width: 20%"><col><col><col><col><col><col></colgroup><tbody><tr>
<th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: bold;">variable</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold;">min</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold;">max</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold;">mean</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold;">median</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold;">std. dev</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: bold;">IQR</th></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">Dani's grumpiness</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">41</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">91</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">63.71</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">62</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">10.05</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">14</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">Dani's hours slept</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">4.84</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">9.00</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">6.97</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">7.03</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">1.02</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">1.45</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">Dani's son's hours slept</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">3.25</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">12.07</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">8.05</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">7.95</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">2.07</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">3.21</td></tr>
</tbody></table>

</div>
</div>
</div>
<p>Let’s turn to a topic close to every parent’s heart: sleep. The data set we’ll use is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to <span class="math inline">\(100\)</span> (grumpy as a very, very grumpy old man or woman). And lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for <span class="math inline">\(100\)</span> days. And, being a nerd, I’ve saved the data as a file called <em>parenthood.csv</em>. If we load the data we can see that the file contains four variables dani.sleep, baby.sleep, dani.grump and day. Note that when you first load this data set jamovi may not have guessed the data type for each variable correctly, in which case you should fix it: dani.sleep, baby.sleep, dani.grump and day can be specified as continuous variables, and ID is a nominal(integer) variable.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Next, I’ll take a look at some basic descriptive statistics and, to give a graphical depiction of what each of the three interesting variables looks like, <a href="#fig-fig12-1">Figure&nbsp;<span>12.1</span></a> plots histograms. One thing to note: just because jamovi can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table 12.1.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s pretty standard.</p>
<div id="fig-fig12-1" class="quarto-layout-panel" width="100%">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-fig12-1a" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-1a.png" class="img-fluid figure-img" alt="A histogram displaying the density distribution of baby sleep durations. The bars range from about 3.5 to 12.5 hours of sleep. There is a higher concentration of data points between 7 and 10 hours. The y-axis is labeled 'density' and the x-axis is labeled 'baby.sleep'." data-ref-parent="fig-fig12-1"></p>
<figcaption class="figure-caption">(a)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-fig12-1b" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-1b.png" class="img-fluid figure-img" alt="A histogram with labelled axes 'density' on the y-axis and 'dan.sleep' on the x-axis. The histogram has bars of varying heights, peaking between the values 6 and 7 on the x-axis. The bars are shaded in light blue. The background is black." data-ref-parent="fig-fig12-1"></p>
<figcaption class="figure-caption">(b)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 33.3%;justify-content: center;">
<div id="fig-fig12-1c" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-1c.png" class="img-fluid figure-img" alt="A histogram displays the density of data points for the variable 'dan.grump'. The x-axis ranges from 40 to 90, and the y-axis is labeled 'density'. The bars show a distribution peaking around 60 and tapering off towards the lower and higher ends." data-ref-parent="fig-fig12-1"></p>
<figcaption class="figure-caption">(c)</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;12.1. Histograms from jamovi for the three interesting variables in the <em>parenthood</em> data set</figcaption><p></p>
</figure>
</div>
</section>
<section id="the-strength-and-direction-of-a-relationship" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="the-strength-and-direction-of-a-relationship"><span class="header-section-number">12.1.2</span> The strength and direction of a relationship</h3>
<p>We can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between baby.sleep and dani.grump (<a href="#fig-fig12-2a">Figure&nbsp;<span>12.2 (a)</span></a>), left, with that between dani.sleep and dani.grump (<a href="#fig-fig12-2b">Figure&nbsp;<span>12.2 (b)</span></a>), right. When looking at these two plots side by side, it’s clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between dani.sleep and dani.grump is stronger than the relationship between baby.sleep and dani.grump. The plot on the right is “neater” than the one on the left. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be more helpful to know how many hours I slept.</p>
<div id="fig-fig12-2" class="quarto-layout-panel" width="90%" data-fig-pos="H">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-fig12-2a" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-2a.png" class="img-fluid figure-img" alt="Scatter plot showing the relationship between 'Baby sleep (hours)' on the x-axis and 'My grumpiness' on the y-axis. The trend indicates that as the baby sleeps more, grumpiness decreases." data-ref-parent="fig-fig12-2"></p>
<figcaption class="figure-caption">(a)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-fig12-2b" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-2b.png" class="img-fluid figure-img" alt="A scatter plot titled 'My grumpiness vs. My sleep (hours)'. The x-axis is labeled 'My sleep (hours)' ranging from 5 to 9, and the y-axis is labeled 'My grumpiness' ranging from 40 to 90. The data points show a negative correlation between sleep and grumpiness." data-ref-parent="fig-fig12-2"></p>
<figcaption class="figure-caption">(b)</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;12.2. Scatterplots from jamovi showing the relationship between baby.sleep and dani.grump (left) and the relationship between dani.sleep and dani.grump (right)</figcaption><p></p>
</figure>
</div>
<p>In contrast, let’s consider the two scatterplots shown in <a href="#fig-fig12-3">Figure&nbsp;<span>12.3</span></a>. If we compare the scatterplot of “baby.sleep v dani.grump” (left) to the scatterplot of “baby.sleep v dani.sleep” (right), the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, right-hand side), but if he sleeps more then I get less grumpy (negative relationship, left-hand side).</p>
<div id="fig-fig12-3" class="quarto-layout-panel" width="90%" data-fig-pos="h!">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-fig12-3a" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-2a.png" class="img-fluid figure-img" alt="Scatter plot showing the relationship between 'Baby sleep (hours)' on the x-axis and 'My grumpiness' on the y-axis. The trend indicates that as the baby sleeps more, grumpiness decreases." data-ref-parent="fig-fig12-3"></p>
<figcaption class="figure-caption">(a)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-fig12-3b" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-3b.png" class="img-fluid figure-img" alt="Scatter plot showing the relationship between 'Baby sleep (hours)' on the x-axis and 'My sleep (hours)' on the y-axis. Data points indicate that as the baby's sleep increases from 5 to 12.5 hours, the parent's sleep also tends to increase from 5 to 9 hours." data-ref-parent="fig-fig12-3"></p>
<figcaption class="figure-caption">(b)</figcaption>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;12.3. Scatterplots from jamovi showing the relationship between baby.sleep and dani.grump (left), as compared to the relationship between baby.sleep and dani.sleep (right)</figcaption><p></p>
</figure>
</div>
</section>
<section id="the-correlation-coefficient" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="the-correlation-coefficient"><span class="header-section-number">12.1.3</span> The correlation coefficient</h3>
<p>We can make these ideas a bit more explicit by introducing the idea of a <strong>correlation coefficient</strong> (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted as r. The correlation coefficient between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (sometimes denoted <span class="math inline">\(r_{XY}\)</span> ), which we’ll define more precisely in the next section, is a measure that varies from -1 to 1. When <span class="math inline">\(r = -1\)</span> it means that we have a perfect negative relationship, and when <span class="math inline">\(r = 1\)</span> it means we have a perfect positive relationship. When <span class="math inline">\(r = 0\)</span>, there’s no relationship at all. If you look at <a href="#fig-fig12-4">Figure&nbsp;<span>12.4</span></a>, you can see several plots showing what different correlations look like.</p>
<p>[Additional technical detail<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>]</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-4" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-4-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Scatter plots are shown with varying degrees of positive and negative correlations. Positive correlations:- 0, 0.33, 0.66, and 1. Negative correlations:- 0, -0.33, -0.66, and -1. 0 shows no correlation, and 1, -1 show perfect correlations"></p>
<figcaption class="figure-caption">Figure&nbsp;12.4. Illustration of the effect of varying the strength and direction of a correlation. In the left-hand column, the correlations are <span class="math inline">\(0, .33, .66\)</span> and <span class="math inline">\(1\)</span>. In the right-hand column, the correlations are <span class="math inline">\(0, -.33, -.66\)</span> and <span class="math inline">\(-1\)</span></figcaption>
</figure>
</div>
</div>
</div>
<p>By standardising the covariance, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of r are on a meaningful scale: r = 1 implies a perfect positive relationship and <span class="math inline">\(r = -1\)</span> implies a perfect negative relationship. I’ll expand a little more on this point later, in the section on <a href="#interpreting-a-correlation">Interpreting a correlation</a>. But before I do, let’s look at how to calculate correlations in jamovi.</p>
</section>
<section id="calculating-correlations-in-jamovi" class="level3" data-number="12.1.4">
<h3 data-number="12.1.4" class="anchored" data-anchor-id="calculating-correlations-in-jamovi"><span class="header-section-number">12.1.4</span> Calculating correlations in jamovi</h3>
<p>Calculating correlations in jamovi can be done by clicking on the ‘Regression’ – ‘Correlation Matrix’ button. Transfer all four continuous variables across into the box on the right to get the output in <a href="#fig-fig12-5">Figure&nbsp;<span>12.5</span></a>.</p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-5" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-5.png" class="img-fluid figure-img" style="width:90.0%" alt="A jamovi screenshot showing data analysis tools. The main focus is on a Correlation Matrix section with variable labels such as &quot;dan sleep,&quot; &quot;baby sleep,&quot; and &quot;dan grump.&quot; Options for Pearson correlation, significance reporting, and confidence intervals are visible"></p>
<figcaption class="figure-caption">Figure&nbsp;12.5. Correlations between variables in the <em>parenthood.csv</em> file</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="interpreting-a-correlation" class="level3" data-number="12.1.5">
<h3 data-number="12.1.5" class="anchored" data-anchor-id="interpreting-a-correlation"><span class="header-section-number">12.1.5</span> Interpreting a correlation</h3>
<p>Naturally, in real life you don’t see many correlations of <span class="math inline">\(1\)</span>. So how should you interpret a correlation of, say, r = <span class="math inline">\(.4\)</span>? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than <span class="math inline">\(.95\)</span> is completely useless (I think he was exaggerating, even for engineering). On the other hand, there are real cases, even in psychology, where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least <span class="math inline">\(.9\)</span> really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above <span class="math inline">\(.3\)</span> you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in <a href="#tbl-tab12-2">Table&nbsp;<span>12.2</span></a> is pretty typical.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab12-2" class="anchored">

<table class="huxtable" data-quarto-disable-processing="true" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; width: 90%; margin-left: auto; margin-right: auto;  " id="tab:tbl-tab12-2"><caption>Table&nbsp;12.2.  <p>A rough guide to interpreting correlations</p> </caption>
<colgroup><col><col><col></colgroup><tbody><tr>
<th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: bold; font-size: 10pt;">Correlation</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold; font-size: 10pt;">Strength</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: bold; font-size: 10pt;">Direction</th></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">-1.00 to -0.90</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Very strong</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Negative</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">-0.90 to -0.70</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Strong</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Negative</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">-0.70 to -0.40</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Moderate</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Negative</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">-0.40 to -0.20</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Weak</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Negative</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">-0.20 to 0.00</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Negligible</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Negative</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">0.00 to 0.20</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Negligible</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Positive</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">0.20 to 0.40</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Weak</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Positive</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">0.40 to 0.70</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Moderate</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Positive</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">0.70 to 0.90</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Strong</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Positive</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal; font-size: 10pt;">0.90 to 1.00</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Very strong</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.8pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal; font-size: 10pt;">Positive</td></tr>
<tr>
<td colspan="3" style="vertical-align: top; text-align: left; white-space: normal; border-style: solid solid solid solid; border-width: 0.8pt 0pt 0pt 0pt;    padding: 6pt 6pt 6pt 6pt; font-weight: normal; font-style: italic; font-size: 8pt;">Note that I say a rough guide. There aren't hard and fast rules for what counts as strong or weak relationships. It depends on the context</td></tr>
</tbody></table>

</div>
</div>
</div>
<p>However, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” <span class="citation" data-cites="Anscombe1973">(<a href="References.html#ref-Anscombe1973" role="doc-biblioref">Anscombe, 1973</a>)</span>, a collection of four data sets. Each data set has two variables, an <span class="math inline">\(X\)</span> and a <span class="math inline">\(Y\)</span>. For all four data sets the mean value for <span class="math inline">\(X\)</span> is <span class="math inline">\(9\)</span> and the mean for <span class="math inline">\(Y\)</span> is <span class="math inline">\(7.5\)</span>. The standard deviations for all <span class="math inline">\(X\)</span> variables are almost identical, as are those for the Y variables. And in each case the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math inline">\(r = 0.816\)</span>. You can verify this yourself, since I happen to have saved it in a file called <em>anscombe.csv</em>.</p>
<p>You’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of <span class="math inline">\(X\)</span> against <span class="math inline">\(Y\)</span> for all four variables, as shown in <a href="#fig-fig12-6">Figure&nbsp;<span>12.6</span></a>, we see that all four of these are spectacularly different to each other. The lesson here, which so very many people seem to forget in real life, is “always graph your raw data” (see <a href="05-Drawing-graphs.html"><span>Chapter&nbsp;5</span></a>).</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-6" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-6-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Four scatter plots with points showing the same positive correlation between x and y. The four plots show linear, non-linear, outlier, and leverage types of relationships."></p>
<figcaption class="figure-caption">Figure&nbsp;12.6. Anscombe’s quartet scatterplots. All four of these data sets have a Pearson correlation of <span class="math inline">\(r\)</span> = .816, but they are qualitatively different from one another</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="spearmans-rank-correlations" class="level3" data-number="12.1.6">
<h3 data-number="12.1.6" class="anchored" data-anchor-id="spearmans-rank-correlations"><span class="header-section-number">12.1.6</span> Spearman’s rank correlations</h3>
<p>The Pearson correlation coefficient is pretty useful, but it does have shortcomings. One issue stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculate. Sometimes though, it isn’t.</p>
<p>One very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable <span class="math inline">\(X\)</span> really is reflected in an increase in another variable Y , but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put zero effort (<span class="math inline">\(X\)</span>) into learning a subject then you should expect a grade of <span class="math inline">\(0\%\)</span> (<span class="math inline">\(Y\)</span>). However, a little bit of effort will cause a massive improvement. Just turning up to lectures means that you learn a fair bit, and if you just turn up to classes and scribble a few things down your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of <span class="math inline">\(90\%\)</span> than it takes to get a grade of <span class="math inline">\(55\%\)</span>. What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.</p>
<p>To illustrate, consider the data plotted in <a href="#fig-fig12-7">Figure&nbsp;<span>12.7</span></a>, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this (highly fictitious) data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. If we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received, with a correlation coefficient of <span class="math inline">\(0.91\)</span>. However, this doesn’t actually capture the observation that increasing hours worked always increases the grade. There’s a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect <strong>ordinal relationship</strong> here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of <span class="math inline">\(r = .91\)</span> says at all.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-7" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-7.png" class="img-fluid figure-img" style="width:80.0%" alt="A scatter plot with a trend line. The x-axis is labeled &quot;hours&quot; and the y-axis is labeled &quot;grade.&quot; Dots represent data points, showing a positive correlation between hours studied and grade achieved, with the trend line ascending from the bottom-left to top-right"></p>
<figcaption class="figure-caption">Figure&nbsp;12.7. jamovi plot showing the relationship between hours worked and grade received for a toy data set consisting of only 10 students (each dot corresponds to one student). The line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of <span class="math inline">\(r = .91\)</span>. However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables. In this toy example, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of <span class="math inline">\(\rho = 1\)</span>. With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved</figcaption>
</figure>
</div>
</div>
</div>
<p>How should we address this? Actually, it’s really easy. If we’re looking for ordinal relationships all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all <span class="math inline">\(10\)</span> of our students in order of hours worked. That is, student <span class="math inline">\(1\)</span> did the least work out of anyone (<span class="math inline">\(2\)</span> hours) so they get the lowest rank (rank = <span class="math inline">\(1\)</span>). Student <span class="math inline">\(4\)</span> was the next laziest, putting in only <span class="math inline">\(6\)</span> hours of work over the whole semester, so they get the next lowest rank (rank = <span class="math inline">\(2\)</span>). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = <span class="math inline">\(1\)</span>” to mean “top rank” rather than “bottom rank”. So be careful, you can rank “from smallest value to largest value” (i.e., small equals rank <span class="math inline">\(1\)</span>) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, but as it’s really easy to forget which way you set things up you have to put a bit of effort into remembering!</p>
<p>Okay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward <a href="#tbl-tab12-3">Table&nbsp;<span>12.3</span></a>.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab12-3" class="anchored">

<table class="huxtable" data-quarto-disable-processing="true" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; width: 90%; margin-left: auto; margin-right: auto;  " id="tab:tbl-tab12-3"><caption>Table&nbsp;12.3.  <p>Students ranked in terms of effort and reward</p> </caption>
<colgroup><col><col><col></colgroup><tbody><tr>
<th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: bold;"></th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold;">rank (hours worked)</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: bold;">rank (grade received)</th></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 1</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">1</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">1</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 2</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">10</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">10</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 3</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">6</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">6</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 4</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">2</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">2</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 5</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">3</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">3</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 6</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">5</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">5</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 7</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">4</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">4</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 8</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">8</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">8</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 9</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">7</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">7</td></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">student 10</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">9</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0pt 0pt 0.4pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">9</td></tr>
</tbody></table>

</div>
</div>
</div>
<p>Hmm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. As the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship, with a correlation of 1.0.</p>
<p>What we’ve just re-invented is <strong>Spearman’s rank order correlation</strong>, usually denoted <span class="math inline">\(\rho\)</span> to distinguish it from the Pearson correlation r. We can calculate Spearman’s <span class="math inline">\(\rho\)</span> using jamovi simply by clicking the ‘Spearman’ check box in the ‘Correlation Matrix’ screen.</p>
</section>
</section>
<section id="scatterplots" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="scatterplots"><span class="header-section-number">12.2</span> Scatterplots</h2>
<p><strong>Scatterplots</strong> are a simple but effective tool for visualising the relationship between two variables, like we saw with the figures in the section on <a href="#correlations">Correlations</a>. It’s this latter application that we usually have in mind when we use the term “scatterplot”. In this kind of plot each observation corresponds to one dot. The horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don’t really have a clear opinion about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let’s look at how to draw scatterplots in jamovi, using the same <em>parenthood</em>data set (i.e.&nbsp;<em>parenthood.csv</em>) that I used when introducing correlations.</p>
<p>Suppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dani.sleep) and how grumpy I am the next day (dani.grump). There are two different ways in which we can use jamovi to get the plot that we’re after. The first way is to use the ‘Plot’ option under the ‘Regression’ – ‘Correlation Matrix’ button, giving us the output shown in <a href="#fig-fig12-8">Figure&nbsp;<span>12.8</span></a>. Note that jamovi draws a line through the points, we’ll come onto this a bit later in the section on <a href="#what-is-a-linear-regression-model">What is a linear regression model?</a>. Plotting a scatterplot in this way also allows you to specify ‘Densities for variables’ and this option adds a density curve showing how the data in each variable is distributed.</p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-8" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-8.png" class="img-fluid figure-img" style="width:80.0%" alt="A jamovi screenshot displaying a correlation matrix analysis. The analysis examines the variables &quot;dan.sleep&quot; and &quot;dan.grump&quot; with options for Pearson and Spearman correlations, covariance, and testing hypotheses. A scatter plot and density plots of the variables are shown."></p>
<figcaption class="figure-caption">Figure&nbsp;12.8. Scatterplot via the ‘Correlation Matrix’ command in jamovi</figcaption>
</figure>
</div>
</div>
</div>
<p>The second way do to it is to use one of the jamovi add-on modules. This module is called ‘scatr’ and you can install it by clicking on the large ‘<span class="math inline">\(+\)</span>’ icon in the top right of the jamovi screen, opening the jamovi library, scrolling down until you find ‘scatr’ and clicking ‘install’. When you have done this, you will find a new ‘Scatterplot’ command available under the ‘Exploration’ button. This plot is a bit different than the first way, see <a href="#fig-fig12-9">Figure&nbsp;<span>12.9</span></a>, but the important information is the same.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-9" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-9.png" class="img-fluid figure-img" style="width:80.0%" alt="A scatter plot showing the relationship between 'dan.grump' and 'dan.sleep'. The x-axis represents 'dan.sleep' ranging from 5 to 9, and the y-axis represents 'dan.grump' ranging from 40 to 90. Data points trend downwards, indicating a negative correlation"></p>
<figcaption class="figure-caption">Figure&nbsp;12.9. Scatterplot via the ‘scatr’ add-on module in jamovi</figcaption>
</figure>
</div>
</div>
</div>
<section id="more-elaborate-options" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="more-elaborate-options"><span class="header-section-number">12.2.1</span> More elaborate options</h3>
<p>Often you will want to look at the relationships between several variables at once, using a <strong>scatterplot matrix</strong> (in jamovi via the ‘Correlation Matrix’ – ‘Plot’ command). Just add another variable, for example baby.sleep to the list of variables to be correlated, and jamovi will create a scatterplot matrix for you, just like the one in <a href="#fig-fig12-10">Figure&nbsp;<span>12.10</span></a>.</p>
</section>
</section>
<section id="what-is-a-linear-regression-model" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="what-is-a-linear-regression-model"><span class="header-section-number">12.3</span> What is a linear regression model?</h2>
<p>Stripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see <a href="#correlations">Correlations</a>), but they are actually much more powerful tools. We’ll return to the <em>parenthood.csv</em> file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I’m not getting enough sleep. We drew a scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in <a href="#fig-fig12-9">Figure&nbsp;<span>12.9</span></a>, and as we saw that this corresponded to a correlation of <span class="math inline">\(r = -.90\)</span>, but what we find ourselves secretly imagining is something that looks closer to <a href="#fig-fig12-11">Figure&nbsp;<span>12.11</span></a>(a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a <strong>regression line</strong>. Notice that, since we’re not idiots, the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in <a href="#fig-fig12-11">Figure&nbsp;<span>12.11</span></a>(b).</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-10" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-10.png" class="img-fluid figure-img" style="width:80.0%" alt="A series of scatter plots arranged in a matrix format showing correlations between variables:- dan.sleep, dan.grump, and baby.sleep. Each plot has points and trend lines with grey confidence intervals. The correlation coefficients are labeled on the plots"></p>
<figcaption class="figure-caption">Figure&nbsp;12.10. A matrix of scatterplots produced using jamovi</figcaption>
</figure>
</div>
</div>
</div>
<p>This is not highly surprising. The line that I’ve drawn in <a href="#fig-fig12-11">Figure&nbsp;<span>12.11</span></a>(b) doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this:</p>
<p><span class="math display">\[y=a+bx\]</span></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-11" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-11-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Two scatter plots compare the fitting of regression lines to the relationship between sleep hours and grumpiness. The left plot shows a strong negative correlation with a steep regression line; the right plot shows a weaker fit and a flatter regression line."></p>
<figcaption class="figure-caption">Figure&nbsp;12.11. Panel (a) shows the sleep-grumpiness scatterplot from <a href="#fig-fig12-9">Figure&nbsp;<span>12.9</span></a> with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel (b) shows the same data, but with a very poor choice of regression line drawn over the top</figcaption>
</figure>
</div>
</div>
</div>
<p>The two variables are <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and we have two coefficients, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> The coefficient <span class="math inline">\(a\)</span> represents the y-intercept of the line, and coefficient <span class="math inline">\(b\)</span> represents the slope of the line. The intercept is interpreted as “the value of y that you get when <span class="math inline">\(x = 0\)</span>”. Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. We use the exact same formula for a regression line. If <span class="math inline">\(Y\)</span> is the outcome variable (the DV) and <span class="math inline">\(X\)</span> is the predictor variable (the <span class="math inline">\(IV\)</span>), then the formula that describes our regression is written like this:</p>
<p><span class="math display">\[\hat{Y}_i=b_0+b_1X_i\]</span></p>
<p>Hmm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> rather than just plain old <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> . This is because we want to remember that we’re dealing with actual data. In this equation, <span class="math inline">\(X_i\)</span> is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and <span class="math inline">\(Y_i\)</span> is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote <span class="math inline">\(\hat{Y}_i\)</span> and not <span class="math inline">\(Y_i\)</span> . This is because we want to make the distinction between the actual data <span class="math inline">\(Y_i\)</span>, and the estimate <span class="math inline">\(\hat{Y}_i\)</span> (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and <span class="math inline">\(b\)</span> to <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>. That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose b, but that’s what they did. In any case <span class="math inline">\(b_0\)</span> always refers to the intercept term, and <span class="math inline">\(b_1\)</span> refers to the slope.</p>
<p>Excellent, excellent. Next, I can’t help but notice that, regardless of whether we’re talking about the good regression line or the bad one, the data don’t fall perfectly on the line. Or, to say it another way, the data <span class="math inline">\(Y_i\)</span> are not identical to the predictions of the regression model <span class="math inline">\(\hat{Y}_i\)</span>. Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a residual, and we’ll refer to it as <span class="math inline">\(\epsilon_i\)</span>.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> Written using mathematics, the residuals are defined as</p>
</section>
<section id="estimating-a-linear-regression-model" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="estimating-a-linear-regression-model"><span class="header-section-number">12.4</span> Estimating a linear regression model</h2>
<p>Okay, now let’s redraw our pictures but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in <a href="#fig-fig12-12">Figure&nbsp;<span>12.12</span></a>(a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at <a href="#fig-fig12-12">Figure&nbsp;<span>12.12</span></a>(b). Hmm. Maybe what we “want” in a regression model is <em>small</em> residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:</p>
<blockquote class="blockquote">
<p>The estimated regression coefficients, <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span>, are those that minimise the sum of the squared residuals, which we could either write as <span class="math inline">\(\sum_i (Y_i - \hat{Y}_i)^2\)</span> or as <span class="math inline">\(\sum_i \epsilon_i^2\)</span>.</p>
</blockquote>
<p>Yes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are estimates (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span> rather than <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>. Finally, I should also note that, since there’s actually more than one way to estimate a regression model, the more technical name for this estimation process is <strong>ordinary least squares (OLS) regression</strong>.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-12" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-12-1.png" class="img-fluid figure-img" style="width:80.0%" alt="Two scatter plots side by side, both displaying &quot;Grumpiness vs. Sleep&quot; with sleep hours (x-axis) ranging from 0 to 9 and grumpiness (y-axis) ranging from 0 to 100. Each plot has a blue regression line showing a negative correlation. The left plot is labeled &quot;Close tc&quot; and the right plot &quot;Distant tc&quot;"></p>
<figcaption class="figure-caption">Figure&nbsp;12.12. A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data</figcaption>
</figure>
</div>
</div>
</div>
<p>At this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, <span class="math inline">\(\hat{b}_0\)</span> and <span class="math inline">\(\hat{b}_1\)</span>. The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn’t help you understand the logic of regression.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> This time I’m going to let you off the hook. Instead of showing you the long and tedious way first and then “revealing” the wonderful shortcut that jamovi provides, let’s cut straight to the chase and just use jamovi to do all the heavy lifting.</p>
<section id="linear-regression-in-jamovi" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="linear-regression-in-jamovi"><span class="header-section-number">12.4.1</span> Linear regression in jamovi</h3>
<p>To run my linear regression, open up the ‘Regression’ – ‘Linear Regression’ analysis in jamovi, using the <em>parenthood.csv</em> data file. Then specify dani.grump as the ‘Dependent Variable’ and dani.sleep as the variable entered in the ‘Covariates’ box. This gives the results shown in <a href="#fig-fig12-13">Figure&nbsp;<span>12.13</span></a>, showing an intercept <span class="math inline">\(\hat{b}_0 = 125.96\)</span> and the slope <span class="math inline">\(\hat{b}_1 = -8.94\)</span>. In other words, the best fitting regression line that I plotted in <a href="#fig-fig12-12">Figure&nbsp;<span>12.12</span></a> has this formula:</p>
<p><span class="math display">\[\hat{Y}_i=125.96+(-8.94 X_i)\]</span></p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-13" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-13.png" class="img-fluid figure-img" style="width:80.0%" alt="A jamovi screenshot showing the interface for conducting linear regression analysis. The interface includes sections for selecting variables, setting covariates and factors, and viewing results such as model fit measures and coefficients."></p>
<figcaption class="figure-caption">Figure&nbsp;12.13. A jamovi screenshot showing a simple linear regression analysis</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="interpreting-the-estimated-model" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="interpreting-the-estimated-model"><span class="header-section-number">12.4.2</span> Interpreting the estimated model</h3>
<p>The most important thing to be able to understand is how to interpret these coefficients. Let’s start with <span class="math inline">\(\hat{b}_1\)</span>, the slope. If we remember the definition of the slope, a regression coefficient of <span class="math inline">\(\hat{b}_1 = -8.94\)</span> means that if I increase <span class="math inline">\(X_i\)</span> by 1, then I’m decreasing <span class="math inline">\(Y_i\)</span> by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since <span class="math inline">\(\hat{b}_0\)</span> corresponds to “the expected value of <span class="math inline">\(Y_i\)</span> when <span class="math inline">\(X_i\)</span> equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (<span class="math inline">\(X_i = 0\)</span>) then my grumpiness will go off the scale, to an insane value of (<span class="math inline">\(Y_i = 125.96\)</span>). Best to be avoided, I think.</p>
</section>
</section>
<section id="multiple-linear-regression" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="multiple-linear-regression"><span class="header-section-number">12.5</span> Multiple linear regression</h2>
<p>The simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of <strong>multiple regression</strong> model would be in order?</p>
<p>Multiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let <span class="math inline">\(Y_{i}\)</span> refer to my grumpiness on the i-th day. But now we have two $ X $ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let <span class="math inline">\(X_{i1}\)</span> refer to the hours I slept on the i-th day and <span class="math inline">\(X_{i2}\)</span> refers to the hours that the baby slept on that day. If so, then we can write our regression model like this: <span class="math display">\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\epsilon_i\]</span></p>
<p>As before, <span class="math inline">\(\epsilon_i\)</span> is the residual associated with the i-th observation, <span class="math inline">\(\epsilon_i = Y_i - \hat{Y}_i\)</span>. In this model, we now have three coefficients that need to be estimated: <span class="math inline">\(b_0\)</span> is the intercept, <span class="math inline">\(b_1\)</span> is the coefficient associated with my sleep, and <span class="math inline">\(b_2\)</span> is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients <span class="math inline">\(\hat{b}_0\)</span>, <span class="math inline">\(\hat{b}_1\)</span> and <span class="math inline">\(\hat{b}_2\)</span> are those that minimise the sum squared residuals.</p>
<section id="doing-it-in-jamovi" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="doing-it-in-jamovi"><span class="header-section-number">12.5.1</span> Doing it in jamovi</h3>
<p>Multiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the ‘Covariates’ box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I’m so grumpy, then move baby.sleep across into the ‘Covariates’ box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in <a href="#tbl-tab12-4">Table&nbsp;<span>12.4</span></a>.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab12-4" class="anchored">

<table class="huxtable" data-quarto-disable-processing="true" style="border-collapse: collapse; border: 0px; margin-bottom: 2em; margin-top: 2em; width: 90%; margin-left: auto; margin-right: auto;  " id="tab:tbl-tab12-4"><caption>Table&nbsp;12.4.  <p>Adding multiple variables as predictors in a regression</p> </caption>
<colgroup><col><col><col></colgroup><tbody><tr>
<th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: bold;">(Intercept)</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: bold;">dani.sleep</th><th style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: bold;">baby.sleep</th></tr>
<tr>
<td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 0pt; font-weight: normal;">125.97</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 12pt 2pt 12pt; font-weight: normal;">-8.95</td><td style="vertical-align: top; text-align: center; white-space: normal; border-style: solid solid solid solid; border-width: 0.4pt 0pt 0.4pt 0pt;    padding: 2pt 0pt 2pt 12pt; font-weight: normal;">0.01</td></tr>
</tbody></table>

</div>
</div>
</div>
<p>The coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, <a href="#fig-fig12-14">Figure&nbsp;<span>12.14</span></a> shows a 3D plot that plots all three variables, along with the regression model itself.</p>
<p>[Additional technical detail<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>]</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-14" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-14-1.png" class="img-fluid figure-img" style="width:100.0%" alt="A 3D scatter plot shows a correlation between 'baby.sleep' (x-axis), 'dan.sleep' (y-axis), and 'dan.grump' (z-axis) with multiple data points marked by blue dots. A grid plane intersects the data points, visualizing the relationship among the variables"></p>
<figcaption class="figure-caption">Figure&nbsp;12.14. A 3D visualisation of a multiple regression model. There are two predictors in the model, dani.sleep and baby.sleep and the outcome variable is dani.grump. Together, these three variables form a 3D space. Each observation (dot) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients what we’re trying to do is find a plane that is as close to all the blue dots as possible</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="quantifying-the-fit-of-the-regression-model" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="quantifying-the-fit-of-the-regression-model"><span class="header-section-number">12.6</span> Quantifying the fit of the regression model</h2>
<p>So we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the regression.1 model claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction <span class="math inline">\(\hat{Y}_i\)</span> about what my mood is like, but my actual mood is <span class="math inline">\(Y_i\)</span> . If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.</p>
<section id="sec-The-R2-value" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="sec-The-R2-value"><span class="header-section-number">12.6.1</span> The <span class="math inline">\(R^2\)</span> value</h3>
<p>Once again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals:</p>
<p><span class="math display">\[SS_{res}=\sum_i (Y_i-\hat{Y_i})^2\]</span></p>
<p>which we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable:</p>
<p><span class="math display">\[SS_{tot}=\sum_i(Y_i-\bar{Y})^2\]</span></p>
<p>While we’re here, let’s calculate these values ourselves, not by hand though. Let’s use something like Excel or another standard spreadsheet programme. I have done this by opening up the <em>parenthood.csv</em> file in Excel and saving it as parenthood rsquared.xls so that I can work on it. The first thing to do is calculate the <span class="math inline">\(\hat{Y}\)</span> values, and for the simple model that uses only a single predictor we would do the following:</p>
<ol type="1">
<li>Create a new column called ’ Y.pred ’ using the formula ’ = 125.97 + (-8.94 <span class="math inline">\(\times\)</span> dani.sleep) ’.</li>
<li>Calculate the <span class="math inline">\(SS_{resid}\)</span> by creating a new column called ’ (Y-Y.pred)^2 ’ using the formula ’ = (dani.grump - Y.pred)^2 ’.</li>
<li>Then, at the bottom of this column calculate the sum of these values, i.e.&nbsp;’ sum( ( Y-Y.pred)^2 ) ’.</li>
<li>At the bottom of the dani.grump column, calculate the mean value for dani.grump (NB Excel uses the word ’ AVERAGE ’ rather than ’ mean ’ in its function).</li>
<li>Then create a new column, called ’ (Y - mean(Y))^2 ) ’ using the formula ’ = (dani.grump - AVERAGE(dani.grump))^2 ’.</li>
<li>Then, at the bottom of this column calculate the sum of these values, i.e.&nbsp;‘sum( (Y - mean(Y))^2 )’.</li>
<li>Calculate <span class="math inline">\(R^2\)</span> by typing into a blank cell the following: ‘= 1 - (SS(resid) / SS(tot) )’.</li>
</ol>
<p>This gives a value for <span class="math inline">\(R^2\)</span> of ‘0.8161018’. The <span class="math inline">\(R^2\)</span> value, sometimes called the <strong>coefficient of determination</strong><a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> has a simple interpretation: it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. So, in this case the fact that we have obtained <span class="math inline">\(R^2 = .816\)</span> means that the predictor (my.sleep) explains <span class="math inline">\(81.6\%\)</span> of the variance in the outcome (my.grump).</p>
<p>Naturally, you don’t actually need to type all these commands into Excel yourself if you want to obtain the <span class="math inline">\(R^2\)</span> value for your regression model. As we’ll see later on in the section on <a href="#running-the-hypothesis-tests-in-jamovi">Running the hypothesis tests in jamovi</a>, all you need to do is specify this as an option in jamovi. However, let’s put that to one side for the moment. There’s another property of <span class="math inline">\(R^2\)</span> that I want to point out.</p>
</section>
<section id="the-relationship-between-regression-and-correlation" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="the-relationship-between-regression-and-correlation"><span class="header-section-number">12.6.2</span> The relationship between regression and correlation</h3>
<p>At this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol <span class="math inline">\(r\)</span> to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient <span class="math inline">\(r\)</span> and the <span class="math inline">\(R^2\)</span> value from linear regression? Of course there is: the squared correlation <span class="math inline">\(r^2\)</span> is identical to the <span class="math inline">\(R^2\)</span> value for a linear regression with only a single predictor. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.</p>
</section>
<section id="the-adjusted-r2-value" class="level3" data-number="12.6.3">
<h3 data-number="12.6.3" class="anchored" data-anchor-id="the-adjusted-r2-value"><span class="header-section-number">12.6.3</span> The adjusted <span class="math inline">\(R^2\)</span> value</h3>
<p>One final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted <span class="math inline">\(R^2\)</span>”. The motivation behind calculating the adjusted <span class="math inline">\(R^2\)</span> value is the observation that adding more predictors into the model will always cause the <span class="math inline">\(R^2\)</span> value to increase (or at least not decrease).</p>
<p>[Additional technical detail<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>]</p>
<p>This adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted <span class="math inline">\(R^2\)</span> value is that when you add more predictors to the model, the adjusted <span class="math inline">\(R^2\)</span> value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted <span class="math inline">\(R^2\)</span> value can’t be interpreted in the elegant way that <span class="math inline">\(R^2\)</span> can. <span class="math inline">\(R^2\)</span> has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model. To my knowledge, no equivalent interpretation exists for adjusted <span class="math inline">\(R^2\)</span>.</p>
<p>An obvious question then is whether you should report <span class="math inline">\(R^2\)</span> or adjusted <span class="math inline">\(R^2\)</span>. This is probably a matter of personal preference. If you care more about interpretability, then <span class="math inline">\(R^2\)</span> is better. If you care more about correcting for bias, then adjusted <span class="math inline">\(R^2\)</span> is probably better. Speaking just for myself, I prefer <span class="math inline">\(R^2\)</span>. My feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll see in <a href="#hypothesis-tests-for-regression-models">Hypothesis tests for regression models</a>, if you’re worried that the improvement in <span class="math inline">\(R^2\)</span> that you get by adding a predictor is just due to chance and not because it’s a better model, well we’ve got hypothesis tests for that.</p>
</section>
</section>
<section id="hypothesis-tests-for-regression-models" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="hypothesis-tests-for-regression-models"><span class="header-section-number">12.7</span> Hypothesis tests for regression models</h2>
<p>So far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero.</p>
<section id="testing-the-model-as-a-whole" class="level3" data-number="12.7.1">
<h3 data-number="12.7.1" class="anchored" data-anchor-id="testing-the-model-as-a-whole"><span class="header-section-number">12.7.1</span> Testing the model as a whole</h3>
<p>Okay, suppose you’ve estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.</p>
<p>[Additional technical detail<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>]</p>
<p>We’ll see much more of the <span class="math inline">\(F\)</span>-statistic in <a href="13-Comparing-several-means-one-way-ANOVA.html"><span>Chapter&nbsp;13</span></a>, but for now just know that we can interpret large <span class="math inline">\(F\)</span>-values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I’ll show you how to do the test in jamovi the easy way, but first let’s have a look at the tests for the individual regression coefficients.</p>
</section>
<section id="tests-for-individual-coefficients" class="level3" data-number="12.7.2">
<h3 data-number="12.7.2" class="anchored" data-anchor-id="tests-for-individual-coefficients"><span class="header-section-number">12.7.2</span> Tests for individual coefficients</h3>
<p>The <span class="math inline">\(F\)</span>-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn’t produce a significant result for the <span class="math inline">\(F\)</span>-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the <a href="#multiple-linear-regression">Multiple linear regression</a> model we have already looked at (<a href="#tbl-tab12-4">Table&nbsp;<span>12.4</span></a>)</p>
<p>I can’t help but notice that the estimated regression coefficient for the baby.sleep variable is tiny (<span class="math inline">\(0.01\)</span>), relative to the value that we get for dani.sleep (<span class="math inline">\(-8.95\)</span>). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this illuminating. In fact, I’m beginning to suspect that it’s really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the <span class="math inline">\(t\)</span>-test. The test that we’re interested in has a null hypothesis that the true regression coefficient is zero (<span class="math inline">\(b = 0\)</span>), which is to be tested against the alternative hypothesis that it isn’t (<span class="math inline">\(b \neq 0\)</span>). That is:</p>
<p><span class="math display">\[H_0:b=0\]</span> <span class="math display">\[H_1:b \neq 0\]</span></p>
<p>How can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of <span class="math inline">\(\hat{b}\)</span>, the estimated regression coefficient, is a normal distribution with mean centred on <span class="math inline">\(b\)</span>. What that would mean is that if the null hypothesis were true, then the sampling distribution of <span class="math inline">\(\hat{b}\)</span> has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, <span class="math inline">\(se(\hat{b})\)</span>, then we’re in luck. That’s exactly the situation for which we introduced the one-sample <span class="math inline">\(t\)</span>-test back in <a href="11-Comparing-two-means.html"><span>Chapter&nbsp;11</span></a>. So let’s define a <span class="math inline">\(t\)</span>-statistic like this:</p>
<p><span class="math display">\[t=\frac{\hat{b}}{SE(\hat{b})}\]</span></p>
<p>I’ll skip over the reasons why, but our degrees of freedom in this case are <span class="math inline">\(df = N - K - 1\)</span>. Irritatingly, the estimate of the standard error of the regression coefficient, <span class="math inline">\(se(\hat{b})\)</span>, is not as easy to calculate as the standard error of the mean that we used for the simpler <span class="math inline">\(t\)</span>-tests in <a href="11-Comparing-two-means.html"><span>Chapter&nbsp;11</span></a>. In fact, the formula is somewhat ugly, and not terribly helpful to look at.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).</p>
<p>In any case, this <span class="math inline">\(t\)</span>-statistic can be interpreted in the same way as the <span class="math inline">\(t\)</span>-statistics that we discussed in <a href="11-Comparing-two-means.html"><span>Chapter&nbsp;11</span></a>. Assuming that you have a two-sided alternative (i.e., you don’t really care if b <span class="math inline">\(&gt;\)</span> 0 or b <span class="math inline">\(&lt;\)</span> 0), then it’s the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.</p>
</section>
<section id="running-the-hypothesis-tests-in-jamovi" class="level3" data-number="12.7.3">
<h3 data-number="12.7.3" class="anchored" data-anchor-id="running-the-hypothesis-tests-in-jamovi"><span class="header-section-number">12.7.3</span> Running the hypothesis tests in jamovi</h3>
<p>To compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in <a href="#fig-fig12-15">Figure&nbsp;<span>12.15</span></a>, we get a whole bunch of useful output.</p>
<p>The ‘Model Coefficients’ at the bottom of the jamovi analysis results shown in <a href="#fig-fig12-15">Figure&nbsp;<span>12.15</span></a> provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of <span class="math inline">\(b\)</span> (e.g., <span class="math inline">\(125.97\)</span> for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate <span class="math inline">\(\hat{\sigma}_b\)</span>. The third and fourth columns provide the lower and upper values for the 95% confidence interval around the <span class="math inline">\(b\)</span> estimate (more on this later). The fifth column gives you the <span class="math inline">\(t\)</span>-statistic, and it’s worth noticing that in this table <span class="math inline">\(t=\frac{\hat{b}} {se({\hat{b}})}\)</span> every time. Finally, the last column gives you the actual <span class="math inline">\(p\)</span>-value for each of these tests.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-15" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-15.png" class="img-fluid figure-img" style="width:80.0%" alt="A jamovi screenshot showing Linear Regression analysis results. The interface includes options for configuring the model, viewing the model’s fit measures such as Adjusted R² and AIC, and examining model coefficients such as &quot;state_reg&quot; and &quot;murders&quot;"></p>
<figcaption class="figure-caption">Figure&nbsp;12.15. A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked</figcaption>
</figure>
</div>
</div>
</div>
<p>The only thing that the coefficients table itself doesn’t list is the degrees of freedom used in the <span class="math inline">\(t\)</span>-test, which is always <span class="math inline">\(N - K - 1\)</span> and is listed in the table at the top of the output, labelled ‘Model Fit Measures’. We can see from this table that the model performs significantly better than you’d expect by chance (<span class="math inline">\(F(2,97) = 215.24, p&lt; .001\)</span>), which isn’t all that surprising: the <span class="math inline">\(R^2 = .81\)</span> value indicate that the regression model accounts for <span class="math inline">\(81\%\)</span> of the variability in the outcome measure (and <span class="math inline">\(82\%\)</span> for the adjusted <span class="math inline">\(R^2\)</span> ). However, when we look back up at the <span class="math inline">\(t\)</span>-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You’d probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model.</p>
</section>
</section>
<section id="regarding-regression-coefficients" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="regarding-regression-coefficients"><span class="header-section-number">12.8</span> Regarding regression coefficients</h2>
<p>Before moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.</p>
<section id="confidence-intervals-for-the-coefficients" class="level3" data-number="12.8.1">
<h3 data-number="12.8.1" class="anchored" data-anchor-id="confidence-intervals-for-the-coefficients"><span class="header-section-number">12.8.1</span> Confidence intervals for the coefficients</h3>
<p>Like any population parameter, the regression coefficients <span class="math inline">\(b\)</span> cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of <span class="math inline">\(b\)</span>. This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable <span class="math inline">\(X\)</span> is related to variable <span class="math inline">\(Y\)</span>, since in those situations the interest is primarily in the regression weight <span class="math inline">\(b\)</span>.</p>
<p>[Additional technical detail<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>]</p>
<p>In jamovi we had already specified the ‘95% Confidence interval’ as shown in <a href="#fig-fig12-15">Figure&nbsp;<span>12.15</span></a>, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.</p>
</section>
<section id="calculating-standardised-regression-coefficients" class="level3" data-number="12.8.2">
<h3 data-number="12.8.2" class="anchored" data-anchor-id="calculating-standardised-regression-coefficients"><span class="header-section-number">12.8.2</span> Calculating standardised regression coefficients</h3>
<p>One more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted <span class="math inline">\(\beta\)</span>. The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s <span class="math inline">\(IQ\)</span> scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by 10,000s of dollars (or more). The units of measurement have a big influence on the regression coefficients. The <span class="math inline">\(b\)</span> coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what <strong>standardised coefficients</strong> aim to do.</p>
<p>The basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to <em>z</em>-scores before running the regression.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> The idea here is that, by converting all the predictors to <em>z</em>-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a <span class="math inline">\(\beta\)</span> value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable <span class="math inline">\(A\)</span> has a larger absolute value of <span class="math inline">\(\beta\)</span> than variable <span class="math inline">\(B\)</span>, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.</p>
<p>[Additional technical detail<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>]</p>
<p>To make things even simpler, jamovi has an option that computes the <span class="math inline">\(\beta\)</span> coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in <a href="#fig-fig12-16">Figure&nbsp;<span>12.16</span></a>.</p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-16" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-16.png" class="img-fluid figure-img" style="width:80.0%" alt="A jamovi screenshot showing a Linear Regression analysis. The interface includes sections for variables, model fit measures, model coefficients, omnibus test, and references. There are also options for ANOVA, Frequencies, and Factor Analysis."></p>
<figcaption class="figure-caption">Figure&nbsp;12.16. Standardised coefficients, with 95% confidence intervals, for multiple linear regression</figcaption>
</figure>
</div>
</div>
</div>
<p>These results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients <span class="math inline">\(\beta\)</span>. After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to <em>z</em>-scores?</p>
</section>
</section>
<section id="assumptions-of-regression" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="assumptions-of-regression"><span class="header-section-number">12.9</span> Assumptions of regression</h2>
<p>The linear regression model that I’ve been discussing relies on several assumptions. In <a href="#sec-Model-checking">Model checking</a> we’ll talk a lot more about how to check that these assumptions are being met, but first let’s have a look at each of them.</p>
<ul>
<li><strong>L</strong>inearity. A pretty fundamental assumption of the linear regression model is that the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> actually is linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relationships involved are linear.</li>
<li><strong>I</strong>ndependence: residuals are independent of each other. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals”. If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up. Independence isn’t something that you can check directly and specifically with diagnostic tools, but if your regression diagnostics are messed up then think carefully about the independence of your observations and residuals.</li>
<li><strong>N</strong>ormality. Like many of the models in statistics, basic simple or multiple linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It’s actually okay if the predictors <span class="math inline">\(X\)</span> and the outcome <span class="math inline">\(Y\)</span> variables are non-normal, so long as the residuals <span class="math inline">\(\epsilon\)</span> are normal. See the <a href="#sec-Checking-the-normality-of-the-residuals">Checking the normality of the residuals</a> section.</li>
<li><strong>E</strong>quality (or “homogeneity”) of variance. Strictly speaking, the regression model assumes that each residual <span class="math inline">\(\epsilon_i\)</span> is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation <span class="math inline">\(\sigma\)</span> that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of <span class="math inline">\(\hat{Y}\)</span>, and (if we’re being especially diligent) all values of every predictor <span class="math inline">\(X\)</span> in the model.</li>
</ul>
<p>So, we have four main assumptions for linear regression (that neatly form the acronym <strong>LINE</strong>). And there are also a couple of other things we should also check for:</p>
<ul>
<li>Uncorrelated predictors. The idea here is that, in a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See <a href="#checking-for-collinearity">Checking for collinearity</a> section.</li>
<li>No “bad” outliers. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases. See the section on <a href="#outliers-and-anomalous-data">Outliers and anomalous data</a>.</li>
</ul>
</section>
<section id="sec-Model-checking" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="sec-Model-checking"><span class="header-section-number">12.10</span> Model checking</h2>
<p>The main focus of this section is <strong>regression diagnostics</strong>, a term that refers to the art of checking that the assumptions of your regression model have been met, figuring out how to fix the model if the assumptions are violated, and generally to check that nothing “funny” is going on. I refer to this as the “art” of model checking with good reason. It’s not easy, and while there are a lot of easily available tools that you can use to diagnose and maybe even cure the problems that affect your model (if there are any, that is!), you really do need to exercise a certain amount of judgement when doing this.</p>
<p>In this section I describe several different things you can do to check that your regression model is doing what it’s supposed to. It doesn’t cover the full space of things you could do, but it’s still much more detailed than what is often done in practice – unfortunately! But it’s important that you get a sense of what tools are at your disposal, so I’ll try to introduce a bunch of them here. Finally, I should note that this section draws quite heavily from <span class="citation" data-cites="Fox2011">Fox &amp; Weisberg (<a href="References.html#ref-Fox2011" role="doc-biblioref">2011</a>)</span>, the book associated with the “car” package that is used to conduct regression analysis in <span class="math inline">\(R\)</span>. The “car” package is notable for providing some excellent tools for regression diagnostics, and the book itself talks about them in an admirably clear fashion. I don’t want to sound too gushy about it, but I do think that <span class="citation" data-cites="Fox2011">Fox &amp; Weisberg (<a href="References.html#ref-Fox2011" role="doc-biblioref">2011</a>)</span> is well worth reading, even if some of the advanced diagnostic techniques are only available in “R” and not jamovi.</p>
<section id="three-kinds-of-residuals" class="level3" data-number="12.10.1">
<h3 data-number="12.10.1" class="anchored" data-anchor-id="three-kinds-of-residuals"><span class="header-section-number">12.10.1</span> Three kinds of residuals</h3>
<p>The majority of regression diagnostics revolve around looking at the residuals, and there are several different kinds of residual that we might consider. In particular, the following three kinds of residuals are referred to in this section: “ordinary residuals”, “standardised residuals”, and “Studentised residuals”. There is a fourth kind that you’ll see referred to in some of the Figures, and that’s the “Pearson residual”. However, for the models that we’re talking about in this chapter the Pearson residual is identical to the ordinary residual.</p>
<p>The first and simplest kind of residuals that we care about are <strong>ordinary residuals</strong>. These are the actual raw residuals that I’ve been talking about throughout this chapter so far. The ordinary residual is just the difference between the predicted value <span class="math inline">\(\hat{Y}_i\)</span> and the observed value <span class="math inline">\(Y_i\)</span>. I’ve been using the notation <span class="math inline">\(\epsilon_i\)</span> to refer to the i-th ordinary residual and so, with this in mind, we have the very simple equation: <span class="math display">\[\epsilon_i=Y_i-\hat{Y_i}\]</span></p>
<p>This is of course what we saw earlier, and unless I specifically refer to some other kind of residual, this is the one I’m talking about. So there’s nothing new here. I just wanted to repeat myself. One drawback to using ordinary residuals is that they’re always on a different scale, depending on what the outcome variable is and how good the regression model is. That is, unless you’ve decided to run a regression model without an intercept term, the ordinary residuals will have mean 0 but the variance is different for every regression. In a lot of contexts, especially where you’re only interested in the pattern of the residuals and not their actual values, it’s convenient to estimate the <strong>standardised residuals</strong>, which are normalised in such a way as to have a standard deviation of 1.</p>
<p>[Additional technical detail<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>]</p>
<p>The third kind of residuals are <strong>Studentised residuals</strong> (also called “jackknifed residuals”) and they’re even fancier than standardised residuals. Again, the idea is to take the ordinary residual and divide it by some quantity in order to estimate some standardised notion of the residual.<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></p>
<p>Before moving on, I should point out that you don’t often need to obtain these residuals yourself, even though they are at the heart of almost all regression diagnostics. Most of the time the various options that provide the diagnostics, or assumption checks, will take care of these calculations for you. Even so, it’s always nice to know how to actually get hold of these things yourself in case you ever need to do something non-standard.</p>
</section>
<section id="checking-the-linearity-of-the-relationship" class="level3" data-number="12.10.2">
<h3 data-number="12.10.2" class="anchored" data-anchor-id="checking-the-linearity-of-the-relationship"><span class="header-section-number">12.10.2</span> Checking the linearity of the relationship</h3>
<p>We should check for the linearity of the relationships between the predictors and the outcomes. There’s a few different things that you might want to do in order to check this. Firstly, it never hurts to just plot the relationship between the predicted values <span class="math inline">\(\hat{Y}_i\)</span> and the observed values <span class="math inline">\(Y_i\)</span> for the outcome variable, as illustrated in <a href="#fig-fig12-17">Figure&nbsp;<span>12.17</span></a>. To draw this in jamovi we saved the predicted values to the data set, and then drew a scatterplot of the observed against the predicted (fitted) values. This gives you a kind of “big picture view” – if this plot looks approximately linear, then we’re probably not doing too badly (though that’s not to say that there aren’t problems). However, if you can see big departures from linearity here, then it strongly suggests that you need to make some changes.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-17" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-17.png" class="img-fluid figure-img" style="width:80.0%" alt="A scatter plot with the x-axis labeled &quot;My grumpiness&quot; and the y-axis labeled &quot;Predicted values.&quot; The plot has numerous scattered data points, mostly clustered between 40-90 on the x-axis and 40-80 on the y-axis. A trend line shows a positive correlation."></p>
<figcaption class="figure-caption">Figure&nbsp;12.17. jamovi plot of the predicted values against the observed values of the outcome variable. A straight(-ish) line is what we are hoping to see here. This looks pretty good, suggesting that there is nothing grossly wrong</figcaption>
</figure>
</div>
</div>
</div>
<p>In any case, in order to get a more detailed picture it’s often more informative to look at the relationship between the predicted values and the residuals themselves. Again, in jamovi you can save the residuals to the data set and then draw a scatterplot of the predicted values against the residual values, as in <a href="#fig-fig12-18">Figure&nbsp;<span>12.18</span></a>. As you can see, not only does it draw the scatterplot showing the predicted value against the residuals, you can also plot a line through the data that shows the relationship between the two. Ideally, this should be a straight, perfectly horizontal line. In practice, we’re looking for a reasonably straight or flat line. This is a matter of judgement.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-18" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-18.png" class="img-fluid figure-img" style="width:80.0%" alt="A scatter plot shows residuals versus predicted values with a non-linear trend. Residuals range from -10 to 10, and predicted values range from 45 to 85. Data points are scattered around a fitted curve that starts flat, dips slightly, and rises towards the end."></p>
<figcaption class="figure-caption">Figure&nbsp;12.18. jamovi plot of the predicted values against the residuals, with a line showing the relationship between the two. If this is horizontal and straight(-ish), then we can feel reasonably confident that the “average residual” for all “predicted values” is more or less the same.</figcaption>
</figure>
</div>
</div>
</div>
<p>More advanced versions of the same plot are produced by checking ‘Residuals plots’ in the regression analysis ‘Assumption checks’ options in jamovi. These are useful for checking linearity, normality and equality of variance assumptions, and we look at these in more detail in <a href="#sec-Checking-the-normality-of-the-residuals"><span>Section&nbsp;12.10.3</span></a>. This option not only draws plots comparing the predicted values to the residuals, it does so for each individual predictor.</p>
</section>
<section id="sec-Checking-the-normality-of-the-residuals" class="level3" data-number="12.10.3">
<h3 data-number="12.10.3" class="anchored" data-anchor-id="sec-Checking-the-normality-of-the-residuals"><span class="header-section-number">12.10.3</span> Checking the normality of the residuals</h3>
<p>Like many of the statistical tools we’ve discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. The first thing we can do is draw a QQ-plot via the ‘Assumption Checks’ – ‘Assumption Checks’ – ‘Q-Q plot of residuals’ option. The output is shown in <a href="#fig-fig12-19">Figure&nbsp;<span>12.19</span></a>, showing the standardised residuals plotted as a function of their theoretical quantiles according to the regression model.</p>
<p>Another thing we should check is the relationship between the predicted (fitted) values and the residuals themselves. We can get jamovi to do this using the ‘Residuals Plots’ option, which provides a scatterplot for each predictor variable, the outcome variable, and the predicted values against residuals, see <a href="#fig-fig12-20">Figure&nbsp;<span>12.20</span></a>. In these plots we are looking for a fairly uniform distribution of dots, with no clear bunching or patterning of the dots. Looking at these plots, there is nothing particularly worrying as the dots are fairly evenly spread across the whole plot. There may be a little bit of non-uniformity in plot (b), but it is not a strong deviation and probably not worth worrying about.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-19" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-19.png" class="img-fluid figure-img" style="width:70.0%" alt="Q-Q plot titled &quot;Assumption Checks&quot; showing standardized residuals along the y-axis and theoretical quantiles along the x-axis. Points closely follow the 45-degree reference line, indicating that the residuals follow a normal distribution"></p>
<figcaption class="figure-caption">Figure&nbsp;12.19. Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in jamovi</figcaption>
</figure>
</div>
</div>
</div>
<div id="fig-fig12-20" class="quarto-layout-panel" width="90%" data-fig-pos="H">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="images/fig12-20a.png" class="img-fluid figure-img" style="width:45.0%" alt="Scatter plot showing residuals versus fitted values. The x-axis is labeled 'Fitted' and the y-axis is labeled 'Residuals'. Data points are scattered across the plot, with residuals ranging from approximately -10 to 10 and fitted values between 50 and 80."> <img src="images/fig12-20b.png" class="img-fluid figure-img" style="width:45.0%" alt="A scatter plot titled 'My grumpiness' on the x-axis and 'Residuals' on the y-axis. Data points are dispersed, showing a positive correlation between grumpiness (ranging from 40 to 90) and residuals (ranging from -10 to 10)."> <img src="images/fig12-20c.png" class="img-fluid figure-img" style="width:45.0%" alt="A scatter plot shows residuals on the vertical axis ranging from -10 to 10 and baby's sleep (hours) on the horizontal axis ranging from 4 to 12 hours. Numerous data points are dispersed across the plot, displaying no clear pattern or trend."> <img src="images/fig12-20d.png" class="img-fluid figure-img" style="width:45.0%" alt="A scatter plot titled 'My sleep (hours) vs Residuals' showing data points representing residual values against the number of hours slept, ranging from 5 to 9 hours. Residuals vary roughly between -10 and 10, with no clear pattern or trend visible."></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;12.20. Residuals plots produced in jamovi</figcaption><p></p>
</figure>
</div>
<p>If we were worried, then in a lot of cases the solution to this problem (and many others) is to transform one or more of the variables. We discussed the basics of variable transformation in <a href="06-Pragmatic-matters.html#sec-Transforming-and-recoding-a-variable"><span>Section&nbsp;6.3</span></a>, but I do want to make special note of one additional possibility that I didn’t explain fully earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one and it’s very widely used.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<p>You can calculate it using the BOXCOX function in the ‘Compute’ variables screen in jamovi.</p>
</section>
<section id="checking-equality-of-variance" class="level3" data-number="12.10.4">
<h3 data-number="12.10.4" class="anchored" data-anchor-id="checking-equality-of-variance"><span class="header-section-number">12.10.4</span> Checking equality of variance</h3>
<p>The regression models that we’ve talked about all make an equality (i.e.homogeneity) of variance assumption: the variance of the residuals is assumed to be constant. To plot this in jamovi first we need to calculate the square root of the (absolute) size of the residual<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> and then plot this against the predicted values, as in <a href="#fig-fig12-21">Figure&nbsp;<span>12.21</span></a>. Note that this plot actually uses the standardised residuals rather than the raw ones, but it’s immaterial from our point of view. What we’re looking to see here is a straight, horizontal line running through the middle of the plot.<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-21" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-21.png" class="img-fluid figure-img" style="width:80.0%" alt="Scatter plot showing the relationship between predicted values (x-axis) and SQR residuals (y-axis). The plot includes scattered data points and a smoothed trend line indicating the general pattern of the data. Predicted values range from 45 to 85, SQR Residuals from 0.5 to 3.5."></p>
<figcaption class="figure-caption">Figure&nbsp;12.21. jamovi plot of the predicted values (model predictions) against the square root of the absolute standardised residuals. This plot is used to diagnose violations of homogeneity of variance. If the variance is really constant, then the line through the middle should be horizontal and flat(-ish).</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="checking-for-collinearity" class="level3" data-number="12.10.5">
<h3 data-number="12.10.5" class="anchored" data-anchor-id="checking-for-collinearity"><span class="header-section-number">12.10.5</span> Checking for collinearity</h3>
<p>Another regression diagnostic is provided by <strong>variance inflation factors</strong> (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor <span class="math inline">\(X_k\)</span> in the model.<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></p>
<p>If you’ve only got two predictors, the VIF values are always going to be the same, as we can see if we click on the ‘Collinearity’ checkbox in the ‘Regression’ – ‘Assumptions’ options in jamovi. For both dani.sleep and baby.sleep the VIF is <span class="math inline">\(1.65\)</span>. And since the square root of <span class="math inline">\(1.65\)</span> is <span class="math inline">\(1.28\)</span>, we see that the correlation between our two predictors isn’t causing much of a problem.</p>
<p>To give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the day on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let’s have a look at the correlation matrix for all four variables (<a href="#fig-fig12-22">Figure&nbsp;<span>12.22</span></a>).</p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-22" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-22.png" class="img-fluid figure-img" style="width:80.0%" alt="A correlation matrix table showing the relationships between variables:- &quot;My grumpiness,&quot; &quot;Baby's sleep (hours),&quot; &quot;My sleep (hours),&quot; and &quot;day.&quot; Notable correlations include -0.57 between &quot;My grumpiness&quot; and &quot;Baby's sleep&quot; and 0.63 between &quot;My sleep&quot; and &quot;Baby's sleep."></p>
<figcaption class="figure-caption">Figure&nbsp;12.22. Correlation matrix in jamovi for all four variables</figcaption>
</figure>
</div>
</div>
</div>
<p>We have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, run the regression, as in <a href="#fig-fig12-23">Figure&nbsp;<span>12.23</span></a> and you can see from the VIF values that, yep, that’s some mighty fine collinearity there.</p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-23" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-23.png" class="img-fluid figure-img" style="width:99.0%" alt="A jamovi screenshot displaying a linear regression analysis. The analysis includes model fit measures, coefficients, and assumption checks. Options to select variables, covariates, and factors are visible on the left sidebar."></p>
<figcaption class="figure-caption">Figure&nbsp;12.23. Collinearity statistics for multiple regression, produced in jamovi</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell" data-layout-align="left" data-fig-plot="4">
<div class="cell-output-display">
<div id="fig-fig12-24" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-24-1.png" class="img-fluid figure-img" style="width:80.0%" alt="A scatter plot with blue dots representing data points, an outlier marked near the top-left region. A solid line shows the regression line, while a dashed line shows the ideal line. The X-axis is labeled 'Predictor' and the Y-axis is labeled 'Outcome'. A vertical line from an outlier point to the dashed regression line illustrates the large residual error for the outlier"></p>
<figcaption class="figure-caption">Figure&nbsp;12.24. An illustration of outliers. The solid line shows the regression line with the anomalous outlier observation included. The dashed line plots the regression line estimated without the anomalous outlier observation included. The vertical line from the outlier point to the dashed regression line illustrates the large residual error for the outlier</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="outliers-and-anomalous-data" class="level3" data-number="12.10.6">
<h3 data-number="12.10.6" class="anchored" data-anchor-id="outliers-and-anomalous-data"><span class="header-section-number">12.10.6</span> Outliers and anomalous data</h3>
<p>One danger that you can run into with linear regression models is that your analysis might be disproportionately sensitive to a smallish number of “unusual” or “anomalous” observations. I discussed this idea previously in <a href="05-Drawing-graphs.html#sec-Using-box-plots-to-detect-outliers"><span>Section&nbsp;5.2.3</span></a> in the context of discussing the outliers that get automatically identified by the boxplot option under ‘Exploration’ – ‘Descriptives’, but this time we need to be much more precise. In the context of linear regression, there are three conceptually distinct ways in which an observation might be called “anomalous”. All three are interesting, but they have rather different implications for your analysis.</p>
<p>The first kind of unusual observation is an <strong>outlier</strong>. The definition of an outlier (in this context) is an observation that is very different from what the regression model predicts. An example is shown in <a href="#fig-fig12-24">Figure&nbsp;<span>12.24</span></a>, the outlier has an unusual value on the outcome (y-axis location), but not the predictor (x-axis location) and lies a long way from the regression line. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large residual, <span class="math inline">\(\epsilon_i^*\)</span>. Also see the lower left plot of Anscombe’s quartet, <a href="#fig-fig12-6">Figure&nbsp;<span>12.6</span></a>.</p>
<p>Outliers are interesting: a big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly in the data set, or some other defect may be detectable. Note that you shouldn’t throw an observation away just because it’s an outlier. But the fact that it’s an outlier is often a cue to look more closely at that case and try to find out why it’s so different.</p>
<div class="cell" data-layout-align="left" data-fig-plot="4">
<div class="cell-output-display">
<div id="fig-fig12-25" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-25-1.png" class="img-fluid figure-img" style="width:80.0%" alt="A scatter plot with blue dots showing the relationship between a predictor and an outcome. A solid black regression line trends upwards, with one point labeled &quot;High leverage&quot; noticeably distant from the other points. A dashed line represents an alternate linear fit"></p>
<figcaption class="figure-caption">Figure&nbsp;12.25. An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x-axis) and the outcome (y-axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations. The observation falls very close to the regression line and does not distort it by very much</figcaption>
</figure>
</div>
</div>
</div>
<p>The second way in which an observation can be unusual is if it has high <strong>leverage</strong>, which happens when the observation is very different from all the other observations. This doesn’t necessarily have to correspond to a large residual. If the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line. An example of this is shown in <a href="#fig-fig12-25">Figure&nbsp;<span>12.25</span></a>. The leverage of an observation is operationalised in terms of its hat value, usually written <span class="math inline">\(h_i\)</span>. The formula for the hat value is rather complicated<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a> but its interpretation is not: <span class="math inline">\(h_i\)</span> is a measure of the extent to which the i-th observation is “in control” of where the regression line ends up going.</p>
<p>In general, if an observation lies far away from the other ones in terms of the predictor variables, it will have a large hat value (as a rough guide, high leverage is when the hat value is more than 2-3 times the average; and note that the sum of the hat values is constrained to be equal to <span class="math inline">\(K + 1\)</span>). High leverage points are also worth looking at in more detail, but they’re much less likely to be a cause for concern unless they are also outliers.</p>
<p>This brings us to our third measure of unusualness, the <strong>influence</strong> of an observation. A high influence observation is an outlier that has high leverage. That is, it is an observation that is very different to all the other ones in some respect, and also lies a long way from the regression line. This is illustrated in <a href="#fig-fig12-26">Figure&nbsp;<span>12.26</span></a>. Notice the contrast to the previous two figures. Outliers don’t move the regression line much and neither do high leverage points. But something that is both an outlier and has high leverage, well that has a big effect on the regression line. That’s why we call these points high influence, and it’s why they’re the biggest worry. We operationalise influence in terms of a measure known as <strong>Cook’s distance</strong>.<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<div class="cell" data-layout-align="left" data-fig-plot="4">
<div class="cell-output-display">
<div id="fig-fig12-26" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="12-Correlation-and-linear-regression_files/figure-html/fig-fig12-26-1.png" class="img-fluid figure-img" style="width:90.0%" alt="Scatter plot showing a linear relationship between Predictor (x-axis) and Outcome (y-axis). Points are scattered around a solid black regression line. One outlier, labeled &quot;High influence,&quot; has a vertical distance to the regression line connected by a dashed line"></p>
<figcaption class="figure-caption">Figure&nbsp;12.26. An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x-axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y-axis)</figcaption>
</figure>
</div>
</div>
</div>
<p>In order to have a large Cook’s distance an observation must be a fairly substantial outlier and have high leverage. As a rough guide, Cook’s distance greater than 1 is often considered large (that’s what I typically use as a quick and dirty rule).</p>
<p>In jamovi, information about Cook’s distance can be calculated by clicking on the ‘Cook’s Distance’ checkbox in the ‘Assumption Checks’ – ‘Data Summary’ options. When you do this, for the multiple regression model we have been using as an example in this chapter, you get the results as shown in <a href="#fig-fig12-27">Figure&nbsp;<span>12.27</span></a>.</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-27" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-27.png" class="img-fluid figure-img" style="width:80.0%" alt="A data summary table is shown with Cooks Distance values. The columns listed are Mean (0.01), Median (0.00), SD (Standard Deviation) (0.02), Min (0.00), and Max (0.11)"></p>
<figcaption class="figure-caption">Figure&nbsp;12.27. jamovi output showing the table for the Cooks distance statistics</figcaption>
</figure>
</div>
</div>
</div>
<p>You can see that, in this example, the mean Cook’s distance value is <span class="math inline">\(0.01\)</span>, and the range is from <span class="math inline">\(0.00\)</span> to <span class="math inline">\(0.11\)</span>, so this is some way off the rule of thumb figure mentioned above that a Cook’s distance greater than 1 is considered large.</p>
<p>An obvious question to ask next is, if you do have large values of Cook’s distance what should you do? As always, there’s no hard and fast rule. Probably the first thing to do is to try running the regression with the outlier with the greatest Cook’s distance<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> excluded and see what happens to the model performance and to the regression coefficients. If they really are substantially different, it’s time to start digging into your data set and your notes that you no doubt were scribbling as your ran your study. Try to figure out why the point is so different. If you start to become convinced that this one data point is badly distorting your results then you might consider excluding it, but that’s less than ideal unless you have a solid explanation for why this particular case is qualitatively different from the others and therefore deserves to be handled separately.</p>
</section>
</section>
<section id="model-selection" class="level2" data-number="12.11">
<h2 data-number="12.11" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">12.11</span> Model selection</h2>
<p>One fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of <strong>variable selection</strong>. In general, model selection is a complex business but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:</p>
<ul>
<li><p>It’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.</p></li>
<li><p>To the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., <span class="math inline">\(R^2\)</span>) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.</p></li>
</ul>
<p>This latter principle is often referred to as <strong>Ockham’s razor</strong> and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don’t chuck in a bunch of largely irrelevant predictors just to boost your <span class="math inline">\(R^2\)</span>. Hmm. Yeah, the original was better.</p>
<p>In any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Ockham’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the <strong>Akaike information criterion</strong> (AIC) <span class="citation" data-cites="Akaike1974">(<a href="References.html#ref-Akaike1974" role="doc-biblioref">Akaike, 1974</a>)</span> simply because it’s available as an option in jamovi.<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a></p>
<p>The smaller the AIC value, the better the model performance. If we ignore the low level details it’s fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left-hand side) using as few predictors as possible (low K, right-hand side). In short, this is a simple implementation of Ockham’s razor.</p>
<p>AIC can be added to the ‘Model Fit Measures’ output Table when the ‘AIC’ checkbox is clicked, and a rather clunky way of assessing different models is seeing if the AIC value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.</p>
<section id="backward-elimination" class="level3" data-number="12.11.1">
<h3 data-number="12.11.1" class="anchored" data-anchor-id="backward-elimination"><span class="header-section-number">12.11.1</span> Backward elimination</h3>
<p>In backward elimination you start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.</p>
</section>
<section id="forward-selection" class="level3" data-number="12.11.2">
<h3 data-number="12.11.2" class="anchored" data-anchor-id="forward-selection"><span class="header-section-number">12.11.2</span> Forward selection</h3>
<p>As an alternative, you can also try <strong>forward selection</strong>. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication. You also need to specify what the largest possible model you’re willing to entertain is.</p>
<p>Although backward and forward selection can lead to the same conclusion, they don’t always.</p>
</section>
<section id="a-caveat" class="level3" data-number="12.11.3">
<h3 data-number="12.11.3" class="anchored" data-anchor-id="a-caveat"><span class="header-section-number">12.11.3</span> A caveat</h3>
<p>Automated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.</p>
<p>Or, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used <span class="math inline">\(F\)</span>-tests to do it, because that was the default method used by the software. I’ve described using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the Bayesian Information Criterion (BIC) for instance (also available in jamovi). Take a different approach again and you get the normalised maximum likelihood (NML) criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.</p>
<p>What does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it. You’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you’re staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn’t make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.</p>
</section>
<section id="comparing-two-regression-models" class="level3" data-number="12.11.4">
<h3 data-number="12.11.4" class="anchored" data-anchor-id="comparing-two-regression-models"><span class="header-section-number">12.11.4</span> Comparing two regression models</h3>
<p>An alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or <strong>covariates</strong> that we want to control for. In this situation, what we would like to know is whether dani.grump ~ dani.sleep + day + baby .sleep (which I’ll call Model 2, or M2) is a better regression model for these data than dani.grump ~ dani.sleep + day (which I’ll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don’t do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></p>
<p>A somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an <span class="math inline">\(F\)</span>-test for this in a fairly straightforward fashion.<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></p>
<p>Okay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the ‘Model Builder’ option and specify the Model 1 predictors dani.sleep and day in ‘Block 1’ and then add the additional predictor from Model 2 (baby.sleep) in ‘Block 2’, as in <a href="#fig-fig12-28">Figure&nbsp;<span>12.28</span></a>. This shows, in the ‘Model Comparisons’ Table, that for the comparisons between Model 1 and Model 2, <span class="math inline">\(F(1,96) = 0.00\)</span>, <span class="math inline">\(p = 0.954\)</span>. Since we have p &gt; .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as <strong>hierarchical regression</strong>.</p>
<p>We can also use this ‘Model Comparison’ option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in <a href="#fig-fig12-28">Figure&nbsp;<span>12.28</span></a>.</p>
<div class="cell enlarge-image" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig12-28" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-28.png" class="img-fluid figure-img" style="width:80.0%" alt="A jamovi screenshot displaying a linear regression analysis. The screen shows options for dependent variables, factors, and weights on the left, while the right side presents the model fit measures, comparisons, and specific results, including coefficients and standard errors."></p>
<figcaption class="figure-caption">Figure&nbsp;12.28. Model comparison in jamovi using the ‘Model Builder’ option</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="summary" class="level2" data-number="12.12">
<h2 data-number="12.12" class="anchored" data-anchor-id="summary"><span class="header-section-number">12.12</span> Summary</h2>
<ul>
<li>Want to know how strong the relationship is between two variables? Calculate <a href="#correlations">Correlations</a>.</li>
<li>Drawing <a href="#scatterplots">Scatterplots</a>.</li>
<li>Basic ideas about <a href="#what-is-a-linear-regression-model">What is a linear regression model?</a> and <a href="#estimating-a-linear-regression-model">Estimating a linear regression model</a>.</li>
<li><a href="#multiple-linear-regression">Multiple linear regression</a>.</li>
<li><a href="#quantifying-the-fit-of-the-regression-model">Quantifying the fit of the regression model</a> using <span class="math inline">\(R^2\)</span>.</li>
<li><a href="#hypothesis-tests-for-regression-models">Hypothesis tests for regression models</a>.</li>
<li>In <a href="#regarding-regression-coefficients">Regarding regression coefficients</a> we talked about calculating <a href="#confidence-intervals-for-the-coefficients">Confidence intervals for the coefficients</a> and <a href="#calculating-standardised-regression-coefficients">Calculating standardised regression coefficients</a>.</li>
<li>The <a href="#assumptions-of-regression">Assumptions of regression</a> and <a href="#sec-Model-checking">Model checking</a>.</li>
<li>Regression <a href="#model-selection">Model selection</a>.</li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Akaike1974" class="csl-entry" role="listitem">
Akaike, H. (1974). A new look at the statistical model identification. <em>IEEE Transactions on Automatic Control</em>, <em>19</em>, 716–723. <a href="https://doi.org/10.1109/TAC.1974.1100705">https://doi.org/10.1109/TAC.1974.1100705</a>
</div>
<div id="ref-Anscombe1973" class="csl-entry" role="listitem">
Anscombe, F. J. (1973). Graphs in statistical analysis. <em>American Statistician</em>, <em>27</em>, 17–21. <a href="https://doi.org/10.1080/00031305.1973.10478966">https://doi.org/10.1080/00031305.1973.10478966</a>
</div>
<div id="ref-Fox2011" class="csl-entry" role="listitem">
Fox, J., &amp; Weisberg, S. (2011). <em>An <span>R</span> companion to applied regression</em> (2nd ed.). Sage.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>I’ve noticed that in jamovi you can also specify an ‘ID’ variable type, but for our purposes it does not matter how we specify the ID variable as we won’t be including it in any analyses.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Actually, even that table is more than I’d bother with. In practice most people pick one measure of central tendency, and one measure of variability only.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The formula for the Pearson’s correlation coefficient can be written in several different ways. I think the simplest way to write down the formula is to break it into two steps. Firstly, let’s introduce the idea of a <strong>covariance</strong>. The covariance between two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is a generalisation of the notion of the variance and is a mathematically simple way of describing the relationship between two variables that isn’t terribly informative to humans: <span class="math display">\[Cov(X,Y)=\frac{1}{N-1}\sum_{i=1}^N(X_i-\bar{X})(Y_i-\bar{Y})\]</span> Because we’re multiplying (i.e., taking the “product” of) a quantity that depends on X by a quantity that depends on Y and then averaging,<span class="math inline">\(^a\)</span> you can think of the formula for the covariance as an “average cross product” between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The covariance has the nice property that, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive (in the sense shown in <a href="#fig-fig12-4">Figure&nbsp;<span>12.4</span></a> then the covariance is also positive, and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn’t easy to interpret as it depends on the units in which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are expressed and, worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if <span class="math inline">\(X\)</span> refers to the dani.sleep variable (units: hours) and <span class="math inline">\(Y\)</span> refers to the dani.grump variable (units: grumps), then the units for their covariance are <span class="math inline">\(hours \times grumps\)</span>. And I have no freaking idea what that would even mean. The Pearson correlation coefficient r fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the <em>z</em>-score standardises a raw score, by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations.<span class="math inline">\(^b\)</span> In other words, the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be written as follows: <span class="math display">\[r_{XY}=\frac{Cov(X,Y)}{\hat{\sigma}_X\hat{\sigma}_Y}\]</span><br>—<br><span class="math inline">\(^a\)</span> Just like we saw with the variance and the standard deviation, in practice we divide by <span class="math inline">\(N - 1\)</span> rather than <span class="math inline">\(N\)</span>. <br> <span class="math inline">\(^b\)</span> This is an oversimplification, but it’ll do for our purposes.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Also sometimes written as <span class="math inline">\(y = mx + c\)</span> where <span class="math inline">\(m\)</span> is the slope coefficient and <span class="math inline">\(c\)</span> is the intercept (constant) coefficient: <span class="math display">\[\hat{Y}_i=b_0+b_1X_i\]</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The <span class="math inline">\(\epsilon\)</span> symbol is the Greek letter epsilon. It’s traditional to use <span class="math inline">\(\epsilon_i\)</span> or <span class="math inline">\(e_i\)</span> to denote a residual: <span class="math display">\[\epsilon_i=Y_i-\hat{Y}_i\]</span> which in turn means that we can write down the complete linear regression model as: <span class="math display">\[Y_i=b_0+b_1X_i+\epsilon_i\]</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Or at least, I’m assuming that it doesn’t help most people. But on the off chance that someone reading this is a proper kung fu master of linear algebra (and to be fair, I always have a few of these people in my intro stats class), it will help you to know that the solution to the estimation problem turns out to be <span class="math inline">\(\hat{b} = (X^{'}X)^{-1}X^{'}y\)</span>, where <span class="math inline">\(\hat{b}\)</span> is a vector containing the estimated regression coefficients, <span class="math inline">\(X\)</span> is the “design matrix” that contains the predictor variables (plus an additional column containing all ones; strictly <span class="math inline">\(X\)</span> is a matrix of the regressors, but I haven’t discussed the distinction yet), and <span class="math inline">\(y\)</span> is a vector containing the outcome variable. For everyone else, this isn’t exactly helpful and can be downright scary. However, since quite a few things in linear regression can be written in linear algebra terms, you’ll see a bunch of footnotes like this one in this chapter. If you can follow the maths in them, great. If not, ignore it.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>The formula for the general case: The equation that I gave in the main text shows you what a multiple regression model looks like when you include two predictors. Not surprisingly then, if you want more than two predictors all you have to do is add more <span class="math inline">\(X\)</span> terms and more <span class="math inline">\(b\)</span> coefficients. In other words, if you have <span class="math inline">\(K\)</span> predictor variables in the model then the regression equation look like this: <span class="math display">\[Y_i=b_0+(\sum_{k=1}^{K}b_k X_{ik})+\epsilon_i\]</span><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>And by “sometimes” I mean “almost never”. In practice everyone just calls it “R-squared”.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>The adjusted <span class="math inline">\(R^2\)</span> value introduces a slight change to the calculation, as follows. For a regression model with <span class="math inline">\(K\)</span> predictors, fit to a data set containing <span class="math inline">\(N\)</span> observations, the adjusted <span class="math inline">\(R^2\)</span> is: <span class="math display">\[\text{adj.}R^2=1-(\frac{SS_{res}}{SS_{tot}} \times \frac{N-1}{N-K-1})\]</span><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Formally, our “null model” corresponds to the fairly trivial “regression” model in which we include 0 predictors and only include the intercept term <span class="math inline">\(b_0\)</span>: <span class="math inline">\(H_0:Y_0=b_0+\epsilon_i\)</span> If our regression model has <span class="math inline">\(K\)</span> predictors, the “alternative model” is described using the usual formula for a multiple regression model: <span class="math display">\[H_1:Y_i=b_0+(\sum_{k=1}^K b_k X_{ik})+\epsilon_i\]</span> How can we test these two hypotheses against each other? The trick is to understand that it’s possible to divide up the total variance <span class="math inline">\(SStot\)</span> into the sum of the residual variance <span class="math inline">\(SSres\)</span> and the regression model variance <span class="math inline">\(SSmod\)</span>. I’ll skip over the technicalities, since we’ll get to that later when we look at ANOVA in <a href="13-Comparing-several-means-one-way-ANOVA.html"><span>Chapter&nbsp;13</span></a>. But just note that <span class="math inline">\(SS_{mod}=SS_{tot}-SS_{res}\)</span> And we can convert the sums of squares into mean squares by dividing by the degrees of freedom: <span class="math display">\[MS_{mod}=\frac{SS_{mod}}{df_{mod}}\]</span> <span class="math display">\[MS_{res}=\frac{SS_{res}}{df_{res}}\]</span> So, how many degrees of freedom do we have? As you might expect the df associated with the model is closely tied to the number of predictors that we’ve included. In fact, it turns out that <span class="math inline">\(df_mod = K\)</span>. For the residuals the total degrees of freedom is <span class="math inline">\(df_res = N - K - 1\)</span>. Now that we’ve got our mean square values we can calculate an <span class="math inline">\(F\)</span>-statistic like this: <span class="math display">\[F=\frac{MS_{mod}}{MS_{res}}\]</span> and the degrees of freedom associated with this are <span class="math inline">\(K\)</span> and <span class="math inline">\(N - K - 1\)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>For advanced readers only. The vector of residuals is <span class="math inline">\(\epsilon=y - X\hat{b}\)</span>. For K predictors plus the intercept, the estimated residual variance is <span class="math inline">\(\hat{\sigma}^2 = \frac{\epsilon^{'}\epsilon}{(N - K - 1)}\)</span>. The estimated covariance matrix of the coefficients is <span class="math inline">\(\hat{\sigma}^{2}(X^{'}X)^{-1}\)</span>, the main diagonal of which is <span class="math inline">\(se(\hat{b})\)</span>, our estimated standard errors.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Note that, although jamovi has done multiple tests here, it hasn’t done a Bonferroni correction or anything (see <a href="13-Comparing-several-means-one-way-ANOVA.html"><span>Chapter&nbsp;13</span></a>). These are standard one-sample <span class="math inline">\(t\)</span>-tests with a two-sided alternative. If you want to make corrections for multiple tests, you need to do that yourself.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Fortunately, confidence intervals for the regression weights can be constructed in the usual fashion <span class="math inline">\(CI(b)=\hat{b} \pm (t_{crit} \times SE(\hat{b}))\)</span> where <span class="math inline">\(se(\hat{b})\)</span> is the standard error of the regression coefficient, and <span class="math inline">\(t_crit\)</span> is the relevant critical value of the appropriate <span class="math inline">\(t\)</span>-distribution. For instance, if it’s a 95% confidence interval that we want, then the critical value is the <span class="math inline">\(97.5\)</span>th quantile of a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(N -K -1\)</span> degrees of freedom. In other words, this is basically the same approach to calculating confidence intervals that we’ve used throughout.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Strictly, you standardise all the <em>regressors</em>. That is, every “thing” that has a regression coefficient associated with it in the model. For the regression models that I’ve talked about so far, each predictor variable maps onto exactly one regressor, and vice versa. However, that’s not actually true in general and we’ll see some examples of this later in <a href="14-Factorial-ANOVA.html"><span>Chapter&nbsp;14</span></a>. But, for now we don’t need to care too much about this distinction.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Leaving aside the interpretation issues, let’s look at how it’s calculated. What you could do is standardise all the variables yourself and then run a regression, but there’s a much simpler way to do it. As it turns out, the <span class="math inline">\(\beta\)</span> coefficient for a predictor <span class="math inline">\(X\)</span> and outcome <span class="math inline">\(Y\)</span> has a very simple formula, namely <span class="math inline">\(\beta_X=b_X \times \frac{\sigma_X}{\sigma_Y}\)</span> where <span class="math inline">\(\sigma_X\)</span> is the standard deviation of the predictor, and <span class="math inline">\(\sigma_Y\)</span> is the standard deviation of the outcome variable <span class="math inline">\(Y\)</span>. This makes matters a lot simpler.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>The way we calculate these is to divide the ordinary residual by an estimate of the (population) standard deviation of these residuals. For technical reasons, the formula for this is: <span class="math display">\[\epsilon_i^{'}=\frac{\epsilon_i}{\hat{\sigma}\sqrt{1-h_i}}\]</span> where <span class="math inline">\(\hat{\sigma}\)</span> in this context is the estimated population standard deviation of the ordinary residuals, and <span class="math inline">\(h_i\)</span> is the “hat value” of the <span class="math inline">\(i\)</span>th observation. I haven’t explained hat values to you yet, so this won’t make a lot of sense. For now, it’s enough to interpret the standardised residuals as if we’d converted the ordinary residuals to <em>z</em>-scores.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>The formula for doing the calculations this time is subtly different <span class="math inline">\(\epsilon _i^*=\frac{\epsilon_i}{\hat{\sigma}_{(-i)}\sqrt{1-h_i}}\)</span> Notice that our estimate of the standard deviation here is written <span class="math inline">\(\hat{\sigma}_{(-i)}\)</span>. What this corresponds to is the estimate of the residual standard deviation that you would have obtained if you just deleted the <span class="math inline">\(i\)</span>th observation from the data set. This sounds like the sort of thing that would be a nightmare to calculate, since it seems to be saying that you have to run <span class="math inline">\(N\)</span> new regression models (even a modern computer might grumble a bit at that, especially if you’ve got a large data set). Fortunately, this standard deviation estimate is actually given by the following equation: <span class="math inline">\(\hat{\sigma}_{(-i)}= \hat{\sigma}\sqrt{\frac{N-K-1-{\epsilon_i^{'}}^2}{N-K-2}}\)</span>.<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p><span class="math inline">\(f(x,\lambda)=\frac{x^{\lambda}-1}{\lambda}\)</span> for all values of <span class="math inline">\(\lambda\)</span> except <span class="math inline">\(\lambda = 0\)</span>. When <span class="math inline">\(\lambda = 0\)</span> we just take the natural logarithm (i.e., ln(<span class="math inline">\(x\)</span>).<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>In jamovi, you can compute this new variable using the formula ‘SQRT(ABS(Residuals))’.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>It’s a bit beyond the scope of this chapter to talk about how to deal with violations of homogeneity of variance, but I’ll give you a quick sense of what you need to consider. The <strong>main</strong> thing to worry about, if homogeneity of variance is violated, is that the standard error estimates associated with the regression coefficients are no longer entirely reliable, and so your <span class="math inline">\(t\)</span>-tests for the coefficients aren’t quite right either. A simple fix to the problem is to make use of a “heteroscedasticity corrected covariance matrix” when estimating the standard errors. These are often called <strong><em>sandwich estimators</em></strong>, and these can be estimated in <span class="math inline">\(R\)</span> (but not directly in jamovi).<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>The formula for the <span class="math inline">\(k\)</span>-th VIF is: <span class="math inline">\(VIF_k=\frac{1}{1-R^2_{(-k)}}\)</span> where <span class="math inline">\(R^2_{(-k)}\)</span> refers to R-squared value you would get if you ran a regression using <span class="math inline">\(X_k\)</span> as the outcome variable, and all the other <span class="math inline">\(X\)</span> variables as the predictors. The idea here is that <span class="math inline">\(R^2_{(-k)}\)</span> is a very good measure of the extent to which <span class="math inline">\(X_k\)</span> is correlated with all the other variables in the model. Better yet, the square root of the VIF is pretty interpretable: it tells you how much wider the confidence interval for the corresponding coefficient <span class="math inline">\(b_k\)</span> is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>Again, for the linear algebra fanatics: the “hat matrix” is defined to be that matrix <span class="math inline">\(H\)</span> that converts the vector of observed values <span class="math inline">\(y\)</span> into a vector of predicted values <span class="math inline">\(\hat{y}\)</span>, such that <span class="math inline">\(\hat{y} = Hy\)</span>. The name comes from the fact that this is the matrix that “puts a hat on y”. The hat value of the i-th observation is the i-th diagonal element of this matrix (so technically I should be writing it as <span class="math inline">\(h_{ii}\)</span> rather than <span class="math inline">\(h_i\)</span>). And here’s how it’s calculated: <span class="math inline">\(H = X(X^{'}X)^{1}X^{'}\)</span>.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p><span class="math inline">\(D_i=\frac{{\epsilon_i^*}^2}{K+1} \times \frac{h_i}{1-h_i}\)</span> Notice that this is a multiplication of something that measures the outlier-ness of the observation (the bit on the left), and something that measures the leverage of the observation (the bit on the right).<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>In jamovi you can save the Cook’s distance values to the data set, then draw a boxplot of the Cook’s distance values to identify the specific outliers. Or you could use a more powerful regression program such as the “car” package in R which has more options for advanced regression diagnostic analysis.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>In the context of a linear regression model (and ignoring terms that don’t depend on the model in any way!), the AIC for a model that has <span class="math inline">\(K\)</span> predictor variables plus an intercept is <span class="math inline">\(AIC=\frac{SS_{res}}{\hat{\sigma}^2}+2K\)</span>.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>While I’m on this topic I should point out that the empirical evidence suggests that BIC is a better criterion than AIC. In most simulation studies that I’ve seen, BIC does a much better job of selecting the correct model.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>We can fit both models to the data and obtain a residual sum of squares for both models. I’ll denote these as: <span class="math inline">\(SS_{res}^{(1)}\)</span> and <span class="math inline">\(SS_{res}^{(2)}\)</span> respectively. The superscripting here just indicates which model we’re talking about. Then our <span class="math inline">\(F\)</span> statistic is: <span class="math display">\[F= \frac {\frac {SS _{res}^{(1)} - SS_{res}^{(2)}} {k}}   {\frac{SS_{res}^2} {N-p-1} }\]</span> where <span class="math inline">\(N\)</span> is the number of observations, <span class="math inline">\(p\)</span> is the number of predictors in the full model (not including the intercept), and <span class="math inline">\(k\)</span> is the difference in the number of parameters between the two models.<span class="math inline">\(^d\)</span> The degrees of freedom here are <span class="math inline">\(k\)</span> and <span class="math inline">\(N -p-1\)</span>. Note that it’s often more convenient to think about the difference between those two <span class="math inline">\(SS\)</span> values as a sum of squares in its own right. That is: <span class="math display">\[SS_\Delta=SS_{res}^{(1)}-SS_{res}^{(2)}\]</span> The reason why this is helpful is that we can express <span class="math inline">\(SS_\Delta\)</span> as a measure of the extent to which the two models make different predictions about the the outcome variable. Specifically: <span class="math display">\[SS_\Delta=\sum_i{(\hat{y}_i^{(2)}-\hat{y}_i^{(1)})^2}\]</span> where <span class="math inline">\(\hat{y}_{i^{(1)}}\)</span> is the predicted value for <span class="math inline">\(y_i\)</span> according to model <span class="math inline">\(M_1\)</span> and <span class="math inline">\(\hat{y}_{i^{(2)}}\)</span> is the predicted value for <span class="math inline">\(y_i\)</span> according to model <span class="math inline">\(M_2\)</span>. <br> — <br> <span class="math inline">\(^d\)</span> It’s worth noting in passing that this same <span class="math inline">\(F\)</span> statistic can be used to test a much broader range of hypotheses than those that I’m mentioning here. Very briefly, notice that the nested model <span class="math inline">\(M_1\)</span> corresponds to the full model <span class="math inline">\(M_2\)</span> when we constrain some of the regression coefficients to zero. It is sometimes useful to construct sub-models by placing other kinds of constraints on the regression coefficients. For instance, maybe two different coefficients might have to sum to zero. You can construct hypothesis tests for those kind of constraints too, but it is somewhat more complicated and the sampling distribution for <span class="math inline">\(F\)</span> can end up being something known as the non-central <span class="math inline">\(F\)</span>-distribution, which is way beyond the scope of this book! All I want to do is alert you to this possibility.<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/davidfoxcroft\.github\.io\/lsj-book\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11-Comparing-two-means.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Comparing two means</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13-Comparing-several-means-one-way-ANOVA.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Comparing several means (one-way ANOVA)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left"><a href="https://learnstatswithjamovi.com">learning statistics with jamovi</a></div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This book was built with <a href="https://quarto.org/">Quarto</a></div>
  </div>
</footer>
<script type="text/javascript">
var elements = document.querySelectorAll('[aria-label]');

for (var i = 0; i < elements.length; i++) {
  var ariaLabel = elements[i].getAttribute('aria-label');
  elements[i].setAttribute('data-bs-title', ariaLabel);
  elements[i].setAttribute('data-bs-toggle', 'tooltip');
}
const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]')
const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl))
</script>



</body></html>