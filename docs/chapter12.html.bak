<!doctype html>
<html>
<head>

<meta charset="utf-8" />
<title>chapitre 10 </title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>


<h1> 12. Correlation and linear regression </h1>



The goal in this chapter is to introduce correlation and linear regression. These are the
standard tools that statisticians rely on when analysing the relationship between continuous
predictors and continuous outcomes.



<h2> 12.1 Correlations </h2>

In this section weâ€™ll talk about how to describe the relationships between variables in the data.
To do that, we want to talk mostly about the correlation between variables. But first, we need
some data.


<h3> 12.1.1  The data </h3>





<table width="50%" border="1" ><caption>Table 12.1: Descriptive statistics for the parenthood data.</caption>
	<tr>
		<td> variable  </td>
		<td>  min </td>
		<td>  max </td>
		<td>  mean </td>
		<td>  median </td>
		<td>  std. dev </td>
		<td> IQR </td>
	</tr>
	<tr>
		<td> Danâ€™s grumpiness </td>
		<td>  41 </td>
		<td>   91 </td>
		<td>  63.71 </td>
		<td>   62 </td>
		<td>   10.05 </td>
		<td>  14 </td>
	</tr>
	<tr>
		<td> Danâ€™s hours slept </td>
		<td>  4.84 </td>
		<td>   9.00 </td>
		<td> 6.97 </td>
		<td>  7.03 </td>
		<td>  1.02 </td>
		<td> 1.45 </td>
	</tr>
	<tr>
		<td> Danâ€™s sonâ€™s hours slept  </td>
		<td>  3.25 </td>
		<td>   12.07 </td>
		<td> 8.05 </td>
		<td>   7.95 </td>
		<td>   2.07 </td>
		<td> 3.21 </td>
	</tr>
</table>


Letâ€™s turn to a topic close to every parentâ€™s heart: sleep. The data set weâ€™ll use is fictitious,
but based on real events. Suppose Iâ€™m curious to find out how much my infant sonâ€™s sleeping
habits affect my mood. Letâ€™s say that I can rate my grumpiness very precisely, on a scale from
0 (not at all grumpy) to 100 (grumpy as a very, very grumpy old man or woman). And lets also
assume that Iâ€™ve been measuring my grumpiness, my sleeping patterns and my sonâ€™s sleeping
patterns for quite some time now. Letâ€™s say, for 100 days. And, being a nerd, Iâ€™ve saved the
data as a file called parenthood.csv. If we load the data we can see that the file contains four
variables dan.sleep, baby.sleep, dan.grump and day. Note that when you first load this data set
jamovi may not have guessed the data type for each variable correctly, in which case you should
fix it: dan.sleep, baby.sleep, dan.grump and day can be specified as continuous variables, and
ID is a nominal(integer) variable.<a href="#note1">[1]</a>

Next, Iâ€™ll take a look at some basic descriptive statistics and, to give a graphical depiction
of what each of the three interesting variables looks like, Figure 12.1 plots histograms. One
thing to note: just because jamovi can calculate dozens of different statistics doesnâ€™t mean you
should report all of them. If I were writing this up for a report, Iâ€™d probably pick out those
statistics that are of most interest to me (and to my readership), and then put them into a nice,
simple table like the one in Table 12.1.
<a href="#note2">[2]</a> Notice that when I put it into a table, I gave everything
â€œhuman readableâ€ names. This is always good practice. Notice also that Iâ€™m not getting enough
sleep. This isnâ€™t good practice, but other parents tell me that itâ€™s pretty standard.


<figure>
<figcaption>Figure 12.1: Histograms for the three interesting variables in the parenthood data set.
</figcaption>
<img src="images/Figure105.PNG" alt="Figure 12.1" />
</figure>




<h3> 12.1.2  The strength and direction of a relationship </h3>


We can draw scatterplots to give us a general sense of how closely related two variables are. 
Ideally though, we might want to say a bit more about it than that. For instance, letâ€™s compare the
relationship between dan.sleep and dan.grump (Figure 12.2, left) with that between baby.sleep
and dan.grump (Figure 12.2, right). When looking at these two plots side by side, itâ€™s clear that
the relationship is qualitatively the same in both cases: more sleep equals less grump! However,
itâ€™s also pretty obvious that the relationship between dan.sleep and dan.grump is stronger than
the relationship between baby.sleep and dan.grump. The plot on the left is â€œneaterâ€ than the
one on the right. What it feels like is that if you want to predict what my mood is, itâ€™d help
you a little bit to know how many hours my son slept, but itâ€™d be more helpful to know how
many hours I slept.


<figure>
<figcaption>Figure 12.2: Scatterplots showing the relationship between dan.sleep and dan.grump (left) and
the relationship between baby.sleep and dan.grump (right).
</figcaption>
<img src="images/Figure106.PNG" alt="Figure 12.2" />
</figure>


In contrast, letâ€™s consider the two scatterplots shown in Figure 12.3. If we compare the
scatterplot of â€œbaby.sleep v dan.grumpâ€ (left) to the scatterplot of â€œâ€˜baby.sleep v dan.sleepâ€
(right), the overall strength of the relationship is the same, but the direction is different. That
is, if my son sleeps more, I get more sleep (positive relationship, right hand side), but if he sleeps
more then I get less grumpy (negative relationship, left hand side).



<h3> 12.1.3  The correlation coefficient </h3>

We can make these ideas a bit more explicit by introducing the idea of a correlation coefficient 
(or, more specifically, Pearsonâ€™s correlation coefficient), which is traditionally denoted as
r. The correlation coefficient between two variables X and Y (sometimes denoted rXY ), which
weâ€™ll define more precisely in the next section, is a measure that varies from Â´1 to 1. When
r â€œ Â´1 it means that we have a perfect negative relationship, and when r â€œ 1 it means we
have a perfect positive relationship. When r â€œ 0, thereâ€™s no relationship at all. If you look at
Figure 12.4, you can see several plots showing what different correlations look like.



<figure>
<figcaption>Figure 12.3: Scatterplots showing the relationship between baby.sleep and dan.grump (left), as
compared to the relationship between baby.sleep and dan.sleep (right).
</figcaption>
<img src="images/Figure107.PNG" alt="Figure 12.3" />
</figure>




<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> The formula for the Pearsonâ€™s correlation coefficient can be written in several different ways.
I think the simplest way to write down the formula is to break it into two steps. Firstly,
letâ€™s introduce the idea of a covariance. The covariance between two variables X and Y is a
generalisation of the notion of the variance amd is a mathematically simple way of describing
the relationship between two variables that isnâ€™t terribly informative to humans

$$Cov(X,Y)=\frac{1}{N-1}\sum_{i=1}^N(X_i-\bar{X})(Y_i-\bar{Y})$$

Because weâ€™re multiplying (i.e., taking the â€œproductâ€ of) a quantity that depends on X by
a quantity that depends on Y and then averaging<a href="#notea">[a]</a>
, you can think of the formula for the
covariance as an â€œaverage cross productâ€ between X and Y .

The covariance has the nice property that, if X and Y are entirely unrelated, then the
covariance is exactly zero. If the relationship between them is positive (in the sense shown in
Figure 12.4) then the covariance is also positive, and if the relationship is negative then the
covariance is also negative. In other words, the covariance captures the basic qualitative idea
of correlation. Unfortunately, the raw magnitude of the covariance isnâ€™t easy to interpret as
it depends on the units in which X and Y are expressed and, worse yet, the actual units that
the covariance itself is expressed in are really weird. For instance, if X refers to the dan.sleep
variable (units: hours) and Y refers to the dan.grump variable (units: grumps), then the units
for their covariance are â€œhours Ë† grumpsâ€. And I have no freaking idea what that would even
mean.

The Pearson correlation coefficient r fixes this interpretation problem by standardising the
covariance, in pretty much the exact same way that the z-score standardises a raw score, by
dividing by the standard deviation. However, because we have two variables that contribute
to the covariance, the standardisation only works if we divide by both standard deviations.<a href="#noteb">[b]</a>
In other words, the correlation between X and Y can be written as follows:

$$r_{XY}=\frac{Cov(X,Y)}{\hat{\sigma}_X\hat{\sigma}_Y}$$



 </td>
	</tr>
	
<tr>
 <td>
	<p id="notea">[a] Just like we saw with the variance and the standard deviation, in practice we divide by N Â´ 1 rather than
N.
</p>
	<p id="noteb">[b] This is an oversimplification, but itâ€™ll do for our purposes.
</p>
 </td>
	</tr>

</table>


<figure>
<figcaption>Figure 12.4: Illustration of the effect of varying the strength and direction of a correlation. In
the left hand column, the correlations are 0, .33, .66 and 1. In the right hand column, the
correlations are 0, -.33, -.66 and -1.
</figcaption>
<img src="images/Figure108.PNG" alt="Figure 12.4" />
</figure>


By standardising the covariance, not only do we keep all of the nice properties of the covariance discussed 
earlier, but the actual values of r are on a meaningful scale: r â€œ 1 implies a
perfect positive relationship and r â€œ Â´1 implies a perfect negative relationship. Iâ€™ll expand a
little more on this point later, in Section 12.1.5. But before I do, letâ€™s look at how to calculate
correlations in jamovi.



<h3> 12.1.4  Calculating correlations in jamovi </h3>


Calculating correlations in jamovi can be done by clicking on the â€˜Regressionâ€™ - â€˜Correlation
Matrixâ€™ button. Transfer all four continuous variables across into the box on the right to get
the output in Figure 12.5.



<h3> 12.1.5  Interpreting a correlation </h3>


Naturally, in real life you donâ€™t see many correlations of 1. So how should you interpret a
correlation of, say, r â€œ .4? The honest answer is that it really depends on what you want to use
the data for, and on how strong the correlations in your field tend to be. A friend of mine in
engineering once argued that any correlation less than .95 is completely useless (I think he was
exaggerating, even for engineering). On the other hand, there are real cases, even in psychology,
where you should really expect correlations that strong. For instance, one of the benchmark
data sets used to test theories of how people judge similarities is so clean that any theory that
canâ€™t achieve a correlation of at least .9 really isnâ€™t deemed to be successful. However, when
looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if
you get a correlation above .3 youâ€™re doing very very well. In short, the interpretation of a
correlation depends a lot on the context. That said, the rough guide in Table 12.2 is pretty
typical.


However, something that can never be stressed enough is that you should always look at the
scatterplot before attaching any interpretation to the data. A correlation might not mean what
you think it means. The classic illustration of this is â€œAnscombeâ€™s Quartetâ€ (Anscombe 1973),
a collection of four data sets. Each data set has two variables, an X and a Y . For all four data
sets the mean value for X is 9 and the mean for Y is 7.5. The standard deviations for all X
variables are almost identical, as are those for the Y variables. And in each case the correlation
between X and Y is r â€œ 0.816. You can verify this yourself, since I happen to have saved it in
a file called anscombe.csv.


<figure>
<figcaption>Figure 12.5: A jamovi screenshot showing correlations between variables in the parenthood.csv
file
</figcaption>
<img src="images/Figure109.PNG" alt="Figure 12.5" />
</figure>




Youâ€™d think that these four data sets would look pretty similar to one another. They do not.
If we draw scatterplots of X against Y for all four variables, as shown in Figure 12.6, we see
that all four of these are spectacularly different to each other. The lesson here, which so very
many people seem to forget in real life, is â€œalways graph your raw dataâ€ (Chapter 5).




<h3> 12.1.6  Spearmanâ€™s rank correlations </h3>


The Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings.
One issue in particular stands out: what it actually measures is the strength of the linear
relationship between two variables. In other words, what it gives you is a measure of the extent
to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good
approximation to what we mean when we say â€œrelationshipâ€, and so the Pearson correlation is
a good thing to calculate. Sometimes though, it isnâ€™t.

One very common situation where the Pearson correlation isnâ€™t quite the right thing to use
arises when an increase in one variable X really is reflected in an increase in another variable
Y , but the nature of the relationship isnâ€™t necessarily linear. An example of this might be the
relationship between effort and reward when studying for an exam. If you put zero effort (X)
into learning a subject then you should expect a grade of 0% (Y ). However, a little bit of effort
will cause a massive improvement. Just turning up to lectures means that you learn a fair bit,
and if you just turn up to classes and scribble a few things down your grade might rise to 35%,
all without a lot of effort. However, you just donâ€™t get the same effect at the other end of the
scale. As everyone knows, it takes a lot more effort to get a grade of 90% than it takes to get
a grade of 55%. What this means is that, if Iâ€™ve got data looking at study effort and grades,
thereâ€™s a pretty good chance that Pearson correlations will be misleading.


<table width="50%" border="1" ><caption>Table 12.2: A rough guide to interpreting correlations. Note that I say a rough guide. There
arenâ€™t hard and fast rules for what counts as strong or weak relationships. It depends on the
context.</caption>
	<tr>
		<td> Correlation  </td>
		<td>  Strength </td>
		<td>  Direction </td>
	</tr>
	<tr>
		<td> -1.0 to -0.9 </td>
		<td>  Very strong </td>
		<td>   Negative </td>
	</tr>
	<tr>
		<td> -0.9 to -0.7 </td>
		<td>  Strong </td>
		<td>   Negative
 </td>
	</tr>
	<tr>
		<td> -0.7 to -0.4  </td>
		<td>  Moderate </td>
		<td>   Negative </td>
	</tr>
	<tr>
		<td> -0.4 to -0.2  </td>
		<td>  Weak </td>
		<td>  Negative </td>
	</tr>
	<tr>
		<td> -0.2 to 0   </td>
		<td>  Negligible </td>
		<td>   Negative </td>
	</tr>
	<tr>
		<td> 0 to 0.2   </td>
		<td>  Negligible </td>
		<td>    Positive </td>
	</tr>
	<tr>
		<td> 0.2 to 0.4   </td>
		<td>  Weak </td>
		<td>    Positive </td>
	</tr>
	<tr>
		<td> 0.4 to 0.7  </td>
		<td> Moderate </td>
		<td>    Positive </td>
	</tr>
	<tr>
		<td> 0.7 to 0.9  </td>
		<td> Strong </td>
		<td>   Positive </td>
	</tr>
	<tr>
		<td> 0.9 to 1.0  </td>
		<td> Very strong </td>
		<td>   Positive </td>
	</tr>
</table>


To illustrate, consider the data plotted in Figure 12.7, showing the relationship between
hours worked and grade received for 10 students taking some class. The curious thing about
this (highly fictitious) data set is that increasing your effort always increases your grade. It
might be by a lot or it might be by a little, but increasing effort will never decrease your grade.
If we run a standard Pearson correlation, it shows a strong relationship between hours worked
and grade received, with a correlation coefficient of 0.91. However, this doesnâ€™t actually capture
the observation that increasing hours worked always increases the grade. Thereâ€™s a sense here
in which we want to be able to say that the correlation is perfect but for a somewhat different
notion of what a â€œrelationshipâ€ is. What weâ€™re looking for is something that captures the fact
that there is a perfect ordinal relationship here. That is, if student 1 works more hours than
student 2, then we can guarantee that student 1 will get the better grade. Thatâ€™s not what a
correlation of r â€œ .91 says at all.


How should we address this? Actually, itâ€™s really easy. If weâ€™re looking for ordinal relationships 
all we have to do is treat the data as if it were ordinal scale! So, instead of measuring
effort in terms of â€œhours workedâ€, lets rank all 10 of our students in order of hours worked.
That is, student 1 did the least work out of anyone (2 hours) so they get the lowest rank (rank
= 1). Student 4 was the next laziest, putting in only 6 hours of work over the whole semester,
so they get the next lowest rank (rank = 2). Notice that Iâ€™m using â€œrank =1â€ to mean â€œlow
rankâ€. Sometimes in everyday language we talk about â€œrank = 1â€ to mean â€œtop rankâ€ rather
than â€œbottom rankâ€. So be careful, you can rank â€œfrom smallest value to largest valueâ€ (i.e.,
small equals rank 1) or you can rank â€œfrom largest value to smallest valueâ€ (i.e., large equals
rank 1). In this case, Iâ€™m ranking from smallest to largest, but as itâ€™s really easy to forget which
way you set things up you have to put a bit of effort into remembering!


<figure>
<figcaption>Figure 12.6: Anscombeâ€™s quartet. All four of these data sets have a Pearson correlation of
r â€œ .816, but they are qualitatively different from one another.
</figcaption>
<img src="images/Figure110.PNG" alt="Figure 12.6" />
</figure>


Okay, so letâ€™s have a look at our students when we rank them from worst to best in terms
of effort and reward:


<figure>
<figcaption>Figure 12.7: The relationship between hours worked and grade received for a toy data set
consisting of only 10 students (each circle corresponds to one student). The dashed line through
the middle shows the linear relationship between the two variables. This produces a strong
Pearson correlation of r â€œ .91. However, the interesting thing to note here is that thereâ€™s actually
a perfect monotonic relationship between the two variables. In this toy example, increasing the
hours worked always increases the grade received, as illustrated by the solid line. This is reflected
in a Spearman correlation of Ï â€œ 1. With such a small data set, however, itâ€™s an open question
as to which version better describes the actual relationship involved.
</figcaption>
<img src="images/Figure111.PNG" alt="Figure 12.7" />
</figure>


<table width="50%" border="1" ><caption>Table 12.2: A rough guide to interpreting correlations. Note that I say a rough guide. There
arenâ€™t hard and fast rules for what counts as strong or weak relationships. It depends on the
context.</caption>
	<tr>
		<td> &nbsp;  </td>
		<td>  rank (hours worked) </td>
		<td>  rank (grade received) </td>
	</tr>
	<tr>
		<td> student 1 </td>
		<td>  1 </td>
		<td>   1 </td>
	</tr>
	<tr>
		<td> student 2 </td>
		<td>  10 </td>
		<td>   10 </td>
	</tr>
	<tr>
		<td> student 3  </td>
		<td>  6 </td>
		<td>  6 </td>
	</tr>
	<tr>
		<td> student 4  </td>
		<td>  2 </td>
		<td> 2 </td>
	</tr>
	<tr>
		<td> student 5   </td>
		<td>  3 </td>
		<td>   3 </td>
	</tr>
	<tr>
		<td> student 6   </td>
		<td>  5 </td>
		<td> 5 </td>
	</tr>
	<tr>
		<td> student 7   </td>
		<td>  4 </td>
		<td>  4 </td>
	</tr>
	<tr>
		<td> student 8  </td>
		<td> 8 </td>
		<td> 8 </td>
	</tr>
	<tr>
		<td> student 9  </td>
		<td> 7 </td>
		<td>   7 </td>
	</tr>
	<tr>
		<td> student 10  </td>
		<td> 9 </td>
		<td>  9 </td>
	</tr>
</table>



Hmm. These are identical. The student who put in the most effort got the best grade, the student
with the least effort got the worst grade, etc. As the table above shows, these two rankings are
identical, so if we now correlate them we get a perfect relationship, with a correlation of 1.0.

What weâ€™ve just re-invented is Spearmanâ€™s rank order correlation, usually denoted Ï
to distinguish it from the Pearson correlation r. We can calculate Spearmanâ€™s Ï using jamovi
simply by clicking the â€˜Spearmanâ€™ check box in the â€˜Correlation Matrixâ€™ screen.


<h2> 12.2  Scatterplots </h2>

Scatterplots are a simple but effective tool for visualising the relationship between two variables,
 like we saw with the figures in the section on correlation (Section 12.1). Itâ€™s this latter
application that we usually have in mind when we use the term â€œscatterplotâ€. In this kind of plot
each observation corresponds to one dot. The horizontal location of the dot plots the value of
the observation on one variable, and the vertical location displays its value on the other variable.
In many situations you donâ€™t really have a clear opinions about what the causal relationship is
(e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If
thatâ€™s the case, it doesnâ€™t really matter which variable you plot on the x-axis and which one you
plot on the y-axis. However, in many situations you do have a pretty strong idea which variable
you think is most likely to be causal, or at least you have some suspicions in that direction.
If so, then itâ€™s conventional to plot the cause variable on the x-axis, and the effect variable on
the y-axis. With that in mind, letâ€™s look at how to draw scatterplots in jamovi, using the same
parenthood data set (i.e. parenthood.csv) that I used when introducing correlations.


Suppose my goal is to draw a scatterplot displaying the relationship between the amount of
sleep that I get (dan.sleep) and how grumpy I am the next day (dan.grump). There are two
different ways in which we can use jamovi to get the plot that weâ€™re after. The first way is to
use the â€˜Plotâ€™ option under the â€˜Regressionâ€™ - â€˜Correlation Matrixâ€™ button, giving us the output
shown in Figure 12.8. Note that jamovi draws a line through the points, weâ€™ll come onto this a
bit later in Section (12.3). Plotting a scatterplot in this way also allow you to specify â€˜Densities
for variablesâ€™ and this option adds a density curve showing how the data in each variable is
distributed.


The second way do to it is to use one of the jamovi add-on modules. This module is called
â€˜scatrâ€™ and you can install it by clicking on the large â€˜+â€™ icon in the top right of the jamovi screen,
opening the jamovi library, scrolling down until you find â€˜scatrâ€™ and clicking â€˜installâ€™. When you
have done this, you will find a new â€˜Scatterplotâ€™ command available under the â€˜Explorationâ€™
button. This plot is a bit different than the first way, see Figure 12.9, but the important
information is the same.

<h3> 12.2.1   More elaborate options </h3>


Often you will want to look at the relationships between several variables at once, using
a scatterplot matrix (in jamovi via the â€˜Correlation Matrixâ€™ - â€˜Plotâ€™ command). Just add
another variable, for example baby.sleep to the list of variables to be correlated, and jamovi
will create a scatterplot matrix for you, just like the one in Figure 12.10.


<figure>
<figcaption>Figure 12.8: Scatterplot via the â€˜Correlation Matrixâ€™ command in jamovi
</figcaption>
<img src="images/Figure110.PNG" alt="Figure 12.8" />
</figure>




<h2> 12.3   What is a linear regression model? </h2>



Stripped to its bare essentials, linear regression models are basically a slightly fancier version
of the Pearson correlation (Section 12.1), though as weâ€™ll see regression models are much more
powerful tools.


Since the basic ideas in regression are closely tied to correlation, weâ€™ll return to the
parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this
data set we were trying to find out why Dan is so very grumpy all the time and our working
hypothesis was that Iâ€™m not getting enough sleep. We drew some scatterplots to help us examine
the relationship between the amount of sleep I get and my grumpiness the following day, as in
Figure 12.9, and as we saw previously this corresponds to a correlation of r â€œ Â´.90, but what
we find ourselves secretly imagining is something that looks closer to Figure 12.11a. That is, we
mentally draw a straight line through the middle of the data. In statistics, this line that weâ€™re
drawing is called a regression line. Notice that, since weâ€™re not idiots, the regression line goes
through the middle of the data. We donâ€™t find ourselves imagining anything like the rather silly
plot shown in Figure 12.11b.


This is not highly surprising. The line that Iâ€™ve drawn in Figure 12.11b doesnâ€™t â€œfitâ€ the
data very well, so it doesnâ€™t make a lot of sense to propose it as a way of summarising the data,
right? This is a very simple observation to make, but it turns out to be very powerful when we
start trying to wrap just a little bit of maths around it. To do so, letâ€™s start with a refresher of
some high school maths. The formula for a straight line is usually written like this

$$y=a+bx$$


<figure>
<figcaption>Figure 12.9: Scatterplot via the â€˜scatrâ€™ add-on module in jamovi
</figcaption>
<img src="images/Figure113.PNG" alt="Figure 12.9" />
</figure>


<figure>
<figcaption>Figure 12.10: A matrix of scatterplots produced using jamovi.
</figcaption>
<img src="images/Figure114.PNG" alt="Figure 12.10" />
</figure>


<figure>
<figcaption>Figure 12.11: Panel a shows the sleep-grumpiness scatterplot from Figure 12.9 with the best
fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of
the data. In contrast, panel b shows the same data, but with a very poor choice of regression
line drawn over the top.
</figcaption>
<img src="images/Figure115.PNG" alt="Figure 12.11" />
</figure>









Or, at least, thatâ€™s what it was when I went to high school all those years ago. The two variables
are x and y, and we have two coefficients, a and b.
3 The coefficient a represents the y-intercept of
the line, and coefficient b represents the slope of the line. Digging further back into our decaying
memories of high school (sorry, for some of us high school was a long time ago), we remember
that the intercept is interpreted as â€œthe value of y that you get when x â€œ 0â€. Similarly, a slope
of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and
a negative slope means that the y-value would go down rather than up. Ah yes, itâ€™s all coming
back to me now. Now that weâ€™ve remembered that it should come as no surprise to discover that
we use the exact same formula for a regression line. If Y is the outcome variable (the DV) and
X is the predictor variable (the IV), then the formula that describes our regression is written
like this

$$\hat{Y}_i=b_0+b_1X_i$$


Hmm. Looks like the same formula, but thereâ€™s some extra frilly bits in this version. Letâ€™s make
sure we understand them. Firstly, notice that Iâ€™ve written Xi and Yi rather than just plain
old X and Y . This is because we want to remember that weâ€™re dealing with actual data. In
this equation, Xi
is the value of predictor variable for the ith observation (i.e., the number of
hours of sleep that I got on day i of my little study), and Yi
is the corresponding value of the
outcome variable (i.e., my grumpiness on that day). And although I havenâ€™t said so explicitly
in the equation, what weâ€™re assuming is that this formula works for all observations in the data
set (i.e., for all i). Secondly, notice that I wrote YË†
i and not Yi
. This is because we want to
make the distinction between the actual data Yi
, and the estimate YË†
i (i.e., the prediction that
our regression line is making). Thirdly, I changed the letters used to describe the coefficients
from a and b to b0 and b1. Thatâ€™s just the way that statisticians like to refer to the coefficients
in a regression model. Iâ€™ve no idea why they chose b, but thatâ€™s what they did. In any case b0
always refers to the intercept term, and b1 refers to the slope.


Excellent, excellent. Next, I canâ€™t help but notice that, regardless of whether weâ€™re talking
about the good regression line or the bad one, the data donâ€™t fall perfectly on the line. Or, to
say it another way, the data Yi are not identical to the predictions of the regression model YË†
i
.
Since statisticians love to attach letters, names and numbers to everything, letâ€™s refer to the
difference between the model prediction and that actual data point as a residual, and weâ€™ll refer
to it as i
.
4 Written using mathematics, the residuals are defined as


$$\epsilon_i=Y_i-\hat{Y}_i$$

which in turn means that we can write down the complete linear regression model as


$$Y_i=b_0+b_1X_i+\epsilon_i$$




<h2> 12.4   Estimating a linear regression model </h2>



<figure>
<figcaption>Figure 12.12: A depiction of the residuals associated with the best fitting regression line (panel
a), and the residuals associated with a poor regression line (panel b). The residuals are much
smaller for the good regression line. Again, this is no surprise given that the good line is the
one that goes right through the middle of the data.
</figcaption>
<img src="images/Figure116.PNG" alt="Figure 12.12" />
</figure>



<dl>
<dt>
Okay, now letâ€™s redraw our pictures but this time Iâ€™ll add some lines to show the size of the
residual for all observations. When the regression line is good, our residuals (the lengths of the
solid black lines) all look pretty small, as shown in Figure 12.12a, but when the regression line
is a bad one the residuals are a lot larger, as you can see from looking at Figure 12.12b. Hmm.
Maybe what we â€œwantâ€ in a regression model is small residuals. Yes, that does seem to make
sense. In fact, I think Iâ€™ll go so far as to say that the â€œbest fittingâ€ regression line is the one
that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of
everything why not say that:
</dt>
<dd>
The estimated regression coefficients, Ë†b0 and Ë†b1, are those that minimise the sum of
the squared residuals, which we could either write as Å™
i
pYi Â´ YË†
iq
2 or as Å™
i
i
2
.
</dd>
</dl>


Yes, yes that sounds even better. And since Iâ€™ve indented it like that, it probably means that
this is the right answer. And since this is the right answer, itâ€™s probably worth making a note
of the fact that our regression coefficients are estimates (weâ€™re trying to guess the parameters
that describe a population!), which is why Iâ€™ve added the little hats, so that we get Ë†b0 and Ë†b1
rather than b0 and b1. Finally, I should also note that, since thereâ€™s actually more than one way
to estimate a regression model, the more technical name for this estimation process is ordinary
least squares (OLS) regression.


At this point, we now have a concrete definition for what counts as our â€œbestâ€ choice of
regression coefficients, Ë†b0 and Ë†b1. The natural question to ask next is, if our optimal regression
coefficients are those that minimise the sum squared residuals, how do we find these wonderful
numbers? The actual answer to this question is complicated and doesnâ€™t help you understand
the logic of regression.5 This time Iâ€™m going to let you off the hook. Instead of showing you the
long and tedious way first and then â€œrevealingâ€ the wonderful shortcut that jamovi provides,
letâ€™s cut straight to the chase and just use jamovi to do all the heavy lifting.




<h3> 12.4.1   Linear regression in jamovi </h3>



To run my linear regression, open up the â€˜Regressionâ€™ - â€˜Linear Regressionâ€™ analysis in jamovi,
using the parenthood.csv data file. Then specify dan.grump as the â€˜Dependent Variableâ€™ and
dan.sleep as the variable entered in the â€˜Covariatesâ€™ box. This gives the results shown in Figure
12.13, showing an intercept Ë†b0 â€œ 125.96 and the slope Ë†b1 â€œ Â´8.94. In other words, the bestfitting 
regression line that I plotted in Figure 12.11 has this formula:


$$\hat{Y}_i=125.96+(-8.94 X_i)$$




<h3> 12.4.2   Interpreting the estimated model </h3>


The most important thing to be able to understand is how to interpret these coefficients.
Letâ€™s start with Ë†b1, the slope. If we remember the definition of the slope, a regression coefficient
of Ë†b1 â€œ Â´8.94 means that if I increase Xi by 1, then Iâ€™m decreasing Yi by 8.94. That is, each
additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94
grumpiness points. What about the intercept? Well, since Ë†b0 corresponds to â€œthe expected
value of Yi when Xi equals 0â€, itâ€™s pretty straightforward. It implies that if I get zero hours of
sleep (Xi â€œ 0) then my grumpiness will go off the scale, to an insane value of (Yi â€œ 125.96).
Best to be avoided, I think.


<figure>
<figcaption>Figure 12.13: A jamovi screenshot showing a simple linear regression analysis.
</figcaption>
<img src="images/Figure117.PNG" alt="Figure 12.13" />
</figure>



<h3> 12.5   Multiple linear regression </h3>

The simple linear regression model that weâ€™ve discussed up to this point assumes that thereâ€™s a
single predictor variable that youâ€™re interested in, in this case dan.sleep. In fact, up to this point
every statistical tool that weâ€™ve talked about has assumed that your analysis uses one predictor
variable and one outcome variable. However, in many (perhaps most) research projects you
actually have multiple predictors that you want to examine. If so, it would be nice to be able to
extend the linear regression framework to be able to include multiple predictors. Perhaps some
kind of multiple regression model would be in order?

Multiple regression is conceptually very simple. All we do is add more terms to our regression
equation. Letâ€™s suppose that weâ€™ve got two variables that weâ€™re interested in; perhaps we want to
use both dan.sleep and baby.sleep to predict the dan.grump variable. As before, we let Yi refer
to my grumpiness on the i-th day. But now we have two X variables: the first corresponding to
the amount of sleep I got and the second corresponding to the amount of sleep my son got. So
weâ€™ll let Xi1 refer to the hours I slept on the i-th day and Xi2 refers to the hours that the baby
slept on that day. If so, then we can write our regression model like this:

$$Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\epsilon_i$$


As before, i
is the residual associated with the i-th observation, i â€œ Yi Â´ YË†
i
. In this model, we
now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient
associated with my sleep, and b2 is the coefficient associated with my sonâ€™s sleep. However,
although the number of coefficients that need to be estimated has changed, the basic idea of
how the estimation works is unchanged: our estimated coefficients Ë†b0,
Ë†b1 and Ë†b2 are those that
minimise the sum squared residuals.


<figure>
<figcaption>Figure 12.14: A 3D visualisation of a multiple regression model. There are two predictors in
the model, dan.sleep and baby.sleep and the outcome variable is dan.grump. Together, these
three variables form a 3D space. Each observation (dot) is a point in this space. In much the
same way that a simple linear regression model forms a line in 2D space, this multiple regression
model forms a plane in 3D space. When we estimate the regression coefficients what weâ€™re trying
to do is find a plane that is as close to all the blue dots as possible.
</figcaption>
<img src="images/Figure118.PNG" alt="Figure 12.14" />
</figure>





<h3> 12.5.1   Doing it in jamovi </h3>



Multiple regression in jamovi is no different to simple regression. All we have to do is
add additional variables to the â€˜Covariatesâ€™ box in jamovi. For example, if we want to use both
dan.sleep and baby.sleep as predictors in our attempt to explain why Iâ€™m so grumpy, then move
baby.sleep across into the â€˜Covariatesâ€™ box alongside dan.sleep. By default, jamovi assumes
that the model should include an intercept. The coefficients we get this time are:


<table width="50%" border="1" ><caption></caption>
	<tr>
		<td> (Intercept) </td>
		<td> dan.sleep </td>
		<td>  baby.sleep </td>
	</tr>
	<tr>
		<td> 125.97  </td>
		<td>  -8.95 </td>
		<td>   0.01 </td>
	</tr>
</table>


The coefficient associated with dan.sleep is quite large, suggesting that every hour of sleep I
lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting
that it doesnâ€™t really matter how much sleep my son gets. What matters as far as my grumpiness
goes is how much sleep I get. To get a sense of what this multiple regression model looks like,
Figure 12.14 shows a 3D plot that plots all three variables, along with the regression model
itself.

<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 
<h3> 12.5.2   Formula for the general case </h3>

The equation that I gave above shows you what a multiple regression model looks like
when you include two predictors. Not surprisingly, then, if you want more than two predictors
all you have to do is add more X terms and more b coefficients. In other words, if you have
K predictor variables in the model then the regression equation looks like this

$$Y_i=b_0+(\sum_{k=1}^{K}b_k X_{ik})+\epsilon_i$$

 </td>
	</tr>
</table>



<h2> 12.6   Quantifying the fit of the regression model </h2>

So we now know how to estimate the coefficients of a linear regression model. The problem is,
we donâ€™t yet know if this regression model is any good. For example, the regression.1 model
claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish.
Remember, the regression model only produces a prediction YË†
i about what my mood is like, but
my actual mood is Yi
. If these two are very close, then the regression model has done a good
job. If they are very different, then it has done a bad job.



<h3> 12.6.1   The \(R^2\) value </h3>

Once again, letâ€™s wrap a little bit of mathematics around this. Firstly, weâ€™ve got the sum of the
squared residuals

$$SS_{res}=\sum_i (Y_i-\hat{Y_i})^2$$



which we would hope to be pretty small. Specifically, what weâ€™d like is for it to be very small
in comparison to the total variability in the outcome variable

$$SS_tot=\sum_i(Y_i-\bar{Y})^2$$



While weâ€™re here, letâ€™s calculate these values ourselves, not by hand though. Letâ€™s use something
like Excel or another standard spreadsheet programme. I have done this by opening up the
parenthood.csv file in Excel and saving it as parenthood rsquared.xls so that I can work on it.
The first thing to do is calculate the YË† values, and for the simple model that uses only a single
predictor we would do the following:



<ol>
<li>
create a new column called â€˜Y.predâ€™ using the formula â€˜= 125.97 + (-8.94 * dan.sleep)â€™
</li>
Okay, now that weâ€™ve got a variable which stores the regression model predictions for how
grumpy I will be on any given day, letâ€™s calculate our sum of squared residuals. We would do
that using the following formula:
<li>
calculate the SS(resid) by creating a new column called â€™(Y-Y.pred)^2â€™ using the formula
â€˜= (dan.grump - Y.pred)^2â€™.
</li>
<li>
Then, at the bottom of this column calculate the sum of these values,
i.e. â€˜sum( ( Y-Y.pred)^2 ).
</li>
This should give you a value of â€˜1838.722â€™. Wonderful. A big number that doesnâ€™t mean
very much. Still, letâ€™s forge boldly onwards anyway and calculate the total sum of squares as
well. Thatâ€™s also pretty simple. Calculate the SS(tot) by:
<li>
At the bottom of the dan.grump column, calculate the mean value for dan.grump
(NB Excel uses the word â€˜AVERAGEâ€™ rather than â€˜meanâ€™ in its function).
</li>
<li>
Then create a new column, called â€˜(Y - mean(Y))^2 )â€™ using the formula
â€˜= (dan.grump - AVERAGE(dan.grump))^2 â€™.
</li>
<li>
 Then, at the bottom of this column calculate the sum of these values,
i.e. â€˜sum( (Y - mean(Y))^2 )â€™.
</li>

This should give you a value of â€˜9998.59â€™. Hmm. Well, itâ€™s a much bigger number than the
last one, so this does suggest that our regression model was making good predictions. But itâ€™s
not very interpretable.



Perhaps we can fix this. What weâ€™d like to do is to convert these two fairly meaningless
numbers into one number. A nice, interpretable number, which for no particular reason weâ€™ll
call R2
. What we would like is for the value of R2
to be equal to 1 if the regression model makes
no errors in predicting the data. In other words, if it turns out that the residual errors are zero.
That is, if SSres â€œ 0 then we expect R2 â€œ 1. Similarly, if the model is completely useless, we
would like R2
to be equal to 0. What do I mean by â€œuselessâ€? Tempting as it is to demand
that the regression model move out of the house, cut its hair and get a real job, Iâ€™m probably
going to have to pick a more practical definition. In this case, all I mean is that the residual
sum of squares is no smaller than the total sum of squares, SSres â€œ SStot. Wait, why donâ€™t we
do exactly that? The formula that provides us with our R2 value is pretty simple to write down,
$$R^2=1-\frac{SS_{res}}{SS_{tot}}$$
and equally simple to calculate in Excel:
<li>
Calculate R.squared by typing into a blank cell the following:
â€˜= 1 - (SS(resid) / SS(tot) ) â€™.
</li>

</ol>


This gives a value for R2 of â€˜0.8161018â€™. The R2 value, sometimes called the coefficient of
determination6 has a simple interpretation: it is the proportion of the variance in the outcome
variable that can be accounted for by the predictor. So, in this case the fact that we have
obtained R2 â€œ .816 means that the predictor (my.sleep) explains 81.6% of the variance in the
outcome (my.grump).


Naturally, you donâ€™t actually need to type all these commands into Excel yourself if you
want to obtain the R2 value for your regression model. As weâ€™ll see later on in Section 12.7.3,
all you need to do is specify this as an option in jamovi. However, letâ€™s put that to one side for
the moment. Thereâ€™s another property of R2
that I want to point out.



<h3> 12.6.2   The relationship between regression and correlation </h3>


At this point we can revisit my earlier claim that regression, in this very simple form that
Iâ€™ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol
r to denote a Pearson correlation. Might there be some relationship between the value of the
correlation coefficient r and the R2 value from linear regression? Of course there is: the squared
correlation r
2
is identical to the R2 value for a linear regression with only a single predictor.
In other words, running a Pearson correlation is more or less equivalent to running a linear
regression model that uses only one predictor variable.



<h3> 12.6.3   The adjusted \(R^2\) value </h3>


One final thing to point out before moving on. Itâ€™s quite common for people to report
a slightly different measure of model performance, known as â€œadjusted R2â€. The motivation
behind calculating the adjusted R2 value is the observation that adding more predictors into
the model will always cause the R2 value to increase (or at least not decrease).

<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> The adjusted \(R^2\) value introduces a slight change to the calculation, as follows. For a regression
model with \(K\) predictors, fit to a data set containing \(N\) observations, the adjusted \(R^2\)
is:
$$\text{adj. }R^2=1-(\frac{SS_{res}}{SS_{tot}} \times \frac{N-1}{N-K-1})$$
 </td>
	</tr>
</table>


This adjustment is an attempt to take the degrees of freedom into account. The big advantage
of the adjusted R2 value is that when you add more predictors to the model, the adjusted R2
value will only increase if the new variables improve the model performance more than youâ€™d
expect by chance. The big disadvantage is that the adjusted R2 value canâ€™t be interpreted in
the elegant way that R2
can. R2 has a simple interpretation as the proportion of variance in the
outcome variable that is explained by the regression model. To my knowledge, no equivalent
interpretation exists for adjusted R2.



An obvious question then is whether you should report R2 or adjusted R2
. This is probably
a matter of personal preference. If you care more about interpretability, then R2
is better. If
you care more about correcting for bias, then adjusted R2
is probably better. Speaking just
for myself, I prefer R2
. My feeling is that itâ€™s more important to be able to interpret your
measure of model performance. Besides, as weâ€™ll see in Section 12.7, if youâ€™re worried that the
improvement in R2
that you get by adding a predictor is just due to chance and not because
itâ€™s a better model, well weâ€™ve got hypothesis tests for that.

<h2> 12.7   Hypothesis tests for regression models </h2>


So far weâ€™ve talked about what a regression model is, how the coefficients of a regression model
are estimated, and how we quantify the performance of the model (the last of these, incidentally,
is basically our measure of effect size). The next thing we need to talk about is hypothesis tests.
There are two different (but related) kinds of hypothesis tests that we need to talk about: those
in which we test whether the regression model as a whole is performing significantly better than
a null model, and those in which we test whether a particular regression coefficient is significantly
different from zero.

<h3> 12.7.1   Testing the model as a whole </h3>



Okay, suppose youâ€™ve estimated your regression model. The first hypothesis test you might try
is the null hypothesis that there is no relationship between the predictors and the outcome, and
the alternative hypothesis that the data are distributed in exactly the way that the regression
model predicts.


<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> Formally, our â€œnull modelâ€ corresponds to the fairly trivial â€œregressionâ€ model in which we
include 0 predictors and only include the intercept term b0:
$$H_0:Y_0=b_0+\epsilon_i$$
If our regression model has K predictors, the â€œalternative modelâ€ is described using the usual
formula for a multiple regression model:
$$H_1:Y_i=b_0+(\sum_{k=1}^K b_k X_{ik})+\epsilon_i$$
How can we test these two hypotheses against each other? The trick is to understand that
itâ€™s possible to divide up the total variance SStot into the sum of the residual variance SSres
and the regression model variance SSmod. Iâ€™ll skip over the technicalities, since weâ€™ll get to
that later when we look at ANOVA in Chapter 13. But just note that
$$SS_{mod}=SS_{tot}-SS_{res}$$
And we can convert the sums of squares into mean squares by dividing by the degrees of
freedom.
$$MS_{mod}=\frac{SS_{mod}}{df_{mod}}$$
$$MS_{res}=\frac{SS_{res}}{df_{res}}$$
So, how many degrees of freedom do we have? As you might expect the df associated with
the model is closely tied to the number of predictors that weâ€™ve included. In fact, it turns out
that dfmod â€œ K. For the residuals the total degrees of freedom is dfres â€œ N Â´ K Â´ 1.
Now that weâ€™ve got our mean square values we can calculate an F-statistic like this
$$F=\frac{MS_{mod}}{MS_{res}}$$
and the degrees of freedom associated with this are K and N Â´ K Â´ 1.
 </td>
	</tr>
</table>


Weâ€™ll see much more of the F statistic in Chapter 13, but for now just know that we can
interpret large F values as indicating that the null hypothesis is performing poorly in comparison
to the alternative hypothesis. In a moment Iâ€™ll show you how to do the test in jamovi the easy
way, but first letâ€™s have a look at the tests for the individual regression coefficients.


<h3> 12.7.2   Tests for individual coefficients </h3>



The F-test that weâ€™ve just introduced is useful for checking that the model as a whole is
performing better than chance. If your regression model doesnâ€™t produce a significant result for
the F-test then you probably donâ€™t have a very good regression model (or, quite possibly, you
donâ€™t have very good data). However, while failing this test is a pretty strong indicator that
the model has problems, passing the test (i.e., rejecting the null) doesnâ€™t imply that the model
is good! Why is that, you might be wondering? The answer to that can be found by looking
at the coefficients for the multiple regression model we have already looked at in section 12.5
above, where the coefficients we got were:


<table width="50%" border="1" ><caption></caption>
	<tr>
		<td> (Intercept) </td>
		<td> dan.sleep </td>
		<td>  baby.sleep </td>
	</tr>
	<tr>
		<td> 125.97  </td>
		<td>  -8.95 </td>
		<td>   0.01 </td>
	</tr>
</table>

I canâ€™t help but notice that the estimated regression coefficient for the baby.sleep variable
is tiny (0.01), relative to the value that we get for dan.sleep (-8.95). Given that these two
variables are absolutely on the same scale (theyâ€™re both measured in â€œhours sleptâ€), I find this
illuminating. In fact, Iâ€™m beginning to suspect that itâ€™s really only the amount of sleep that I
get that matters in order to predict my grumpiness.
We can re-use a hypothesis test that we discussed earlier, the t-test. The test that weâ€™re
interested in has a null hypothesis that the true regression coefficient is zero (b â€œ 0), which is
to be tested against the alternative hypothesis that it isnâ€™t (b â€° 0). That is:



$$H_0:b=0$$

$$H_1:b \neq 0$$




How can we test this? Well, if the central limit theorem is kind to us we might be able to guess
that the sampling distribution of Ë†b, the estimated regression coefficient, is a normal distribution
with mean centred on b. What that would mean is that if the null hypothesis were true, then
the sampling distribution of Ë†b has mean zero and unknown standard deviation. Assuming that
we can come up with a good estimate for the standard error of the regression coefficient, sep
Ë†bq,
then weâ€™re in luck. Thatâ€™s exactly the situation for which we introduced the one-sample t-test
way back in Chapter 11. So letâ€™s define a t-statistic like this

$$t=\frac{\hat{b}}{SE(\hat{b})}$$


Iâ€™ll skip over the reasons why, but our degrees of freedom in this case are df â€œ N Â´ K Â´ 1.
Irritatingly, the estimate of the standard error of the regression coefficient, sep
Ë†bq, is not as easy
to calculate as the standard error of the mean that we used for the simpler t-tests in Chapter 11.
In fact, the formula is somewhat ugly, and not terribly helpful to look at.7 For our purposes itâ€™s
sufficient to point out that the standard error of the estimated regression coefficient depends
on both the predictor and outcome variables, and it is somewhat sensitive to violations of the
homogeneity of variance assumption (discussed shortly).


In any case, this t-statistic can be interpreted in the same way as the t-statistics that we
discussed in Chapter 11. Assuming that you have a two-sided alternative (i.e., you donâ€™t really
care if b Ä… 0 or b Äƒ 0), then itâ€™s the extreme values of t (i.e., a lot less than zero or a lot greater
than zero) that suggest that you should reject the null hypothesis.




<h3> 12.7.3   Running the hypothesis tests in jamovi </h3>

To compute all of the statistics that we have talked about so far, all you need to do is make
sure the relevant options are checked in jamovi and then run the regression. If we do that, as in
Figure 12.15, we get a whole bunch of useful output.


The â€˜Model Coefficientsâ€™ at the bottom of the jamovi analysis results shown in 12.15 provides
the coefficients of the regression model. Each row in this table refers to one of the coefficients in
the regression model. The first row is the intercept term, and the later ones look at each of the
predictors. The columns give you all of the relevant information. The first column is the actual
estimate of b (e.g., 125.97 for the intercept, and -8.95 for the dan.sleep predictor). The second
column is the standard error estimate Ë†Ïƒb. The third and fourth columns provide the lower and
upper values for the 95% confidence interval around the b estimate (more on this later). The
fifth column gives you the t-statistic, and itâ€™s worth noticing that in this table t â€œ Ë†b{sep
Ë†bq every
time. Finally, the last column gives you the actual p-value for each of these tests.



<figure>
<figcaption>Figure 12.15: A jamovi screenshot showing a multiple linear regression analysis, with some useful
options checked.
</figcaption>
<img src="images/Figure119.PNG" alt="Figure 12.15" />
</figure>



The only thing that the coefficients table itself doesnâ€™t list is the degrees of freedom used in
the t-test, which is always N Â´ K Â´ 1 and is listed in the table at the top of the output, labelled
â€˜Model Fit Measuresâ€™. We can see from this table that the model performs significantly better
than youâ€™d expect by chance (Fp2, 97q â€œ 215.24, p Äƒ .001), which isnâ€™t all that surprising:
the R2 â€œ .81 value indicate that the regression model accounts for 81% of the variability in
the outcome measure (and 82% for the adjusted R2
). However, when we look back up at the
t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep
variable has no significant effect. All the work in this model is being done by the dan.sleep
variable. Taken together, these results suggest that this regression model is actually the wrong
model for the data. Youâ€™d probably be better off dropping the baby.sleep predictor entirely. In
other words, the simple regression model that we started with is the better model.




<h2> 12.8   Regarding regression coefficients </h2>


Before moving on to discuss the assumptions underlying linear regression and what you can do to
check if theyâ€™re being met, thereâ€™s two more topics I want to briefly discuss, both of which relate
to the regression coefficients. The first thing to talk about is calculating confidence intervals
for the coefficients. After that, Iâ€™ll discuss the somewhat murky question of how to determine
which predictor is most important.



<h3> 12.8.1   Confidence intervals for the coefficients </h3>


Like any population parameter, the regression coefficients b cannot be estimated with complete
precision from a sample of data; thatâ€™s part of why we need hypothesis tests. Given this, itâ€™s
quite useful to be able to report confidence intervals that capture our uncertainty about the true
value of b. This is especially useful when the research question focuses heavily on an attempt to
find out how strongly variable X is related to variable Y , since in those situations the interest
is primarily in the regression weight b.




<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 
		Fortunately, confidence intervals for the regression weights can be constructed in the usual
fashion

$$CI(b)=\hat{b} \pm (t_{crit} \times SE(\hat{b}))$$

where sep
Ë†bq is the standard error of the regression coefficient, and tcrit is the relevant critical
value of the appropriate t distribution. For instance, if itâ€™s a 95% confidence interval that we
want, then the critical value is the 97.5th quantile of a t distribution with N Â´K Â´1 degrees of
freedom. In other words, this is basically the same approach to calculating confidence intervals
that weâ€™ve used throughout.


 </td>
	</tr>
</table>




In jamovi we had already specified the â€˜95% Confidence intervalâ€™ as shown if Figure 12.15,
although we could easily have chosen another value, say a â€˜99% Confidence intervalâ€™ if that is
what we decided on.





<h3> 12.8.2   Calculating standardised regression coefficients </h3>


One more thing that you might want to do is to calculate â€œstandardisedâ€ regression coefficients, 
often denoted Î². The rationale behind standardised coefficients goes like this. In a lot
of situations, your variables are on fundamentally different scales. Suppose, for example, my
regression model aims to predict peopleâ€™s IQ scores using their educational attainment (number
of years of education) and their income as predictors. Obviously, educational attainment and
income are not on the same scales. The number of years of schooling might only vary by 10s of
years, whereas income can vary by 10,000s of dollars (or more). The units of measurement have
a big influence on the regression coefficients. The b coefficients only make sense when interpreted
in light of the units, both of the predictor variables and the outcome variable. This makes it
very difficult to compare the coefficients of different predictors. Yet there are situations where
you really do want to make comparisons between different coefficients. Specifically, you might
want some kind of standard measure of which predictors have the strongest relationship to the
outcome. This is what standardised coefficients aim to do.

The basic idea is quite simple; the standardised coefficients are the coefficients that you would
have obtained if youâ€™d converted all the variables to z-scores before running the regression.9 The
idea here is that, by converting all the predictors to z-scores, they all go into the regression on
the same scale, thereby removing the problem of having variables on different scales. Regardless
of what the original variables were, a Î² value of 1 means that an increase in the predictor of 1
standard deviation will produce a corresponding 1 standard deviation increase in the outcome
variable. Therefore, if variable A has a larger absolute value of Î² than variable B, it is deemed
to have a stronger relationship with the outcome. Or at least thatâ€™s the idea. Itâ€™s worth being
a little cautious here, since this does rely very heavily on the assumption that â€œa 1 standard
deviation changeâ€ is fundamentally the same kind of thing for all variables. Itâ€™s not always
obvious that this is true.





<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 
Leaving aside the interpretation issues, letâ€™s look at how itâ€™s calculated. What you could do
is standardise all the variables yourself and then run a regression, but thereâ€™s a much simpler
way to do it. As it turns out, the Î² coefficient for a predictor X and outcome Y has a very
simple formula, namely

$$\beta_X=b_X \times \frac{\sigma_X}{\sigma_Y}$$

where ÏƒX is the standard deviation of the predictor, and ÏƒY is the standard deviation of the
outcome variable Y . This makes matters a lot simpler.



 </td>
	</tr>
</table>


To make things even simpler, jamovi has an option that computes the Î² coefficients for you
using the â€˜Standardized estimateâ€™ checkbox in the â€˜Model Coefficientsâ€™ options, see results in
Figure 12.16.


This clearly shows that the dan.sleep variable has a much stronger effect than the baby.sleep
variable. However, this is a perfect example of a situation where it would probably make sense
to use the original coefficients b rather than the standardised coefficients Î². After all, my sleep
and the babyâ€™s sleep are already on the same scale: number of hours slept. Why complicate
matters by converting these to z-scores?



<figure>
<figcaption>Figure 12.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression 

</figcaption>
<img src="images/Figure120.PNG" alt="Figure 12.16" />
</figure>



<h2> 12.9   Assumptions of regression </h2>



The linear regression model that Iâ€™ve been discussing relies on several assumptions. In Section 12.10 
weâ€™ll talk a lot more about how to check that these assumptions are being met, but
first letâ€™s have a look at each of them.



<ul>
<li>
Normality. Like many of the models in statistics, basic simple or multiple linear 
regression relies on an assumption of normality. Specifically, it assumes that the residuals are
normally distributed. Itâ€™s actually okay if the predictors X and the outcome Y are nonnormal, so 
long as the residuals  are normal. See Section 12.10.3.
</li>
<li>
Linearity. A pretty fundamental assumption of the linear regression model is that the
relationship between X and Y actually is linear! Regardless of whether itâ€™s a simple
regression or a multiple regression, we assume that the relationships involved are linear.
</li>
<li>
Homogeneity of variance. Strictly speaking, the regression model assumes that each residual i
is generated from a normal distribution with mean 0, and (more importantly for
the current purposes) with a standard deviation Ïƒ that is the same for every single residual. 
In practice, itâ€™s impossible to test the assumption that every residual is  identically
distributed. Instead, what we care about is that the standard deviation of the residual is
the same for all values of YË† , and (if weâ€™re being especially paranoid) all values of every
predictor X in the model.
</li>
<li>
Uncorrelated predictors. The idea here is that, in a multiple regression model, you donâ€™t
want your predictors to be too strongly correlated with each other. This isnâ€™t â€œtechnicallyâ€
an assumption of the regression model, but in practice itâ€™s required. Predictors that are
too strongly correlated with each other (referred to as â€œcollinearityâ€) can cause problems
when evaluating the model. See Section 12.10.4
</li>
<li>
Residuals are independent of each other. This is really just a â€œcatch allâ€ assumption, to
the effect that â€œthereâ€™s nothing else funny going on in the residualsâ€. If there is something
weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on,
it might screw things up.
</li>
<li>
No â€œbadâ€ outliers. Again, not actually a technical assumption of the model (or rather, itâ€™s
sort of implied by all the others), but there is an implicit assumption that your regression
model isnâ€™t being too strongly influenced by one or two anomalous data points because
this raises questions about the adequacy of the model and the trustworthiness of the data
in some cases. See Section 12.10.2.
</li>
</ul>



<h2> 12.10   Model checking </h2>



The main focus of this section is regression diagnostics, a term that refers to the art of
checking that the assumptions of your regression model have been met, figuring out how to fix
the model if the assumptions are violated, and generally to check that nothing â€œfunnyâ€ is going
on. I refer to this as the â€œartâ€ of model checking with good reason. Itâ€™s not easy, and while
there are a lot of fairly standardised tools that you can use to diagnose and maybe even cure the
problems that ail your model (if there are any, that is!), you really do need to exercise a certain
amount of judgement when doing this. Itâ€™s easy to get lost in all the details of checking this
thing or that thing, and itâ€™s quite exhausting to try to remember what all the different things
are. This has the very nasty side effect that a lot of people get frustrated when trying to learn
all the tools, so instead they decide not to do any model checking. This is a bit of a worry!


In this section I describe several different things you can do to check that your regression
model is doing what itâ€™s supposed to. It doesnâ€™t cover the full space of things you could do, but
itâ€™s still much more detailed than what I see a lot of people doing in practice, and even I donâ€™t
usually cover all of this in my intro stats class either. However, I do think itâ€™s important that
you get a sense of what tools are at your disposal, so Iâ€™ll try to introduce a bunch of them here.
Finally, I should note that this section draws quite heavily from the Fox and Weisberg (2011)
text, the book associated with the car package that is used to conduct regression analysis in R.
The car package is notable for providing some excellent tools for regression diagnostics, and the
book itself talks about them in an admirably clear fashion. I donâ€™t want to sound too gushy
about it, but I do think that Fox et al. (2011) is well worth reading, even if some of the advanced
diagnostic techniques are only available in R and not jamovi.



<h3> 12.10.1   Three kinds of residuals </h3>



The majority of regression diagnostics revolve around looking at the residuals, and by now
youâ€™ve probably formed a sufficiently pessimistic theory of statistics to be able to guess that,
precisely because of the fact that we care a lot about the residuals, there are several different
kinds of residual that we might consider. In particular, the following three kinds of residuals
are referred to in this section: â€œordinary residualsâ€, â€œstandardised residualsâ€, and â€œStudentised
residualsâ€. There is a fourth kind that youâ€™ll see referred to in some of the Figures, and thatâ€™s
the â€œPearson residualâ€. However, for the models that weâ€™re talking about in this chapter the
Pearson residual is identical to the ordinary residual.

The first and simplest kind of residuals that we care about are ordinary residuals. These
are the actual raw residuals that Iâ€™ve been talking about throughout this chapter so far. The
ordinary residual is just the difference between the fitted value YË†
i and the observed value Yi
.
Iâ€™ve been using the notation i to refer to the i-th ordinary residual, and by gum Iâ€™m going to
stick to it. With this in mind, we have the very simple equation


$$\epsilon_i=Y_i-\hat{Y_i}$$

This is of course what we saw earlier, and unless I specifically refer to some other kind of residual,
this is the one Iâ€™m talking about. So thereâ€™s nothing new here. I just wanted to repeat myself.
One drawback to using ordinary residuals is that theyâ€™re always on a different scale, depending
on what the outcome variable is and how good the regression model is. That is, unless youâ€™ve
decided to run a regression model without an intercept term, the ordinary residuals will have
mean 0 but the variance is different for every regression. In a lot of contexts, especially where
youâ€™re only interested in the pattern of the residuals and not their actual values, itâ€™s convenient to
estimate the standardised residuals, which are normalised in such a way as to have standard
deviation 1.




<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 
The way we calculate these is to divide the ordinary residual by an estimate of the (population)
standard deviation of these residuals. For technical reasons, mumble mumble, the formula for
this is

$$\epsilon_i^{'}=\frac{\epsilon_i}{\hat{\sigma}\sqrt{1-h_i}}$$

where Ë†Ïƒ in this context is the estimated population standard deviation of the ordinary residuals,
and hi
is the â€œhat valueâ€ of the ith observation. I havenâ€™t explained hat values to you yet (but
have no fear,a
itâ€™s coming shortly), so this wonâ€™t make a lot of sense. For now, itâ€™s enough to
interpret the standardised residuals as if weâ€™d converted the ordinary residuals to z-scores. In
fact, that is more or less the truth, itâ€™s just that weâ€™re being a bit fancier.



 </td>
	</tr>
	<tr>
		<td> 
		Or have no hope, as the case may be.

		 </td>
	</tr>
		
</table>






The third kind of residuals are Studentised residuals (also called â€œjackknifed residualsâ€)
and theyâ€™re even fancier than standardised residuals. Again, the idea is to take the ordinary
residual and divide it by some quantity in order to estimate some standardised notion of the
residual.



<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 
The formula for doing the calculations this time is subtly different

$$\epsilon_i^*=\frac{\epsilon_i}{\hat{\sigma_i}\sqrt{1-h_i}}$$

Notice that our estimate of the standard deviation here is written Ë†ÏƒpÂ´iq
. What this corresponds
to is the estimate of the residual standard deviation that you would have obtained if you just
deleted the ith observation from the data set. This sounds like the sort of thing that would be
a nightmare to calculate, since it seems to be saying that you have to run N new regression
models (even a modern computer might grumble a bit at that, especially if youâ€™ve got a large
data set). Fortunately, some terribly clever person has shown that this standard deviation
estimate is actually given by the following equation:

$$\hat{\sigma}_{(-i)}=\hat{\sigma}\sqrt{\frac{N-K-1-{\epsilon_i^{'}}^2}{N-K-2}}$$

Isnâ€™t that a pip?
 </td>
	</tr>
		
</table>



Before moving on, I should point out that you donâ€™t often need to obtain these residuals
yourself, even though they are at the heart of almost all regression diagnostics. Most of the time
the various options that provide the diagnostics, or assumption checks, will take care of these
calculations for you. Even so, itâ€™s always nice to know how to actually get hold of these things
yourself in case you ever need to do something non-standard.




<h3> 12.10.2    Three kinds of anomalous data </h3>



One danger that you can run into with linear regression models is that your analysis might
be disproportionately sensitive to a smallish number of â€œunusualâ€ or â€œanomalousâ€ observations.
I discussed this idea previously in Section 5.2.3 in the context of discussing the outliers that
get automatically identified by the boxplot option under â€˜Explorationâ€™ - â€˜Descriptivesâ€™, but this
time we need to be much more precise. In the context of linear regression, there are three
conceptually distinct ways in which an observation might be called â€œanomalousâ€. All three are
interesting, but they have rather different implications for your analysis.


The first kind of unusual observation is an outlier. The definition of an outlier (in this
context) is an observation that is very different from what the regression model predicts. An
example is shown in Figure 12.17. In practice, we operationalise this concept by saying that an
outlier is an observation that has a very large Studentised residual, 
Ëš
i
. Outliers are interesting: a
big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly
in the data set, or some other defect may be detectable. Note that you shouldnâ€™t throw an
observation away just because itâ€™s an outlier. But the fact that itâ€™s an outlier is often a cue to
look more closely at that case and try to find out why itâ€™s so different.


<figure>
<figcaption>Figure 12.17: An illustration of outliers. The dotted lines plot the regression line that would have
been estimated without the anomalous observation included, and the corresponding residual
(i.e., the Studentised residual). The solid line shows the regression line with the anomalous
observation included. The outlier has an unusual value on the outcome (y axis location) but not
the predictor (x axis location), and lies a long way from the regression line.
</figcaption>
<img src="images/Figure121.PNG" alt="Figure 12.17" />
</figure>



The second way in which an observation can be unusual is if it has high leverage, which
happens when the observation is very different from all the other observations. This doesnâ€™t
necessarily have to correspond to a large residual. If the observation happens to be unusual on
all variables in precisely the same way, it can actually lie very close to the regression line. An
example of this is shown in Figure 12.18. The leverage of an observation is operationalised in
terms of its hat value, usually written hi
. The formula for the hat value is rather complicated10
but its interpretation is not: hi
is a measure of the extent to which the i-th observation is â€œin
controlâ€ of where the regression line ends up going.



In general, if an observation lies far away from the other ones in terms of the predictor
variables, it will have a large hat value (as a rough guide, high leverage is when the hat value
is more than 2-3 times the average; and note that the sum of the hat values is constrained to
be equal to K ` 1). High leverage points are also worth looking at in more detail, but theyâ€™re
much less likely to be a cause for concern unless they are also outliers.


<figure>
<figcaption>Figure 12.18: An illustration of high leverage points. The anomalous observation in this case is
unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness
is highly consistent with the pattern of correlations that exists among the other observations.
The observation falls very close to the regression line and does not distort it.
</figcaption>
<img src="images/Figure122.PNG" alt="Figure 12.18" />
</figure>


This brings us to our third measure of unusualness, the influence of an observation. A high
influence observation is an outlier that has high leverage. That is, it is an observation that is
very different to all the other ones in some respect, and also lies a long way from the regression
line. This is illustrated in Figure 12.19. Notice the contrast to the previous two figures. Outliers
donâ€™t move the regression line much and neither do high leverage points. But something that is
both an outlier and has high leverage, well that has a big effect on the regression line. Thatâ€™s
why we call these points high influence, and itâ€™s why theyâ€™re the biggest worry. We operationalise
influence in terms of a measure known as Cookâ€™s distance.



<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 

$$D_i=\frac{{\epsilon_i^*}^2}{K+1} \times \frac{h_i}{1-h_i}$$

Notice that this is a multiplication of something that measures the outlier-ness of the observation (the bit on the left), 
and something that measures the leverage of the observation (the
bit on the right).


 </td>
	</tr>
		
</table>




In order to have a large Cookâ€™s distance an observation must be a fairly substantial outlier
and have high leverage. As a rough guide, Cookâ€™s distance greater than 1 is often considered
large (thatâ€™s what I typically use as a quick and dirty rule).


In jamovi, information about Cookâ€™s distance can be calculated by clicking on the â€˜Cookâ€™s
Distanceâ€™ checkbox in the â€˜Assumption Checksâ€™ - â€˜Data Summaryâ€™ options. When you do this,
for the multiple regression model we have been using as an example in this chapter, you get the
results as shown in Figure 12.20.


You can see that, in this example, the mean Cookâ€™s distance value is 0.01, and the range is
from 0.00000262 to 0.11, so this is some way off the rule of thumb figure mentioned above that
a Cookâ€™s distance greater than 1 is considered large.




<figure>
<figcaption>Figure 12.19: An illustration of high influence points. In this case, the anomalous observation
is highly unusual on the predictor variable (x axis), and falls a long way from the regression
line. As a consequence, the regression line is highly distorted, even though (in this case) the
anomalous observation is entirely typical in terms of the outcome variable (y axis).
</figcaption>
<img src="images/Figure123.PNG" alt="Figure 12.19" />
</figure>



<figure>
<figcaption>Figure 12.20: jamovi output showing the table for the Cookâ€™s distance statistics
</figcaption>
<img src="images/Figure124.PNG" alt="Figure 12.20" />
</figure>




An obvious question to ask next is, if you do have large values of Cookâ€™s distance what
should you do? As always, thereâ€™s no hard and fast rule. Probably the first thing to do is to
try running the regression with the outlier with the greatest Cookâ€™s distance11 excluded and see
what happens to the model performance and to the regression coefficients. If they really are
substantially different, itâ€™s time to start digging into your data set and your notes that you no
doubt were scribbling as your ran your study. Try to figure out why the point is so different.
If you start to become convinced that this one data point is badly distorting your results then
you might consider excluding it, but thatâ€™s less than ideal unless you have a solid explanation
for why this particular case is qualitatively different from the others and therefore deserves to
be handled separately.




<h3> 12.10.3    Checking the normality of the residuals </h3>




Like many of the statistical tools weâ€™ve discussed in this book, regression models rely on a
normality assumption. In this case, we assume that the residuals are normally distributed. The
first thing we can do is draw a QQ-plot via the â€˜Assumption Checksâ€™ - â€˜Assumption Checksâ€™ -
â€˜Q-Q plot of residualsâ€™ option.


The output is shown in Figure 12.21, showing the standardised residuals plotted as a function
of their theoretical quantiles according to the regression model.



Another thing we should check is the relationship between the fitted values and the residuals
themselves. We can get jamovi to do this using the â€˜Residuals Plotsâ€™ option, which provides
a scatterplot for each predictor variable, the outcome variable, and the fitted values against
residuals, see Figure 12.22. In these plots we are looking for a fairly uniform distribution of
â€˜dotsâ€™, with no clear bunching or patterning of the â€˜dotsâ€™. Looking at these plots, there is
nothing particularly worrying as the dots are fairly evenly spread across the whole plot. There
may be a little bit of non-uniformity in plot (b), but it is not a strong deviation and probably
not worth worrying about.



<figure>
<figcaption>Figure 12.21: Plot of the theoretical quantiles according to the model, against the quantiles of
the standardised residuals, produced in jamovi
</figcaption>
<img src="images/Figure125.PNG" alt="Figure 12.21" />
</figure>



If we were worried, then in a lot of cases the solution to this problem (and many others) is
to transform one or more of the variables. We discussed the basics of variable transformation
in Sections 6.3 and 6.4, but I do want to make special note of one additional possibility that I
didnâ€™t explain fully earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one
and itâ€™s very widely used.





<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 


$$f(x,\lambda)=\frac{x^{\lambda}-1}{\lambda}$$

for all values of Î» except Î» â€œ 0. When Î» â€œ 0 we just take the natural logarithm (i.e., lnpxq).


 </td>
	</tr>
		
</table>




<figure>
<figcaption>Figure 12.22: Residuals plots produced in jamovi
</figcaption>
<img src="images/Figure126.PNG" alt="Figure 12.22" />
</figure>




You can calculate it using the BOXCOX function in the â€˜Computeâ€™ variables screen in jamovi.





<h3> 12.10.4    Checking for collinearity </h3>


The last kind of regression diagnostic that Iâ€™m going to discuss in this chapter is the use
of variance inflation factors (VIFs), which are useful for determining whether or not the
predictors in your regression model are too highly correlated with each other. There is a variance
inflation factor associated with each predictor Xk in the model.




<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 

The formula for the k-th VIF is:

$$VIF_k=\frac{1}{1-R_{-k}^2}$$

where R2
pÂ´kq
refers to R-squared value you would get if you ran a regression using Xk as the
outcome variable, and all the other X variables as the predictors. The idea here is that R2
pÂ´kq
is a very good measure of the extent to which Xk is correlated with all the other variables in
the model.

 </td>
	</tr>
		
</table>


The square root of the VIF is pretty interpretable. It tells you how much wider the confidence
interval for the corresponding coefficient bk is, relative to what you would have expected if the
predictors are all nice and uncorrelated with one another. If youâ€™ve only got two predictors, the
VIF values are always going to be the same, as we can see if we click on the â€˜Collinearityâ€™ checkbox
in the â€˜Regressionâ€™ - â€˜Assumptionsâ€™ options in jamovi. For both dan.sleep and baby.sleep the
VIF is 1.65. And since the square root of 1.65 is 1.28, we see that the correlation between our
two predictors isnâ€™t causing much of a problem.


To give a sense of how we could end up with a model that has bigger collinearity problems,
suppose I were to run a much less interesting regression model, in which I tried to predict the
day on which the data were collected, as a function of all the other variables in the data set. To
see why this would be a bit of a problem, letâ€™s have a look at the correlation matrix for all four
variables:


<table width="50%" border="1" ><caption></caption>
	<tr>
		<td> &nbsp;  </td>
		<td> dan.sleep </td>
		<td>  baby.sleep </td>
		<td> dan.grump </td>
		<td>  day </td>
	</tr>
	<tr>
		<td> dan.sleep </td>
		<td>   1.00000000 </td>
		<td>    0.62794934 </td>
		<td>  -0.90338404 </td>
		<td>  -0.09840768 </td>
	</tr>
	<tr>
		<td> baby.sleep </td>
		<td>    0.62794934 </td>
		<td>    1.00000000 </td>
		<td>   -0.56596373 </td>
		<td>   -0.01043394 </td>
	</tr>
	<tr>
		<td> dan.grump </td>
		<td>     -0.90338404 </td>
		<td>     -0.56596373 </td>
		<td>    1.00000000 </td>
		<td>  0.07647926 </td>
	</tr>
	<tr>
		<td> day </td>
		<td> -0.09840768 </td>
		<td>  -0.01043394 </td>
		<td> 0.07647926 </td>
		<td>   1.00000000 </td>
	</tr>
</table>


We have some fairly large correlations between some of our predictor variables! When we
run the regression model and look at the VIF values, we see that the collinearity is causing a
lot of uncertainty about the coefficients. First, run the regression, as in Figure 12.23 and you
can see from the VIF values that, yep, thatâ€™s some mighty fine collinearity there.




<h2> 12.11    Model selection </h2>



One fairly major problem that remains is the problem of â€œmodel selectionâ€. That is, if we have
a data set that contains several variables, which ones should we include as predictors, and which
ones should we not include? In other words, we have a problem of variable selection. In
general, model selection is a complex business but itâ€™s made somewhat simpler if we restrict
ourselves to the problem of choosing a subset of the variables that ought to be included in the
model. Nevertheless, Iâ€™m not going to try covering even this reduced topic in a lot of detail.
Instead, Iâ€™ll talk about two broad principles that you need to think about, and then discuss one
concrete tool that jamovi provides to help you select a subset of variables to include in your
model. First, the two principles:




<figure>
<figcaption>Figure 12.23: Collinearity statistics for multiple regression, produced in jamovi
</figcaption>
<img src="images/Figure127.PNG" alt="Figure 12.23" />
</figure>




<ul>
<li>
Itâ€™s nice to have an actual substantive basis for your choices. That is, in a lot of situations
you the researcher have good reasons to pick out a smallish number of possible regression
models that are of theoretical interest. These models will have a sensible interpretation
in the context of your field. Never discount the importance of this. Statistics serves the
scientific process, not the other way around.
</li>
<li>
To the extent that your choices rely on statistical inference, there is a trade off between
simplicity and goodness of fit. As you add more predictors to the model you make it more
complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and
each new parameter increases the modelâ€™s capacity to â€œabsorbâ€ random variations. So the
goodness of fit (e.g., R2
) continues to rise, sometimes trivially or by chance, as you add
more predictors no matter what. If you want your model to be able to generalise well to
new observations you need to avoid throwing in too many variables.
</li>
</ul>




<dl>
<dt>
This latter principle is often referred to as Ockhamâ€™s razor and is often summarised in terms
of the following pithy saying: do not multiply entities beyond necessity. In this context, it means
donâ€™t chuck in a bunch of largely irrelevant predictors just to boost your R2
. Hmm. Yeah, the
original was better.
</dt>
<dd>
In any case, what we need is an actual mathematical criterion that will implement the
qualitative principle behind Ockhamâ€™s razor in the context of selecting a regression model. As it
turns out there are several possibilities. The one that Iâ€™ll talk about is the Akaike information
criterion (AIC; <a href="https://ieeexplore.ieee.org/document/1100705"  target="_blank"><font color="blue">Akaike 1974</font></a>) simply because itâ€™s available as an option in jamovi.
</dd>
</dl>



<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 

In the context of a linear regression model (and ignoring terms that donâ€™t depend on the model
in any way!), the AIC for a model that has K predictor variables plus an intercept is

$$AIC=\frac{SS_{res}}{\hat{\sigma}^2}+2K$$
 </td>
	</tr>
		
</table>



The smaller the AIC value, the better the model performance. If we ignore the low level
details itâ€™s fairly obvious what the AIC does. On the left we have a term that increases as the
model predictions get worse; on the right we have a term that increases as the model complexity
increases. The best model is the one that fits the data well (low residuals, left hand side) using
as few predictors as possible (low K, right hand side). In short, this is a simple implementation
of Ockhamâ€™s razor.


AIC can be added to the â€˜Model Fit Measuresâ€™ output Table when the â€˜AICâ€™ checkbox is
clicked, and a rather clunky way of assessing different models is seeing if the â€˜AICâ€™ value is lower if
you remove one or more of the predictors in the regression model. This is the only way currently
implemented in jamovi, but there are alternatives in other more powerful programmes, such
as R. These alternative methods can automate the process of selectively removing (or adding)
predictor variables to find the best AIC. Although these methods are not implemented in jamovi,
I will mention them briefly below just so you know about them.






<h3> 12.11.1  Backward elimination </h3>



In backward elimination you start with the complete regression model, including all possible
predictors. Then, at each â€œstepâ€ we try all possible ways of removing one of the variables, and
whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new
regression model, and we then try all possible deletions from the new model, again choosing the
option with lowest AIC. This process continues until we end up with a model that has a lower
AIC value than any of the other possible models that you could produce by deleting one of its
predictors.



<h3> 12.11.2  Forward selection</h3>


As an alternative, you can also try forward selection. This time around we start with
the smallest possible model as our start point, and only consider the possible additions to the
model. However, thereâ€™s one complication. You also need to specify what the largest possible
model youâ€™re willing to entertain is.

Although backward and forward selection can lead to the same conclusion, they donâ€™t always.


<h3> 12.11.3  A caveat</h3>


Automated variable selection methods are seductive things, especially when theyâ€™re bundled
up in (fairly) simple functions in powerful statistical programmes. They provide an element of
objectivity to your model selection, and thatâ€™s kind of nice. Unfortunately, theyâ€™re sometimes
used as an excuse for thoughtlessness. No longer do you have to think carefully about which
predictors to add to the model and what the theoretical basis for their inclusion might be.
Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockhamâ€™s
razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can
argue with.



Or, perhaps not. Firstly, thereâ€™s very little agreement on what counts as an appropriate
model selection criterion. When I was taught backward elimination as an undergraduate, we
used F-tests to do it, because that was the default method used by the software. Iâ€™ve described
using AIC, and since this is an introductory text thatâ€™s the only method Iâ€™ve described, but the
AIC is hardly the Word of the Gods of Statistics. Itâ€™s an approximation, derived under certain
assumptions, and itâ€™s guaranteed to work only for large samples when those assumptions are
met. Alter those assumptions and you get a different criterion, like the BIC for instance (also
available in jamovi). Take a different approach again and you get the NML criterion. Decide
that youâ€™re a Bayesian and you get model selection based on posterior odds ratios. Then there
are a bunch of regression specific tools that I havenâ€™t mentioned. And so on. All of these
different methods have strengths and weaknesses, and some are easier to calculate than others
(AIC is probably the easiest of the lot, which might account for its popularity). Almost all
of them produce the same answers when the answer is â€œobviousâ€ but thereâ€™s a fair amount of
disagreement when the model selection problem becomes hard.


What does this mean in practice? Well, you could go and spend several years teaching
yourself the theory of model selection, learning all the ins and outs of it so that you could finally
decide on what you personally think the right thing to do is. Speaking as someone who actually
did that, I wouldnâ€™t recommend it. Youâ€™ll probably come out the other side even more confused
than when you started. A better strategy is to show a bit of common sense. If youâ€™re staring
at the results of an automated backwards or forwards selection procedure, and the model that
makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesnâ€™t
make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as
I said at the beginning, interpretability matters.




<h3> 12.11.4  Comparing two regression models</h3>




An alternative to using automated model selection procedures is for the researcher to explicitly select
 two or more regression models to compare to each other. You can do this in
a few different ways, depending on what research question youâ€™re trying to answer. Suppose
we want to know whether or not the amount of sleep that my son got has any relationship
to my grumpiness, over and above what we might expect from the amount of sleep that I
got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, weâ€™re interested in the relationship between baby.sleep
and dan.grump, and from that perspective dan.sleep and day are nuisance variable or covariates that we want to control for. In this situation, what we would like to know is whether
dan.grump ~ dan.sleep + day + baby.sleep (which Iâ€™ll call Model 2, or M2) is a better regression model for these data than dan.grump ~ dan.sleep + day (which Iâ€™ll call Model 1, or M1).
There are two different ways we can compare these two models, one based on a model selection
criterion like AIC, and the other based on an explicit hypothesis test. Iâ€™ll show you the AIC
based approach first because itâ€™s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one,
and then select the model with the smaller AIC value as it is judged to be the better model for
these data. Actually, donâ€™t do this just yet. Read on because there is an easy way in jamovi to
get the AIC values for different models included in one table




A somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset
of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors
included in Model 1, plus one or more additional predictors. When this happens we say that
Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis
and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a
fairly straightforward fashion.



<table width="100%" border="1" ><caption></caption>
	<tr>
		<td> 

We can fit both models to the data and obtain a residual sum of squares for both models.
Iâ€™ll denote these as SSp1q
res and SSp2q
res respectively. The superscripting here just indicates which
model weâ€™re talking about. Then our F statistic is

$$F=\frac{\frac{SS_{res}^{(1)}-SS_{res}^{(2)}}{k}}{\frac{SS_{res}^2}{N-p-1}}$$

where N is the number of observations, p is the number of predictors in the full model (not
including the intercept), and k is the difference in the number of parameters between the two
models.a The degrees of freedom here are k and N Â´pÂ´1. Note that itâ€™s often more convenient
to think about the difference between those two SS values as a sum of squares in its own right.
That is


$$SS_\triangle=SS_{res}^{1}-SS_{res}^{2}$$

The reason why this is helpful is that we can express SSâˆ† as a measure of the extent to which
the two models make different predictions about the the outcome variable. Specifically,


$$SS_\triangle=\sum_i(\hat{y}_i^{(2)}-\hat{y}_i^{(1)})^2$$

where Ë†y
p1q
i
is the fitted value for yi according to model M1 and Ë†y
p2q
i
is the fitted value for yi
according to model M2.



 </td>
	</tr>
		
	<tr>
		<td> 
		<p id="notea">[a]  Itâ€™s worth noting in passing that this same F statistic can be used to test a much broader range of hypotheses
than those that Iâ€™m mentioning here. Very briefly, notice that the nested model M1 corresponds to the full
model M2 when we constrain some of the regression coefficients to zero. It is sometimes useful to construct
sub-models by placing other kinds of constraints on the regression coefficients. For instance, maybe two different
coefficients might have to sum to zero, or something like that. You can construct hypothesis tests for those
kind of constraints too, but it is somewhat more complicated and the sampling distribution for F can end up
being something known as the non-central F distribution, which is waaaaay beyond the scope of this book! All
I want to do is alert you to this possibility.
	</td>
	</tr>
		
</table>




<figure>
<figcaption>Figure 12.24: Model comparison in jamovi using the â€˜Model Builderâ€™ option
</figcaption>
<img src="images/Figure128.PNG" alt="Figure 12.24" />
</figure>



Okay, so thatâ€™s the hypothesis test that we use to compare two regression models to one
another. Now, how do we do it in jamovi? The answer is to use the â€˜Model Builderâ€™ option
and specify the Model 1 predictors dan.sleep and day in â€˜Block 1â€™ and then add the additional
predictor from Model 2 (baby.sleep) in â€˜Block 2â€™, as in Figure 12.24. This shows, in the â€˜Model
Comparisonsâ€™ Table, that for the comparisons between Model 1 and Model 2, F(1,96) = 0.00, p
= 0.954. Since we have p Ä… .05 we retain the null hypothesis (M1). This approach to regression,
in which we add all of our covariates into a null model, then add the variables of interest into
an alternative model, and then compare the two models in a hypothesis testing framework, is
often referred to as hierarchical regression.

We can also use this â€˜Model Comparisonâ€™ option to display a table that shows the AIC and
BIC for each model, making it easy to compare and identify which model has the lowest value,
as in Figure 12.24.




<h2> 12.12  Summary</h2>











<p id="note1">[1] Iâ€™ve noticed that in some versions of jamovi you can also specify an â€˜IDâ€™ variable type, but for our purposes
it does not matter how we specify the ID variable as we wonâ€™t be including it in any analyses.
</p>


<p id="note2">[2] Actually, even that table is more than Iâ€™d bother with. In practice most people pick one measure of central
tendency, and one measure of variability only.
</p>









</body>

</html>