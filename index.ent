\@doanenote {1}
macro:->When
all
groups
have
the
same
number
of
observations,
the
experimental
design
is
said
to
be
``balanced''.
Balance
isn't
such
a
big
deal
for
one-way
ANOVA,
which
is
the
topic
of
this
chapter.
It
becomes
more
important
when
you
start
doing
more
complicated
ANOVAs.
\@endanenote 
\@doanenote {2}
macro:->So
the
formula
for
the
total
sum
of
squares
is
almost
identical
to
the
formula
for
the
variance
\[SS_{tot}=\sum
_{k=1}^{G}
\sum
_{i=1}^{N_k}
(Y_{ik}
-
\bar
{Y})^2\]
\@endanenote 
\@doanenote {3}
macro:->One
very
nice
thing
about
the
total
sum
of
squares
is
that
we
can
break
it
up
into
two
different
kinds
of
variation
First,
we
can
talk
about
the
within-group
sum
of
squares,
in
which
we
look
to
see
how
different
each
individual
person
is
from
their
own
group
mean
\[SS_{w}=
\sum
_{k=1}^{G}
\sum
_{i=1}^{N_k}
(Y_{ik}
-
\bar
{Y}_k)^2\]
where
\(\bar
{Y}_k\)
is
a
group
mean.
In
our
example,
\(\bar
{Y}_k\)
would
be
the
average
mood
change
experienced
by
those
people
given
the
k-th
drug.
So,
instead
of
comparing
individuals
to
the
average
of
all
people
in
the
experiment,
we're
only
comparing
them
to
those
people
in
the
the
same
group.
As
a
consequence,
you'd
expect
the
value
of
\(SS_w\)
to
be
smaller
than
the
total
sum
of
squares,
because
it's
completely
ignoring
any
group
differences,
i.e.,
whether
the
drugs
will
have
different
effects
on
people's
moods.
\@endanenote 
\@doanenote {4}
macro:->In
order
to
quantify
the
extent
of
this
variation,
what
we
do
is
calculate
the
between-group
sum
of
squares
\[\begin
{aligned}
SS_{b}
&=
\sum
_{k=1}^{G}
\sum
_{i=1}^{N_k}
(
\bar
{Y}_{k}
-
\bar
{Y}
)^2
\\
&=
\sum
_{k=1}^{G}
N_k
(
\bar
{Y}_{k}
-
\bar
{Y}
)^2
\end
{aligned}\]
\@endanenote 
\@doanenote {5}
macro:->\(SS_w\)
is
also
referred
to
in
an
independent
ANOVA
as
the
error
variance,
or
\(SS_{error}\).
\@endanenote 
\@doanenote {6}
macro:->At
a
fundamental
level
ANOVA
is
a
competition
between
two
different
statistical
models,
\(H_0\)
and
\(H_1\).
When
I
described
the
null
and
alternative
hypotheses
at
the
start
of
the
section,
I
was
a
little
imprecise
about
what
these
models
actually
are.
I'll
remedy
that
now,
though
you
probably
won't
like
me
for
doing
so.
If
you
recall,
our
null
hypothesis
was
that
all
of
the
group
means
are
identical
to
one
another.
If
so,
then
a
natural
way
to
think
about
the
outcome
variable
\(Y_{ik}\)
is
to
describe
individual
scores
in
terms
of
a
single
population
mean
\(\mu
\),
plus
the
deviation
from
that
population
mean.
This
deviation
is
usually
denoted
\(\epsilon
_{ik}\)
and
is
traditionally
called
the
error
or
residual
associated
with
that
observation.
Be
careful
though.
Just
like
we
saw
with
the
word
``significant'',
the
word
``error''
has
a
technical
meaning
in
statistics
that
isn't
quite
the
same
as
its
everyday
English
definition.
In
everyday
language,
``error''
implies
a
mistake
of
some
kind,
but
in
statistics
it
doesn't
(or
at
least,
not
necessarily).
With
that
in
mind,
the
word
``residual''
is
a
better
term
than
the
word
``error''.
In
statistics
both
words
mean
``leftover
variability'',
that
is
``stuff''
that
the
model
can't
explain.
In
any
case,
here's
what
the
null
hypothesis
looks
like
when
we
write
it
as
a
statistical
model
\[Y_{ik}=\mu
+\epsilon
_{ik}\]
where
we
make
the
assumption
(discussed
later)
that
the
residual
values
\(\epsilon
_{ik}\)
are
normally
distributed,
with
mean
\(0\)
and
a
standard
deviation
\(\sigma
\)
that
is
the
same
for
all
groups.
To
use
the
notation
that
we
introduced
in
the
{[}Introduction
to
probability{]}
we
would
write
this
assumption
like
this
\[\epsilon
_{ik}
\sim
Normal(0,\sigma
^2)\]
What
about
the
alternative
hypothesis,
\(H_1\)?
The
only
difference
between
the
null
hypothesis
and
the
alternative
hypothesis
is
that
we
allow
each
group
to
have
a
different
population
mean.
So,
if
we
let
\(\mu
_k\)
denote
the
population
mean
for
the
k-th
group
in
our
experiment,
then
the
statistical
model
corresponding
to
\(H_1\)
is
\[Y_{ik}=\mu
_k+\epsilon
_{ik}\]
where,
once
again,
we
assume
that
the
error
terms
are
normally
distributed
with
mean
0
and
standard
deviation
\(\sigma
\).
That
is,
the
alternative
hypothesis
also
assumes
that
\(\epsilon
\sim
Normal(0,\sigma
^2)\)
Okay,
now
that
we've
described
the
statistical
models
underpinning
\(H_0\)
and
\(H_1\)
in
more
detail,
it's
now
pretty
straightforward
to
say
what
the
mean
square
values
are
measuring,
and
what
this
means
for
the
interpretation
of
\(F\).
I
won't
bore
you
with
the
proof
of
this
but
it
turns
out
that
the
within-groups
mean
square,
\(MS_w\),
can
be
viewed
as
an
estimator
of
the
error
variance
\(\sigma
^2\)
.
The
between-groups
mean
square
\(MS_b\)
is
also
an
estimator,
but
what
it
estimates
is
the
error
variance
plus
a
quantity
that
depends
on
the
true
differences
among
the
group
means.
If
we
call
this
quantity
\(Q\),
then
we
can
see
that
the
\(F\)
statistic
is
basically\(^a\)
\[F=\frac
{\hat
{Q}+\hat
{\sigma
}^2}{\hat
{\sigma
}^2}\]
where
the
true
value
\(Q
=
0\)
if
the
null
hypothesis
is
true,
and
\(Q
<
0\)
if
the
alternative
hypothesis
is
true
(e.g.,
Hays
(1994),
ch.~10).
Therefore,
at
a
bare
minimum
the
\(F\)
\emph
{value
must
be
larger
than}
1
to
have
any
chance
of
rejecting
the
null
hypothesis.
Note
that
this
doesn't
mean
that
it's
impossible
to
get
an
\(F\)-value
less
than
1.
What
it
means
is
that
if
the
null
hypothesis
is
true
the
sampling
distribution
of
the
\(F\)-ratio
has
a
mean
of
1,{[}\^{}b{]}
and
so
we
need
to
see
\(F\)-values
larger
than
1
in
order
to
safely
reject
the
null.
To
be
a
bit
more
precise
about
the
sampling
distribution,
notice
that
if
the
null
hypothesis
is
true,
both
\(MS\)b
and
\(MSw\)
are
estimators
of
the
variance
of
the
residuals
\(\epsilon
_{ik}\).
If
those
residuals
are
normally
distributed,
then
you
might
suspect
that
the
estimate
of
the
variance
of
\(\epsilon
_{ik}\)
is
chi-square
distributed,
because
(as
discussed
in
the
\textbf
{?@sec-Other-useful-distributions})
that's
what
a
chi-square
distribution
is:
it's
what
you
get
when
you
square
a
bunch
of
normally-distributed
things
and
add
them
up.
And
since
the
\(F\)
distribution
is
(again,
by
definition)
what
you
get
when
you
take
the
ratio
between
two
things
that
are
\(\chi
^2\)
distributed,
we
have
our
sampling
distribution.
Obviously,
I'm
glossing
over
a
whole
lot
of
stuff
when
I
say
this,
but
in
broad
terms,
this
really
is
where
our
sampling
distribution
comes
from.
---
\(^a\)If
you
read
ahead
to
Chapter~\ref
{sec-Factorial-ANOVA}
and
look
at
how
the
``treatment
effect''
at
level
\(k\)
of
a
factor
is
defined
in
terms
of
the
\(\alpha
_k\)
values
(see
{[}Factorial
ANOVA
2:
balanced
designs,
interactions
allowed{]}),
it
turns
out
that
\(Q\)
refers
to
a
weighted
mean
of
the
squared
treatment
effects,
\(Q
=
\frac
{(\sum
_{k=1}^{G}N_k
\alpha
_k^2)}{(G-1)}\).
---
\(^b\)Or,
if
we
want
to
be
sticklers
for
accuracy,
\(1+
\frac
{2}{df_2-2}\).
\@endanenote 
\@doanenote {7}
macro:->Or,
to
be
precise,
party
like
``it's
1899
and
we've
got
no
friends
and
nothing
better
to
do
with
our
time
than
do
some
calculations
that
wouldn't
have
made
any
sense
in
1899
because
ANOVA
didn't
exist
until
about
the
1920s''.
\@endanenote 
\@doanenote {8}
macro:->In
the
Excel
\emph
{clinicaltrial-anova.xls}
the
value
for
\(SS_b\)
worked
out
to
be
very
slightly
different,
\(3.45\),
than
that
shown
in
the
text
above
(rounding
errors!).
\@endanenote 
\@doanenote {9}
macro:->The
jamovi
results
are
more
accurate
than
the
ones
in
the
text
above,
due
to
rounding
errors.
\@endanenote 
\@doanenote {10}
macro:->If
you
\emph
{do}
have
some
theoretical
basis
for
wanting
to
investigate
some
comparisons
but
not
others,
it's
a
different
story.
In
those
circumstances
you're
not
really
running
``post
hoc''
analyses
at
all,
you're
making
``planned
comparisons''.
I
do
talk
about
this
situation
later
in
the
book
-
Section~\ref
{sec-The-method-of-planned-comparisons},
but
for
now
I
want
to
keep
things
simple.
\@endanenote 
\@doanenote {11}
macro:->It's
worth
noting
in
passing
that
not
all
adjustment
methods
try
to
do
this.
What
I've
described
here
is
an
approach
for
controlling
``family
wise
type
I
error
rate''.
However,
there
are
other
post
hoc
tests
that
seek
to
control
the
``false
discovery
rate'',
which
is
a
somewhat
different
thing.
\@endanenote 
\@doanenote {12}
macro:->If
you
remember
back
to
\protect
\hyperlink
{a-worked-example}{A
worked
example},
which
I
hope
you
at
least
skimmed
even
if
you
didn't
read
the
whole
thing,
I
described
the
statistical
models
underpinning
ANOVA
in
this
way:
\[H_0:Y_{ik}=\mu
+
\epsilon
_{ik}\]
\[H_1:Y_{ik}=\mu
_k
+
\epsilon
_{ik}\]
In
these
equations
\(\mu
\)
refers
to
a
single
grand
population
mean
which
is
the
same
for
all
groups,
and
Âµk
is
the
population
mean
for
the
k-th
group.
Up
to
this
point
we've
been
mostly
interested
in
whether
our
data
are
best
described
in
terms
of
a
single
grand
mean
(the
null
hypothesis)
or
in
terms
of
different
group-specific
means
(the
alternative
hypothesis).
This
makes
sense,
of
course,
as
that's
actually
the
important
research
question!
However,
all
of
our
testing
procedures
have,
implicitly,
relied
on
a
specific
assumption
about
the
residuals,
\(\epsilon
\_{ik}\),
namely
that
\[\epsilon
_{ik}
\sim
Normal(0,\sigma
^2)\]
None
of
the
maths
works
properly
without
this
bit.
Or,
to
be
precise,
you
can
still
do
all
the
calculations
and
you'll
end
up
with
an
\(F\)
statistic,
but
you
have
no
guarantee
that
this
\(F\)
statistic
actually
measures
what
you
think
it's
measuring,
and
so
any
conclusions
that
you
might
draw
on
the
basis
of
the
\(F\)-test
might
be
wrong.
\@endanenote 
\@doanenote {13}
macro:->The
Levene
test
is
shockingly
simple.
Suppose
we
have
our
outcome
variable
\(Y_{ik}\).
All
we
do
is
define
a
new
variable,
which
I'll
call
\(Z_{ik}\),
corresponding
to
the
absolute
deviation
from
the
group
mean
\[Z_{ik}=Y_{ik}-\bar
{Y}_{k}\]
Okay,
what
good
does
this
do
us?
Well,
let's
take
a
moment
to
think
about
what
\(Z_{ik}\)
actually
is
and
what
we're
trying
to
test.
The
value
of
\(Z_{ik}\)
is
a
measure
of
how
the
\(i\)-th
observation
in
the
\(k\)-th
group
deviates
from
its
group
mean.
And
our
null
hypothesis
is
that
all
groups
have
the
same
variance,
i.e.,
the
same
overall
deviations
from
the
group
means!
So
the
null
hypothesis
in
a
Levene
test
is
that
the
population
means
of
\(Z\)
are
identical
for
all
groups.
Hmm.
So
what
we
need
now
is
a
statistical
test
of
the
null
hypothesis
that
all
group
means
are
identical.
Where
have
we
seen
that
before?
Oh
right,
that's
what
ANOVA
is,
and
so
all
that
the
Levene
test
does
is
run
an
ANOVA
on
the
new
variable
\(Z_{ik}\).
What
about
the
Brown-Forsythe
test?
Does
that
do
anything
particularly
different?
Nope.
The
only
change
from
the
Levene
test
is
that
it
constructs
the
transformed
variable
\(Z\)
in
a
slightly
different
way,
using
deviations
from
the
group
medians
rather
than
deviations
from
the
group
means.
That
is,
for
the
Brown-Forsythe
test:
\[Z_{ik}=Y_{ik}-median_k(Y)\]
where
\(median_k(Y)\)
is
the
median
for
group
\(k\).
\@endanenote 
\@doanenote {14}
macro:->So
let's
let
\(R\_{ik}\)
refer
to
the
ranking
given
to
the
\(i\)th
member
of
the
\(k\)th
group.
Now,
let's
calculate
\(\bar
{R}_k\),
the
average
rank
given
to
observations
in
the
\(k\)th
group
\[\bar
{R}_k=\frac
{1}{N_k}\sum
_i
R_{ik}\]
and
let's
also
calculate
\(\bar
{R}\),
the
grand
mean
rank
\[\bar
{R}=\frac
{1}{N}\sum
_i\sum
_k
R_{ik}\]
Now
that
we've
done
this,
we
can
calculate
the
squared
deviations
from
the
grand
mean
rank
\(\bar
{R}\).
When
we
do
this
for
the
individual
scores,
i.e.,
if
we
calculate
\((R_{ik}
-
\bar
{R})^2\)
,
what
we
have
is
a
``nonparametric''
measure
of
how
far
the
\(ik\)-th
observation
deviates
from
the
grand
mean
rank.
When
we
calculate
the
squared
deviation
of
the
group
means
from
the
grand
means,
i.e.,
if
we
calculate
\((R_{ik}
-
\bar
{R})^2\),
then
what
we
have
is
a
nonparametric
measure
of
how
much
the
group
deviates
from
the
grand
mean
rank.
With
this
in
mind,
we'll
follow
the
same
logic
that
we
did
with
ANOVA
and
define
our
ranked
sums
of
squares
measures,
much
like
we
did
earlier.
First,
we
have
our
``total
ranked
sums
of
squares''
\[RSS_{tot}=\sum
_k\sum
_i
(R_{ik}-\bar
{R})^2\]
and
we
can
define
the
``between
groups
ranked
sums
of
squares''
like
this
\[\begin
{aligned}
RSS_{b}&
=\sum
{k}\sum
_{i}(\bar
{R}_{k}-\bar
{R})^2
\\
&=
\sum
_{k}
N_k
(\bar
{R}_{k}-\bar
{R})^2
\end
{aligned}\]
So,
if
the
null
hypothesis
is
true
and
there
are
no
true
group
differences
at
all,
you'd
expect
the
between
group
rank
sums
\(RSS_b\)
to
be
very
small,
much
smaller
than
the
total
rank
sums
\(RSS_{tot}\).
Qualitatively
this
is
very
much
the
same
as
what
we
found
when
we
went
about
constructing
the
ANOVA
\(F\)-statistic,
but
for
technical
reasons
the
Kruskal-Wallis
test
statistic,
usually
denoted
\(K\),
is
constructed
in
a
slightly
different
way,
\[K=(N-1)
\times
\frac
{RSS_b}{RSS_{tot}}\]
and
if
the
null
hypothesis
is
true,
then
the
sampling
distribution
of
\(K\)
is
approximately
chi-square
with
\(G-1\)
degrees
of
freedom
(where
\(G\)
is
the
number
of
groups).
The
larger
the
value
of
\(K\),
the
less
consistent
the
data
are
with
the
null
hypothesis,
so
this
is
a
one-sided
test.
We
reject
\(H_0\)
when
\(K\)
is
sufficiently
large.
\@endanenote 
\@doanenote {15}
macro:->However,
from
a
purely
mathematical
perspective
it's
needlessly
complicated.
I
won't
show
you
the
derivation,
but
you
can
use
a
bit
of
algebraic
jiggery-pokery\(^b\)
to
show
that
the
equation
for
\(K\)
can
be
\[K=\frac
{12}{N(N-1)}\sum
_k
N_k
\bar
{R}_k^2
-3(N+1)\]
It's
this
last
equation
that
you
sometimes
see
given
for
\(K\).
This
is
way
easier
to
calculate
than
the
version
I
described
in
the
previous
section,
but
it's
just
that
it's
totally
meaningless
to
actual
humans.
It's
probably
best
to
think
of
\(K\)
the
way
I
described
it
earlier,
as
an
analogue
of
ANOVA
based
on
ranks.
But
keep
in
mind
that
the
test
statistic
that
gets
calculated
ends
up
with
a
rather
different
look
to
it
than
the
one
we
used
for
our
original
ANOVA.
---
\(b\)
A
technical
term.
\@endanenote 
\@doanenote {16}
macro:->More
to
the
point,
in
the
mathematical
notation
I
introduced
above,
this
is
telling
us
that
\(f_3
=
2\).
Yay.
So,
now
that
we
know
this,
the
tie
correction
factor
(TCF)
is:
\[TCF=1-\frac
{\sum
_j
f_j^3
-
f_j}{N^3
-
N}\]
The
tie-corrected
value
of
the
Kruskal-Wallis
statistic
is
obtained
by
dividing
the
value
of
\(K\)
by
this
quantity.
It
is
this
tie-corrected
version
that
jamovi
calculates.
\@endanenote 
\@doanenote {17}
macro:->\((n
-
k)\):
(number
of
subjects
-
number
of
groups)
\@endanenote 
\@doanenote {18}
macro:->As
with
all
of
the
chapters
in
this
book,
there
are
quite
a
few
different
sources
that
I've
relied
upon,
but
the
one
stand-out
text
that
I've
been
most
heavily
influenced
by
is
Sahai
\&
Ageel
(2000).
It's
not
a
good
book
for
beginners,
but
it's
an
excellent
book
for
more
advanced
readers
who
are
interested
in
understanding
the
mathematics
behind
ANOVA.
\@endanenote 
\@doanenote {19}
macro:->The
nice
thing
about
the
subscript
notation
is
that
it
generalises
nicely.
If
our
experiment
had
involved
a
third
factor,
then
we
could
just
add
a
third
subscript.
In
principle,
the
notation
extends
to
as
many
factors
as
you
might
care
to
include,
but
in
this
book
we'll
rarely
consider
analyses
involving
more
than
two
factors,
and
never
more
than
three.
\@endanenote 
\@doanenote {20}
macro:->Technically,
marginalising
isn't
quite
identical
to
a
regular
mean.
It's
a
weighted
average
where
you
take
into
account
the
frequency
of
the
different
events
that
you're
averaging
over.
However,
in
a
balanced
design,
all
of
our
cell
frequencies
are
equal
by
definition
so
the
two
are
equivalent.
We'll
discuss
unbalanced
designs
later,
and
when
we
do
so
you'll
see
that
all
of
our
calculations
become
a
real
headache.
But
let's
ignore
this
for
now.
\@endanenote 
\@doanenote {21}
macro:->NB
There
are
some
rounding
errors
here,
the
value
of
the
mean
square,
to
5
decimal
places,
is
1.72667.
And
the
value
of
the
residual
mean
square
to
5
decimal
places,
is
0.05444.
jamovi
actually
uses
many
more
decimal
places
in
its
calculations,
but
the
figures
shown
in
the
results
tables
are
rounded
for
clarity.
Though
you
can
change
the
number
of
decimal
places
displayed
by
jamovi
if
you
want.
\@endanenote 
\@doanenote {22}
macro:->Now
that
we've
got
our
notation
straight,
we
can
compute
the
sum
of
squares
values
for
each
of
the
two
factors
in
a
relatively
familiar
way.
For
Factor
A,
our
between
group
sum
of
squares
is
calculated
by
assessing
the
extent
to
which
the
(row)
marginal
means
\(\bar
{Y}_{1.}
,
\bar
{Y}_{2.}\)
etc,
are
different
from
the
grand
mean
\(\bar
{Y}_{..}\)
We
do
this
in
the
same
way
that
we
did
for
one-way
ANOVA:
calculate
the
sum
of
squared
difference
between
the
\(\bar
{Y}_{i.}\)
values
and
the
\(\bar
{Y}_{..}\)
values.
Specifically,
if
there
are
\(N\)
people
in
each
group,
then
we
calculate
this
\[SS_A=(N
\times
C)\sum
_{r=1}^R
(\bar
{Y}_{r.}-\bar
{Y}_{..})^2\]
As
with
one-way
ANOVA,
the
most
interesting
\(^a\)
part
of
this
formula
is
the
bit,
which
corresponds
to
the
squared
deviation
associated
with
level
r.
All
that
this
formula
does
is
calculate
this
squared
deviation
for
all
R
levels
of
the
factor,
add
them
up,
and
then
multiply
the
result
by
\(N
\times
C\).
The
reason
for
this
last
part
is
that
there
are
multiple
cells
in
our
design
that
have
level
\(r\)
on
Factor
A.
In
fact,
there
are
C
of
them,
one
corresponding
to
each
possible
level
of
Factor
B.
For
instance,
in
our
example
there
are
two
different
cells
in
the
design
corresponding
to
the
anxifree
drug:
one
for
people
with
no.therapy
and
one
for
the
CBT
group.
Not
only
that,
within
each
of
these
cells
there
are
\(N\)
observations.
So,
if
we
want
to
convert
our
\(SS\)
value
into
a
quantity
that
calculates
the
between-groups
sum
of
squares
on
a
``per
observation''
basis,
we
have
to
multiply
by
\(N
\times
C\).
The
formula
for
factor
\(B\)
is
of
course
the
same
thing,
just
with
some
subscripts
shuffled
around
\[SS_B=(N
\times
R)\sum
_{c=1}^C
(\bar
{Y}_{.c}-\bar
{Y}_{..})^2\]
Now
that
we
have
these
formulas
we
can
check
them
against
the
jamovi
output
from
the
earlier
section.
Once
again,
a
dedicated
spreadsheet
programme
is
helpful
for
these
sorts
of
calculations,
so
please
have
a
go
yourself.
You
can
also
take
a
look
at
the
version
I
did
in
Excel
in
the
file
\emph
{clinicaltrial\_factorialanova.xls}.
First,
let's
calculate
the
sum
of
squares
associated
with
the
main
effect
of
drug.
There
are
a
total
of
\(N
=
3\)
people
in
each
group
and
\(C
=
2\)
different
types
of
therapy.
Or,
to
put
it
another
way,
there
are
\(3
\times
2
=
6\)
people
who
received
any
particular
drug.
When
we
do
these
calculations
in
a
spreadsheet
programme,
we
get
a
value
of
3.45
for
the
sum
of
squares
associated
with
the
main
effect
of
drug.
Not
surprisingly,
this
is
the
same
number
that
you
get
when
you
look
up
the
\(SS\)
value
for
the
drugs
factor
in
the
ANOVA
table
that
I
presented
earlier,
in
Figure~\ref
{fig-fig14-3}.
We
can
repeat
the
same
kind
of
calculation
for
the
effect
of
therapy.
Again
there
are
\(N
=
3\)
people
in
each
group,
but
since
there
are
\(R
=
3\)
different
drugs,
this
time
around
we
note
that
there
are
\(3
\times
3
=
9\)
people
who
received
CBT
and
an
additional
9
people
who
received
the
placebo.
So
our
calculation
in
this
case
gives
us
a
value
of
\(0.47\)
for
the
sum
of
squares
associated
with
the
main
effect
of
therapy.
Once
again,
we
are
not
surprised
to
see
that
our
calculations
are
identical
to
the
ANOVA
output
in
Figure~\ref
{fig-fig14-3}.
So
that's
how
you
calculate
the
\(SS\)
values
for
the
two
main
effects.
These
\(SS\)
values
are
analogous
to
the
between-group
sum
of
squares
values
that
we
calculated
when
doing
one-way
ANOVA
in
Chapter~\ref
{sec-Comparing-several-means-one-way-ANOVA}.
However,
it's
not
a
good
idea
to
think
of
them
as
between-groups
\(SS\)
values
anymore,
just
because
we
have
two
different
grouping
variables
and
it's
easy
to
get
confused.
In
order
to
construct
an
\(F\)
test,
however,
we
also
need
to
calculate
the
within-groups
sum
of
squares.
In
keeping
with
the
terminology
that
we
used
in
\textbf
{?@sec-Correlation-and-linear-regression}
and
the
terminology
that
jamovi
uses
when
printing
out
the
ANOVA
table,
I'll
start
referring
to
the
within-groups
\(SS\)
value
as
the
residual
sum
of
squares
\(SS_R\).
The
easiest
way
to
think
about
the
residual
\(SS\)
values
in
this
context,
I
think,
is
to
think
of
it
as
the
leftover
variation
in
the
outcome
variable
after
you
take
into
account
the
differences
in
the
marginal
means
(i.e.,
after
you
remove
\(SS_A\)
and
\(SS_B\)).
What
I
mean
by
that
is
we
can
start
by
calculating
the
total
sum
of
squares,
which
I'll
label
\(SS_T\)
.
The
formula
for
this
is
pretty
much
the
same
as
it
was
for
one-way
ANOVA.
We
take
the
difference
between
each
observation
\(Yrci\)
and
the
grand
mean
\(\hat
{Y}_{..}\),
square
the
differences,
and
add
them
all
\[SS_T=\sum
_{r=1}^R
\sum
_{c=1}^C
\sum
_{i=1}^N
(Y_{rci}-\bar
{Y}_{..})^2\]
The
``triple
summation''
here
looks
more
complicated
than
it
is.
In
the
first
two
summations,
we're
summing
across
all
levels
of
Factor
\(A\)
(i.e.,
over
all
possible
rows
r
in
our
table)
and
across
all
levels
of
Factor
\(B\)
(i.e.,
all
possible
columns
\(c\)).
Each
rc
combination
corresponds
to
a
single
group
and
each
group
contains
\(N\)
people,
so
we
have
to
sum
across
all
those
people
(i.e.,
all
\(i\)
values)
too.
In
other
words,
all
we're
doing
here
is
summing
across
all
observations
in
the
data
set
(i.e.,
all
possible
\(rci\)
combinations).
At
this
point,
we
know
the
total
variability
of
the
outcome
variable
\(SST\)
,
and
we
know
how
much
of
that
variability
can
be
attributed
to
Factor
A
(\(SS_A\))
and
how
much
of
it
can
be
attributed
to
Factor
B
(\(SS_B\)).
The
residual
sum
of
squares
is
thus
defined
to
be
the
variability
in
\(Y\)
that
can't
be
attributed
to
either
of
our
two
factors
(or
their
interaction
if
you
also
calculate
the
interaction
effect,
which
is
the
default
in
jamovi).
In
other
words
\[SS_R=SS_T-(SS_A+SS_B)\]
Of
course,
there
is
a
formula
that
you
can
use
to
calculate
the
residual
\(SS\)
directly,
but
I
think
that
it
makes
more
conceptual
sense
to
think
of
it
like
this.
The
whole
point
of
calling
it
a
residual
is
that
it's
the
leftover
variation,
and
the
formula
above
makes
that
clear.
I
should
also
note
that,
in
keeping
with
the
terminology
used
in
the
regression
chapter,
it
is
commonplace
to
refer
to
\(SS_A
+
SS_B\)
as
the
variance
attributable
to
the
``ANOVA
model'',
denoted
\(SSM\),
and
so
we
often
say
that
the
total
sum
of
squares
is
equal
to
the
model
sum
of
squares
plus
the
residual
sum
of
squares.
Later
on
in
this
chapter
we'll
see
that
this
isn't
just
a
surface
similarity:
ANOVA
and
regression
are
actually
the
same
thing
under
the
hood.
In
any
case,
it's
probably
worth
taking
a
moment
to
check
that
we
can
calculate
\(SS_R\)
using
this
formula
and
verify
that
we
do
obtain
the
same
answer
that
jamovi
produces
in
its
ANOVA
table.
The
calculations
are
pretty
straightforward
when
done
in
a
spreadsheet
(see
the
clinicaltrial\_factorialanova.xls
file).
---
\(^a\)English
translation:
``least
tedious''.
\@endanenote 
\@doanenote {23}
macro:->As
a
consequence,
the
way
that
the
idea
of
an
interaction
effect
is
formalised
in
terms
of
null
and
alternative
hypotheses
is
slightly
difficult,
and
I'm
guessing
that
a
lot
of
readers
of
this
book
probably
won't
be
all
that
interested.
Even
so,
I'll
try
to
give
the
basic
idea
here.
To
start
with,
we
need
to
be
a
little
more
explicit
about
our
main
effects.
Consider
the
main
effect
of
Factor
\(A\)
(drug
in
our
running
example).
We
originally
formulated
this
in
terms
of
the
null
hypothesis
that
the
two
marginal
means
\(\mu
_r\).
are
all
equal
to
each
other.
Obviously,
if
all
of
these
are
equal
to
each
other,
then
they
must
also
be
equal
to
the
grand
mean
\(\mu
_{..}\)
as
well,
right?
So
what
we
can
do
is
define
the
effect
of
Factor
\(A\)
at
level
\(r\)
to
be
equal
to
the
difference
between
the
marginal
mean
\(\mu
_{r.}\)
and
the
grand
mean
\(\mu
_{..}\).
Let's
denote
this
effect
by
\(\alpha
_r\),
and
note
that
\[\alpha
_r=\mu
_{r.}-\mu
_{..}\]
Now,
by
definition
all
of
the
\(\alpha
_r\)
values
must
sum
to
zero,
for
the
same
reason
that
the
average
of
the
marginal
means
\(\mu
_c\)
must
be
the
grand
mean
\(\mu
_{..}\).
We
can
similarly
define
the
effect
of
Factor
B
at
level
i
to
be
the
difference
between
the
column
marginal
mean
\(\mu
_{.c}\)
and
the
grand
mean
\(\mu
_{..}\)
\[\beta
_c=\mu
_{.c}-\mu
_{..}\]
and
once
again,
these
\(\beta
_c\)
values
must
sum
to
zero.
The
reason
that
statisticians
sometimes
like
to
talk
about
the
main
effects
in
terms
of
these
\(\alpha
_r\)
and
\(\beta
_c\)
values
is
that
it
allows
them
to
be
precise
about
what
it
means
to
say
that
there
is
no
interaction
effect.
If
there
is
no
interaction
at
all,
then
these
\(\alpha
_r\)
and
\(\beta
_c\)
values
will
perfectly
describe
the
group
means
\(\mu
_{rc}\).
Specifically,
it
means
that
\[\mu
_{rc}=\mu
_{..}+\alpha
_{r}+\beta
_{c}\]
That
is,
there's
nothing
special
about
the
group
means
that
you
couldn't
predict
perfectly
by
knowing
all
the
marginal
means.
And
that's
our
null
hypothesis,
right
there.
The
alternative
hypothesis
is
that
\[\mu
_{rc}
\neq
\mu
_{..}+\alpha
_{r}+\beta
_{c}\]
for
at
least
one
group
\(rc\)
in
our
table.
However,
statisticians
often
like
to
write
this
slightly
differently.
They'll
usually
define
the
specific
interaction
associated
with
group
\(rc\)
to
be
some
number,
awkwardly
referred
to
as
\((\alpha
\beta
)_{rc}\),
and
then
they
will
say
that
the
alternative
hypothesis
is
that
\[\mu
_{rc}=\mu
_{..}
+\alpha
_{r}
+\beta
_{c}
+
(\alpha
\beta
)_{rc}\]
where
\((\alpha
\beta
)_{rc}\)
is
non-zero
for
at
least
one
group.
This
notation
is
kind
of
ugly
to
look
at,
but
it
is
handy
as
we'll
see
when
discussing
how
to
calculate
the
sum
of
squares.
How
should
we
calculate
the
sum
of
squares
for
the
interaction
terms,
\(SS_{A:B}\)?
Well,
first
off,
it
helps
to
notice
how
we
have
just
defined
the
interaction
effect
in
terms
of
the
extent
to
which
the
actual
group
means
differ
from
what
you'd
expect
by
just
looking
at
the
marginal
means.
Of
course,
all
of
those
formulas
refer
to
population
parameters
rather
than
sample
statistics,
so
we
don't
actually
know
what
they
are.
However,
we
can
estimate
them
by
using
sample
means
in
place
of
population
means.
So
for
Factor
A,
a
good
way
to
estimate
the
main
effect
at
level
r
is
as
the
difference
between
the
sample
marginal
mean
\(\bar
{Y}_{rc}\)
and
the
sample
grand
mean
\(\bar
{Y}_{..}\).
That
is,
we
would
use
this
as
our
estimate
of
the
effect
\[\hat
{\alpha
}_r
=
bar{Y}_{r.}-\bar
{Y}_{..}\]
Similarly,
our
estimate
of
the
main
effect
of
Factor
B
at
level
c
can
be
defined
as
follows
\[\hat
{\beta
}_{c}=\hat
{Y}_{.c}-\bar
{Y}_{..}\]
Now,
if
you
go
back
to
the
formulas
that
I
used
to
describe
the
\(SS\)
values
for
the
two
main
effects,
you'll
notice
that
these
effect
terms
are
exactly
the
quantities
that
we
were
squaring
and
summing!
So,
what's
the
analog
of
this
for
interaction
terms?
The
answer
to
this
can
be
found
by
first
rearranging
the
formula
for
the
group
means
\(\mu
_{rc}\)
under
the
alternative
hypothesis,
so
that
we
get
this
\[\begin
{aligned}
(\alpha
\beta
)_{rc}
&
=
\mu
_{rc}
-
\mu
_{..}
-
\alpha
_r
-
\beta
_c
\\
&
=
\mu
_{rc}
-
\mu
_{..}
-
(\mu
_{r.}-\mu
_{..})-(\mu
_{.c}-\mu
_{..})
\\
&
=
\mu
_{rc}
-
\mu
_{r.}
-
\mu
_{.c}
+\mu
_{..}
\end
{aligned}\]
So,
once
again
if
we
substitute
our
sample
statistics
in
place
of
the
population
means,
we
get
the
following
as
our
estimate
of
the
interaction
effect
for
group
\(rc\),
which
is
\[(\hat
{\alpha
\beta
})_{rc}=\bar
{Y}_{rc}-\hat
{Y}_{r.}-\bar
{Y}_{.c}+\bar
{Y}_{..}\]
Now
all
we
have
to
do
is
sum
all
of
these
estimates
across
all
\(R\)
levels
of
Factor
A
and
all
\(C\)
levels
of
Factor
B,
and
we
obtain
the
following
formula
for
the
sum
of
squares
associated
with
the
interaction
as
a
whole
\[SS_{A:B}=N
\sum
_{r=1}^R
\sum
_{c=1}^C
(\bar
{Y}_{rc}-\bar
{Y}_{r.}-\bar
{Y}_{.c}+\bar
{Y}_{..})^2\]
where
we
multiply
by
\(N\)
because
there
are
\(N\)
observations
in
each
of
the
groups,
and
we
want
our
\(SS\)
values
to
reflect
the
variation
among
observations
accounted
for
by
the
interaction,
not
the
variation
among
groups.
Now
that
we
have
a
formula
for
calculating
\(SS_{A:B}\),
it's
important
to
recognise
that
the
interaction
term
is
part
of
the
model
(of
course),
so
the
total
sum
of
squares
associated
with
the
model,
\(SSM\),
is
now
equal
to
the
sum
of
the
three
relevant
\(SS\)
values,
\(SS_A
+
SS_B
+
SS_{A:B}\).
The
residual
sum
of
squares
\(SSR\)
is
still
defined
as
the
leftover
variation,
namely
\(SS_T
-
SS_M\),
but
now
that
we
have
the
interaction
term
this
becomes
\[SS_R=SS_T-(SS_A+SS_B+SS_{A:B})\]
As
a
consequence,
the
residual
sum
of
squares
\(SS_R\)
will
be
smaller
than
in
our
original
ANOVA
that
didn't
include
interactions.
\@endanenote 
\@doanenote {24}
macro:->You
may
have
spotted
this
already
when
looking
at
the
main
effects
analysis
in
jamovi
that
we
described
earlier.
For
the
purpose
of
the
explanation
in
this
book
I
removed
the
interaction
component
from
the
earlier
model
to
keep
things
clean
and
simple
\@endanenote 
\@doanenote {25}
macro:->This
chapter
seems
to
be
setting
a
new
record
for
the
number
of
different
things
that
the
letter
R
can
stand
for.
So
far
we
have
R
referring
to
the
software
package,
the
number
of
rows
in
our
table
of
means,
the
residuals
in
the
model,
and
now
the
correlation
coefficient
in
a
regression.
Sorry.
We
clearly
don't
have
enough
letters
in
the
alphabet.
However,
I've
tried
pretty
hard
to
be
clear
on
which
thing
R
is
referring
to
in
each
case
\@endanenote 
\@doanenote {26}
macro:->Implausibly
large,
I
would
think.
The
artificiality
of
this
data
set
is
really
starting
to
show!
\@endanenote 
\@doanenote {27}
macro:->What's
the
difference
between
treatment
and
simple
contrasts,
I
hear
you
ask?
Well,
as
a
basic
example
consider
a
gender
main
effect,
with
\(m=0\)
and
\(f=1\).
The
coefficient
corresponding
to
the
treatment
contrast
will
measure
the
difference
in
mean
between
females
and
males,
and
the
intercept
would
be
the
mean
of
the
males.
However,
with
a
simple
contrast,
i.e.,
\(m=-1\)
and
\(f=1\),
the
intercept
is
the
average
of
the
means
and
the
main
effect
is
the
difference
of
each
group
mean
from
the
intercept.
\@endanenote 
\@doanenote {28}
macro:->If,
for
instance,
you
actually
find
yourself
interested
to
know
if
Group
A
is
significantly
different
from
the
mean
of
Group
B
and
Group
C,
then
you
need
to
use
a
different
tool
(e.g.,
Scheffe's
method,
which
is
more
conservative,
and
beyond
the
scope
of
this
book).
However,
in
most
cases
you
probably
are
interested
in
pairwise
group
differences,
so
Tukey's
HSD
is
a
pretty
useful
thing
to
know
about.
\@endanenote 
\@doanenote {29}
macro:->This
discrepancy
in
standard
deviations
might
(and
should)
make
you
wonder
if
we
have
a
violation
of
the
homogeneity
of
variance
assumption.
I'll
leave
it
as
an
exercise
for
the
reader
to
double
check
this
using
the
Levene
test
option.
\@endanenote 
\@doanenote {30}
macro:->Actually,
this
is
a
bit
of
a
lie.
ANOVAs
can
vary
in
other
ways
besides
the
ones
I've
discussed
in
this
book.
For
instance,
I've
completely
ignored
the
difference
between
fixed-effect
models
in
which
the
levels
of
a
factor
are
``fixed''
by
the
experimenter
or
the
world,
and
random-effect
models
in
which
the
levels
are
random
samples
from
a
larger
population
of
possible
levels
(this
book
only
covers
fixed-effect
models).
Don't
make
the
mistake
of
thinking
that
this
book,
or
any
other
one,
will
tell
you
``everything
you
need
to
know''
about
statistics,
any
more
than
a
single
book
could
possibly
tell
you
everything
you
need
to
know
about
psychology,
physics
or
philosophy.
Life
is
too
complicated
for
that
to
ever
be
true.
This
isn't
a
cause
for
despair,
though.
Most
researchers
get
by
with
a
basic
working
knowledge
of
ANOVA
that
doesn't
go
any
further
than
this
book
does.
I
just
want
you
to
keep
in
mind
that
this
book
is
only
the
beginning
of
a
very
long
story,
not
the
whole
story.
\@endanenote 
\@doanenote {31}
macro:->Or,
at
the
very
least,
rarely
of
interest.
\@endanenote 
\@doanenote {32}
macro:->However,
in
jamovi
the
results
for
Type
III
sum
of
squares
ANOVA
are
the
same
regardless
of
the
contrast
selected,
so
jamovi
is
obviously
doing
something
different!
\@endanenote 
\@doanenote {33}
macro:->Note,
of
course,
that
this
does
depend
on
the
model
that
the
user
specified.
If
the
original
ANOVA
model
doesn't
contain
an
interaction
term
for
\(B
\times
C\),
then
obviously
it
won't
appear
in
either
the
null
or
the
alternative.
But
that's
true
for
Types
I,
II
and
III.
They
never
include
any
terms
that
you
didn't
include,
but
they
make
different
choices
about
how
to
construct
tests
for
the
ones
that
you
did
include.
\@endanenote 
\@doanenote {34}
macro:->I
find
it
amusing
to
note
that
the
default
in
R
is
Type
I
and
the
default
in
SPSS
and
jamovi
is
Type
III.
Neither
of
these
appeals
to
me
all
that
much.
Relatedly,
I
find
it
depressing
that
almost
nobody
in
the
psychological
literature
ever
bothers
to
report
which
Type
of
tests
they
ran,
much
less
the
order
of
variables
(for
Type
I)
or
the
contrasts
used
(for
Type
III).
Often
they
don't
report
what
software
they
used
either.
The
only
way
I
can
ever
make
any
sense
of
what
people
typically
report
is
to
try
to
guess
from
auxiliary
cues
which
software
they
were
using,
and
to
assume
that
they
never
changed
the
default
settings.
Please
don't
do
this!
Now
that
you
know
about
these
issues
make
sure
you
indicate
what
software
you
used,
and
if
you're
reporting
ANOVA
results
for
unbalanced
data,
then
specify
what
Type
of
tests
you
ran,
specify
order
information
if
you've
done
Type
I
tests
and
specify
contrasts
if
you've
done
Type
III
tests.
Or,
even
better,
do
hypotheses
tests
that
correspond
to
things
you
really
care
about
and
then
report
those!
\@endanenote 
